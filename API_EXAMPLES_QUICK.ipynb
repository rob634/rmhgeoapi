{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Examples - Quick Reference\n",
    "\n",
    "Condensed notebook covering the core Platform endpoints (updated 18 DEC 2025):\n",
    "\n",
    "### CoreMachine API (Direct ETL Access)\n",
    "1. Health Check\n",
    "2. Container Check (Sync)\n",
    "3. Process Vector\n",
    "4. **Process Raster v2** (single file, ‚â§800 MB)\n",
    "5. **Process Large Raster v2** (100 MB - 30 GB, tiled processing)\n",
    "6. **Process Raster Collection v2** (‚â§20 files, each ‚â§800 MB)\n",
    "7. Rejection Examples (size/count limit violations)\n",
    "\n",
    "### Platform API (Anti-Corruption Layer)\n",
    "8. Platform Single Raster (DDH identifiers)\n",
    "9. Platform Raster Collection (DDH identifiers)\n",
    "10. Platform Status Check\n",
    "11. **Unpublish Vector** (3 options: DDH IDs, request_id, cleanup mode)\n",
    "12. **Unpublish Raster** (3 options: DDH IDs, request_id, cleanup mode)\n",
    "13. Platform Operations (health, stats, failures)\n",
    "\n",
    "### Size Routing Summary\n",
    "\n",
    "| File Size | Job Type | Notes |\n",
    "|-----------|----------|-------|\n",
    "| ‚â§800 MB | `process_raster_v2` | Standard COG conversion |\n",
    "| 100 MB - 30 GB | `process_large_raster_v2` | Tiled COG workflow |\n",
    "| Collection ‚â§20 files | `process_raster_collection_v2` | Each file must be ‚â§800 MB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\nimport time\n\n# =============================================================================\n# CONFIGURATION - All variables defined here\n# =============================================================================\n\n# Function App Base URL\nBASE_URL = \"https://rmhazuregeoapi-a3dma3ctfdgngwf6.eastus-01.azurewebsites.net\"\n\n# Storage Containers\nBRONZE_CONTAINER = \"rmhazuregeobronze\"  # Main bronze container for raw input\nSILVER_CONTAINER = \"silver-cogs\"         # Processed COGs output\n\n# PostGIS Schema\nPOSTGIS_SCHEMA = \"geo\"\n\n# =============================================================================\n# Helper Functions\n# =============================================================================\n\ndef api_call(method, endpoint, data=None, params=None, timeout=30):\n    \"\"\"Make API call and return formatted response.\"\"\"\n    url = f\"{BASE_URL}{endpoint}\"\n    headers = {\"Content-Type\": \"application/json\"}\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"{method} {endpoint}\")\n    print(f\"{'='*60}\")\n    \n    if data:\n        print(f\"\\nRequest Body:\")\n        print(json.dumps(data, indent=2))\n    \n    try:\n        if method == \"GET\":\n            response = requests.get(url, params=params, timeout=timeout)\n        elif method == \"POST\":\n            response = requests.post(url, json=data, headers=headers, timeout=timeout)\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n        \n        print(f\"\\nStatus: {response.status_code}\")\n        \n        try:\n            result = response.json()\n            print(f\"\\nResponse:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        except:\n            print(f\"\\nResponse (text): {response.text[:500]}\")\n            return response.text\n            \n    except requests.exceptions.Timeout:\n        print(f\"\\n‚ùå Request timed out (timeout={timeout}s)\")\n        return None\n    except Exception as e:\n        print(f\"\\n‚ùå Error: {e}\")\n        return None\n\ndef check_job_status(job_id, max_polls=20, poll_interval=5):\n    \"\"\"Poll job status until completion or timeout.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Polling job: {job_id}\")\n    print(f\"{'='*60}\")\n    \n    for i in range(max_polls):\n        result = requests.get(f\"{BASE_URL}/api/jobs/status/{job_id}\", timeout=30).json()\n        status = result.get(\"status\", \"unknown\")\n        stage = result.get(\"current_stage\", \"?\")\n        \n        print(f\"  [{i+1}/{max_polls}] Status: {status}, Stage: {stage}\")\n        \n        if status in [\"completed\", \"failed\"]:\n            print(f\"\\nFinal Result:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        \n        time.sleep(poll_interval)\n    \n    print(f\"\\n‚ö†Ô∏è Polling timeout after {max_polls * poll_interval}s\")\n    return result\n\n# Display configuration\nprint(\"=\" * 60)\nprint(\"API CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"Base URL:          {BASE_URL}\")\nprint(f\"Bronze Container:  {BRONZE_CONTAINER}\")\nprint(f\"Silver Container:  {SILVER_CONTAINER}\")\nprint(f\"PostGIS Schema:    {POSTGIS_SCHEMA}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Health Check\n",
    "\n",
    "Comprehensive system health check (~60s due to database, Service Bus, and storage checks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Check (takes ~60s)\n",
    "result = api_call(\"GET\", \"/api/health\", timeout=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Container Check (Sync)\n",
    "\n",
    "Quick synchronous endpoint to list blobs in a container. No job queue - returns immediately.\n",
    "\n",
    "**Parameters:**\n",
    "- `suffix`: Filter by extension (e.g., `.tif`, `.geojson`)\n",
    "- `metadata`: `true` (default) returns full blob info, `false` returns just names\n",
    "- `limit`: Max blobs to return (default: 500, max: 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Container Check - Sync endpoint (returns immediately, no job queue)\n# List first 10 TIF files with full metadata\n\nresult = api_call(\"GET\", f\"/api/containers/{BRONZE_CONTAINER}/blobs\", \n                  params={\"suffix\": \".tif\", \"limit\": 10, \"metadata\": \"true\"})\n\n# Show summary\nif result and isinstance(result, dict):\n    count = result.get(\"count\", 0)\n    blobs = result.get(\"blobs\", [])\n    print(f\"\\nüìä Found {count} TIF files\")\n    if blobs:\n        total_mb = sum(b.get(\"size_mb\", 0) for b in blobs)\n        print(f\"üì¶ Total size: {total_mb:.2f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Process Vector\n",
    "\n",
    "Submit a vector file (GeoJSON, Shapefile, GeoPackage) for ingestion into PostGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Submit Vector\nvector_request = {\n    \"dataset_id\": \"test-vectors\",\n    \"resource_id\": \"geojson-8\",\n    \"version_id\": \"v1\",\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": \"8.geojson\",\n    \"service_name\": \"Test GeoJSON 8\"\n}\n\nresult = api_call(\"POST\", \"/api/platform/submit\", vector_request)\nvector_job_id = result.get(\"job_id\") if result else None\nprint(f\"\\nüìã Job ID: {vector_job_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Vector Job Status\n",
    "if vector_job_id:\n",
    "    check_job_status(vector_job_id)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job_id from previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Process Raster (Single File)\n",
    "\n",
    "Submit a single raster file for COG conversion and STAC cataloging.\n",
    "\n",
    "### Size Limits (13 DEC 2025)\n",
    "\n",
    "| Limit | Value | Behavior |\n",
    "|-------|-------|----------|\n",
    "| **Max file size** | 800 MB | Files >800MB rejected ‚Üí use `process_large_raster_v2` |\n",
    "| **Min file size** | None | Any size accepted |\n",
    "\n",
    "**Pre-flight validation** automatically checks file size before processing.\n",
    "\n",
    "### Test Data\n",
    "- **dctest.tif** (25.8 MB) - Small RGB GeoTIFF, processes in ~22 seconds\n",
    "- **antigua.tif** (11.16 GB) - Too large, will be rejected with error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Submit Single Raster via CoreMachine API (direct)\n# Using dctest.tif (25.8 MB) - verified working 13 DEC 2025\n\nraster_request = {\n    \"blob_name\": \"dctest.tif\",\n    \"container_name\": BRONZE_CONTAINER\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_raster_v2\", raster_request)\nraster_job_id = result.get(\"job_id\") if result else None\n\n# Show size metadata from pre-flight validation\nif result and \"parameters\" in result:\n    params = result[\"parameters\"]\n    print(f\"\\nüìè Pre-flight Size Check:\")\n    print(f\"   File size: {params.get('_blob_size_mb', 'N/A'):.2f} MB\")\n    print(f\"   File exists: ‚úÖ\")\n\nprint(f\"\\nüìã Job ID: {raster_job_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Raster Job Status\n",
    "if raster_job_id:\n",
    "    check_job_status(raster_job_id)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job_id from previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Process Large Raster (100 MB - 30 GB)\n",
    "\n",
    "Submit a large raster for tiled COG processing. Uses 5-stage workflow:\n",
    "1. Generate tiling scheme\n",
    "2. Extract tiles (sequential)\n",
    "3. Create COGs (parallel)\n",
    "4. Create MosaicJSON\n",
    "5. Create STAC collection\n",
    "\n",
    "### Size Limits (13 DEC 2025)\n",
    "\n",
    "| Limit | Value | Behavior |\n",
    "|-------|-------|----------|\n",
    "| **Min file size** | 100 MB | Files <100MB should use `process_raster_v2` |\n",
    "| **Max file size** | 30 GB | Files >30GB not supported |\n",
    "\n",
    "### Test Data\n",
    "- **antigua.tif** (11.16 GB) - Maxar Vivid Imagery of Antigua island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Submit Large Raster via CoreMachine API (direct)\n# Using antigua.tif (11.16 GB) - verified 13 DEC 2025\n# Note: This is a long-running job (~30+ minutes for tiling and COG creation)\n\nlarge_raster_request = {\n    \"blob_name\": \"antigua.tif\",\n    \"container_name\": BRONZE_CONTAINER\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_large_raster_v2\", large_raster_request)\nlarge_raster_job_id = result.get(\"job_id\") if result else None\n\n# Show size metadata from pre-flight validation\nif result and \"parameters\" in result:\n    params = result[\"parameters\"]\n    size_mb = params.get('_blob_size_mb', 0)\n    print(f\"\\nüìè Pre-flight Size Check:\")\n    print(f\"   File size: {size_mb:.2f} MB ({size_mb/1024:.2f} GB)\")\n    print(f\"   Valid for large raster: {'‚úÖ' if 100 <= size_mb <= 30000 else '‚ùå'}\")\n\nprint(f\"\\nüìã Job ID: {large_raster_job_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Large Raster Job Status\n",
    "if large_raster_job_id:\n",
    "    check_job_status(large_raster_job_id, max_polls=30, poll_interval=10)  # Longer timeout for large files\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job_id from previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Process Raster Collection (Multi-File)\n",
    "\n",
    "Submit multiple raster files to be processed as a collection with MosaicJSON.\n",
    "\n",
    "### Size and Count Limits (13 DEC 2025)\n",
    "\n",
    "| Limit | Value | Behavior |\n",
    "|-------|-------|----------|\n",
    "| **Max files per collection** | 20 | Collections with >20 files rejected |\n",
    "| **Max individual file size** | 800 MB | Collections with ANY file >800MB rejected |\n",
    "| **Min files per collection** | 2 | Single files should use `process_raster_v2` |\n",
    "\n",
    "**Pre-flight validation order:**\n",
    "1. **Collection count** - Rejected immediately if >20 files (before any blob checks)\n",
    "2. **Individual file sizes** - Each blob checked in parallel; rejected if ANY exceeds 800MB\n",
    "3. **File existence** - All blobs must exist in the container\n",
    "\n",
    "### Size Metadata Captured\n",
    "After validation, these fields are available in job parameters:\n",
    "- `_blob_list_count` - Number of files\n",
    "- `_blob_list_max_size_mb` - Largest file size\n",
    "- `_blob_list_total_size_mb` - Total size of all files\n",
    "- `_blob_list_largest_blob` - Name of largest file\n",
    "- `_blob_list_has_large_raster` - True if any file >800MB\n",
    "\n",
    "### Test Data\n",
    "- **namangan/** folder (4 tiles, 1.6 GB total):\n",
    "  - R1C1: 778 MB, R1C2: 704 MB, R2C1: 73 MB, R2C2: 65 MB\n",
    "  - All under 800 MB limit ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Submit Raster Collection via CoreMachine API (direct)\n# Using namangan 4-tile collection (1.6 GB total) - verified 13 DEC 2025\n\ncollection_request = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"blob_list\": [\n        \"namangan/namangan14aug2019_R1C1cog.tif\",  # 778 MB\n        \"namangan/namangan14aug2019_R1C2cog.tif\",  # 704 MB\n        \"namangan/namangan14aug2019_R2C1cog.tif\",  # 73 MB\n        \"namangan/namangan14aug2019_R2C2cog.tif\"   # 65 MB\n    ],\n    \"collection_id\": \"namangan-test\"\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_raster_collection_v2\", collection_request)\ncollection_job_id = result.get(\"job_id\") if result else None\n\n# Show size metadata from pre-flight validation\nif result and \"parameters\" in result:\n    params = result[\"parameters\"]\n    print(f\"\\nüìè Pre-flight Size Check:\")\n    print(f\"   Files in collection: {params.get('_blob_list_count', 'N/A')}\")\n    print(f\"   Largest file: {params.get('_blob_list_max_size_mb', 0):.2f} MB\")\n    print(f\"   Total size: {params.get('_blob_list_total_size_mb', 0):.2f} MB\")\n    print(f\"   Largest blob: {params.get('_blob_list_largest_blob', 'N/A')}\")\n    print(f\"   Has large raster (>800MB): {'‚ùå Yes' if params.get('_blob_list_has_large_raster') else '‚úÖ No'}\")\n\nprint(f\"\\nüìã Job ID: {collection_job_id}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Raster Collection Job Status\n",
    "if collection_job_id:\n",
    "    check_job_status(collection_job_id, max_polls=30, poll_interval=10)  # Longer timeout for multi-file\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No job_id from previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference: Manual Job Status Check\n",
    "\n",
    "Use this cell to check any job by ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Job Status Check\n",
    "# Replace with your job_id\n",
    "manual_job_id = \"YOUR_JOB_ID_HERE\"\n",
    "\n",
    "if manual_job_id != \"YOUR_JOB_ID_HERE\":\n",
    "    check_job_status(manual_job_id)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Replace 'YOUR_JOB_ID_HERE' with an actual job_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Rejection Examples (Size/Count Limit Violations)\n",
    "\n",
    "These examples demonstrate the pre-flight validation rejecting invalid requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Single raster too large (>800 MB)\n# antigua.tif is 11.16 GB - should be rejected with message to use process_large_raster_v2\n\nprint(\"=\" * 60)\nprint(\"TEST 1: Single raster exceeding 800 MB limit\")\nprint(\"=\" * 60)\n\nlarge_single_request = {\n    \"blob_name\": \"antigua.tif\",  # 11.16 GB\n    \"container_name\": BRONZE_CONTAINER\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_raster_v2\", large_single_request)\nif result and \"error\" in result:\n    print(f\"\\n‚úÖ Correctly rejected: {result.get('message', '')[:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Collection with too many files (>20)\n# Should be rejected before any blob checks are made\n\nprint(\"=\" * 60)\nprint(\"TEST 2: Collection exceeding 20 file limit\")\nprint(\"=\" * 60)\n\ntoo_many_files_request = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"blob_list\": [f\"file{i}.tif\" for i in range(21)],  # 21 files\n    \"collection_id\": \"test-too-many\"\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_raster_collection_v2\", too_many_files_request)\nif result and \"error\" in result:\n    print(f\"\\n‚úÖ Correctly rejected: {result.get('message', '')[:100]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 3: Collection with missing blob\n# Should be rejected with list of missing files\n\nprint(\"=\" * 60)\nprint(\"TEST 3: Collection with non-existent file\")\nprint(\"=\" * 60)\n\nmissing_blob_request = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"blob_list\": [\n        \"namangan/namangan14aug2019_R1C1cog.tif\",  # exists\n        \"nonexistent_file_xyz123.tif\"               # does not exist\n    ],\n    \"collection_id\": \"test-missing\"\n}\n\nresult = api_call(\"POST\", \"/api/jobs/submit/process_raster_collection_v2\", missing_blob_request)\nif result and \"error\" in result:\n    print(f\"\\n‚úÖ Correctly rejected: {result.get('message', '')[:100]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Platform API (Anti-Corruption Layer)\n",
    "\n",
    "The Platform API is an **Anti-Corruption Layer (ACL)** that shields external applications from CoreMachine internals. External apps use high-level DDH identifiers (`dataset_id`, `resource_id`, `version_id`); Platform translates them to CoreMachine job parameters automatically.\n",
    "\n",
    "### Platform API vs CoreMachine API\n",
    "\n",
    "| Aspect | Platform API | CoreMachine API |\n",
    "|--------|--------------|-----------------|\n",
    "| **Audience** | External applications (DDH) | Internal tools, power users |\n",
    "| **Identifiers** | `dataset_id`, `resource_id`, `version_id` | `blob_name`, `table_name`, `collection_id` |\n",
    "| **Output naming** | Auto-generated from DDH IDs | You specify everything |\n",
    "| **Status tracking** | `request_id` (DDH-friendly) | `job_id` (internal hash) |\n",
    "\n",
    "### Endpoints Summary\n",
    "\n",
    "| Endpoint | Purpose |\n",
    "|----------|---------|\n",
    "| `/api/platform/raster` | Single raster file processing |\n",
    "| `/api/platform/raster-collection` | Multiple raster files (2-20 files) |\n",
    "| `/api/platform/submit` | Generic submission (auto-detects data type) |\n",
    "| `/api/platform/status/{request_id}` | Check request/job status |\n",
    "| `/api/platform/unpublish/vector` | Remove vector data |\n",
    "| `/api/platform/unpublish/raster` | Remove raster data |\n",
    "| `/api/platform/health` | Platform health check |\n",
    "| `/api/platform/stats` | Aggregated job statistics |\n",
    "| `/api/platform/failures` | Recent failures |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Platform API - Single Raster\n",
    "\n",
    "Submit a single raster file using DDH identifiers. Output paths are auto-generated from identifiers.\n",
    "\n",
    "### Key Benefit\n",
    "Files exceeding 800 MB are automatically routed to the large raster tiling workflow - no action required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Platform API - Single Raster with DDH Identifiers\n# Uses dataset_id/resource_id/version_id for output naming\n\nplatform_raster_request = {\n    \"dataset_id\": \"test-raster-notebook\",\n    \"resource_id\": \"dctest\",\n    \"version_id\": \"v1\",\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": \"dctest.tif\",\n    \"service_name\": \"DC Test Imagery\",\n    \"access_level\": \"OUO\",\n    \"description\": \"Test raster via Platform API\"\n}\n\nresult = api_call(\"POST\", \"/api/platform/raster\", platform_raster_request)\nplatform_raster_request_id = result.get(\"request_id\") if result else None\nplatform_raster_job_id = result.get(\"job_id\") if result else None\n\nprint(f\"\\nüìã Request ID: {platform_raster_request_id}\")\nprint(f\"üìã Job ID: {platform_raster_job_id}\")\nif result:\n    print(f\"üìç Monitor URL: {result.get('monitor_url', 'N/A')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Platform API - Raster Collection\n",
    "\n",
    "Submit multiple raster files using DDH identifiers. Creates a unified STAC collection with MosaicJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Platform API - Raster Collection with DDH Identifiers\n# Uses dataset_id/resource_id/version_id for output naming\n\nplatform_collection_request = {\n    \"dataset_id\": \"namangan-imagery\",\n    \"resource_id\": \"aug2019\",\n    \"version_id\": \"v1\",\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": [\n        \"namangan/namangan14aug2019_R1C1cog.tif\",\n        \"namangan/namangan14aug2019_R1C2cog.tif\",\n        \"namangan/namangan14aug2019_R2C1cog.tif\",\n        \"namangan/namangan14aug2019_R2C2cog.tif\"\n    ],\n    \"service_name\": \"Namangan Satellite Imagery\",\n    \"access_level\": \"OUO\"\n}\n\nresult = api_call(\"POST\", \"/api/platform/raster-collection\", platform_collection_request)\nplatform_collection_request_id = result.get(\"request_id\") if result else None\n\nprint(f\"\\nüìã Request ID: {platform_collection_request_id}\")\nprint(f\"üìã File Count: {result.get('file_count', 'N/A')}\" if result else \"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Platform Status Check\n",
    "\n",
    "Check request status using DDH-friendly `request_id` (shorter than `job_id`).\n",
    "\n",
    "Returns comprehensive status including job progress, stage info, and task summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform Status Check - Use request_id from Platform submission\n",
    "# Replace with your request_id from earlier Platform API calls\n",
    "\n",
    "platform_request_id = platform_raster_request_id or \"YOUR_REQUEST_ID_HERE\"\n",
    "\n",
    "if platform_request_id and platform_request_id != \"YOUR_REQUEST_ID_HERE\":\n",
    "    result = api_call(\"GET\", f\"/api/platform/status/{platform_request_id}\")\n",
    "    \n",
    "    if result and result.get(\"success\"):\n",
    "        print(f\"\\nüìä Status Summary:\")\n",
    "        print(f\"   Job Status: {result.get('job_status', 'N/A')}\")\n",
    "        print(f\"   Job Stage: {result.get('job_stage', 'N/A')}\")\n",
    "        print(f\"   Data Type: {result.get('data_type', 'N/A')}\")\n",
    "        \n",
    "        task_summary = result.get('task_summary', {})\n",
    "        if task_summary:\n",
    "            print(f\"\\n   Tasks: {task_summary.get('completed', 0)}/{task_summary.get('total', 0)} completed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Set platform_request_id to a valid request_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Unpublish Vector Data\n",
    "\n",
    "Remove vector data from the platform. Three ways to identify what to delete:\n",
    "\n",
    "1. **By DDH Identifiers** (Preferred) - Uses `dataset_id`, `resource_id`, `version_id`\n",
    "2. **By Request ID** - Uses original platform request_id from submission\n",
    "3. **Cleanup Mode** - Direct table_name for orphaned data\n",
    "\n",
    "### Workflow Stages\n",
    "1. **Inventory** - Query `geo.table_metadata` for ETL/STAC linkage\n",
    "2. **Drop Table** - DROP PostGIS table + DELETE metadata row\n",
    "3. **Cleanup** - Delete STAC item if linked + create audit record\n",
    "\n",
    "### Important\n",
    "- Default `dry_run=true` for safety (shows what would be deleted)\n",
    "- Set `dry_run=false` to actually execute deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Vector - Option 1: By DDH Identifiers (Preferred)\n",
    "# dry_run=true shows what would be deleted without executing\n",
    "\n",
    "unpublish_vector_ddh = {\n",
    "    \"dataset_id\": \"test-vectors\",\n",
    "    \"resource_id\": \"geojson-8\",\n",
    "    \"version_id\": \"v1\",\n",
    "    \"dry_run\": True  # Set to False to actually delete\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/unpublish/vector\", unpublish_vector_ddh)\n",
    "if result:\n",
    "    print(f\"\\nüìã Mode: {result.get('mode', 'N/A')}\")\n",
    "    print(f\"üîç Dry Run: {result.get('dry_run', 'N/A')}\")\n",
    "    print(f\"üìç Table: {result.get('table_name', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Vector - Option 2: By Request ID\n",
    "# Uses the request_id from the original platform submission\n",
    "\n",
    "unpublish_vector_request = {\n",
    "    \"request_id\": \"YOUR_ORIGINAL_REQUEST_ID\",  # From /api/platform/submit response\n",
    "    \"dry_run\": True\n",
    "}\n",
    "\n",
    "# Uncomment to test (replace with actual request_id)\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/vector\", unpublish_vector_request)\n",
    "print(\"‚ö†Ô∏è Uncomment and replace request_id to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Vector - Option 3: Cleanup Mode (Direct table_name)\n",
    "# For orphaned tables that don't have platform request records\n",
    "\n",
    "unpublish_vector_cleanup = {\n",
    "    \"table_name\": \"orphaned_table_v1_0\",\n",
    "    \"schema_name\": \"geo\",  # Optional, defaults to \"geo\"\n",
    "    \"dry_run\": True\n",
    "}\n",
    "\n",
    "# Uncomment to test (replace with actual table_name)\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/vector\", unpublish_vector_cleanup)\n",
    "print(\"‚ö†Ô∏è Uncomment and replace table_name to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Unpublish Raster Data\n",
    "\n",
    "Remove raster data from the platform. Three ways to identify what to delete:\n",
    "\n",
    "1. **By DDH Identifiers** (Preferred) - Uses `dataset_id`, `resource_id`, `version_id`\n",
    "2. **By Request ID** - Uses original platform request_id from submission\n",
    "3. **Cleanup Mode** - Direct STAC identifiers for orphaned data\n",
    "\n",
    "### Workflow Stages\n",
    "1. **Inventory** - Query STAC item, extract asset hrefs for deletion\n",
    "2. **Delete Blobs** - Fan-out deletion of COG/MosaicJSON blobs\n",
    "3. **Cleanup** - Delete STAC item + create audit record\n",
    "\n",
    "### Important\n",
    "- Default `dry_run=true` for safety (shows what would be deleted)\n",
    "- Set `dry_run=false` to actually execute deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Raster - Option 1: By DDH Identifiers (Preferred)\n",
    "# dry_run=true shows what would be deleted without executing\n",
    "\n",
    "unpublish_raster_ddh = {\n",
    "    \"dataset_id\": \"test-raster-notebook\",\n",
    "    \"resource_id\": \"dctest\",\n",
    "    \"version_id\": \"v1\",\n",
    "    \"dry_run\": True  # Set to False to actually delete\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_raster_ddh)\n",
    "if result:\n",
    "    print(f\"\\nüìã Mode: {result.get('mode', 'N/A')}\")\n",
    "    print(f\"üîç Dry Run: {result.get('dry_run', 'N/A')}\")\n",
    "    print(f\"üìç STAC Item: {result.get('stac_item_id', 'N/A')}\")\n",
    "    print(f\"üìÅ Collection: {result.get('collection_id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Raster - Option 2: By Request ID\n",
    "# Uses the request_id from the original platform submission\n",
    "\n",
    "unpublish_raster_request = {\n",
    "    \"request_id\": \"YOUR_ORIGINAL_REQUEST_ID\",  # From /api/platform/raster response\n",
    "    \"dry_run\": True\n",
    "}\n",
    "\n",
    "# Uncomment to test (replace with actual request_id)\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_raster_request)\n",
    "print(\"‚ö†Ô∏è Uncomment and replace request_id to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Raster - Option 3: Cleanup Mode (Direct STAC identifiers)\n",
    "# For orphaned STAC items that don't have platform request records\n",
    "\n",
    "unpublish_raster_cleanup = {\n",
    "    \"stac_item_id\": \"aerial-imagery-2024-site-alpha-v1-0\",\n",
    "    \"collection_id\": \"aerial-imagery-2024\",\n",
    "    \"dry_run\": True\n",
    "}\n",
    "\n",
    "# Uncomment to test (replace with actual STAC identifiers)\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_raster_cleanup)\n",
    "print(\"‚ö†Ô∏è Uncomment and replace STAC identifiers to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Platform Operations\n",
    "\n",
    "Operational endpoints for monitoring platform health and job statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform Health Check\n",
    "# Simplified health status for DDH consumption\n",
    "\n",
    "result = api_call(\"GET\", \"/api/platform/health\")\n",
    "\n",
    "if result and result.get(\"status\"):\n",
    "    print(f\"\\nüìä Platform Status: {result.get('status', 'unknown')}\")\n",
    "    \n",
    "    components = result.get(\"components\", {})\n",
    "    for component, status in components.items():\n",
    "        icon = \"‚úÖ\" if status == \"ok\" else \"‚ùå\"\n",
    "        print(f\"   {icon} {component}: {status}\")\n",
    "    \n",
    "    activity = result.get(\"recent_activity\", {})\n",
    "    if activity:\n",
    "        print(f\"\\nüìà Recent Activity (24h):\")\n",
    "        print(f\"   Jobs: {activity.get('jobs_24h', 0)}\")\n",
    "        print(f\"   Success Rate: {activity.get('success_rate', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform Statistics\n",
    "# Aggregated job statistics over a configurable time window\n",
    "\n",
    "result = api_call(\"GET\", \"/api/platform/stats\", params={\"hours\": 24})\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nüìä Statistics (Last {result.get('time_window_hours', 24)} hours):\")\n",
    "    print(f\"   Total Jobs: {result.get('total_jobs', 0)}\")\n",
    "    \n",
    "    by_status = result.get(\"by_status\", {})\n",
    "    if by_status:\n",
    "        print(f\"\\n   By Status:\")\n",
    "        for status, count in by_status.items():\n",
    "            print(f\"      {status}: {count}\")\n",
    "    \n",
    "    by_type = result.get(\"by_data_type\", {})\n",
    "    if by_type:\n",
    "        print(f\"\\n   By Data Type:\")\n",
    "        for dtype, count in by_type.items():\n",
    "            print(f\"      {dtype}: {count}\")\n",
    "    \n",
    "    avg_time = result.get(\"avg_processing_time_seconds\", 0)\n",
    "    if avg_time:\n",
    "        print(f\"\\n   Avg Processing Time: {avg_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recent Failures\n",
    "# Troubleshooting endpoint with sanitized error messages\n",
    "\n",
    "result = api_call(\"GET\", \"/api/platform/failures\", params={\"hours\": 24, \"limit\": 5})\n",
    "\n",
    "if result:\n",
    "    failures = result.get(\"failures\", [])\n",
    "    total = result.get(\"total_failures\", 0)\n",
    "    \n",
    "    print(f\"\\nüìä Recent Failures: {total} total\")\n",
    "    \n",
    "    if failures:\n",
    "        print(f\"\\nMost Recent:\")\n",
    "        for i, f in enumerate(failures[:5], 1):\n",
    "            print(f\"\\n   [{i}] {f.get('job_type', 'unknown')}\")\n",
    "            print(f\"       Request: {f.get('request_id', 'N/A')[:16]}...\")\n",
    "            print(f\"       Category: {f.get('error_category', 'unknown')}\")\n",
    "            print(f\"       Error: {f.get('error_summary', 'N/A')[:60]}...\")\n",
    "    else:\n",
    "        print(\"\\n   ‚úÖ No failures in the time window\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}