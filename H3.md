# H3 Aggregation Architecture - Project Context

## Overview

This document provides architectural context for a massively parallelized H3 grid aggregation system using Azure Functions, DuckDB, and Overture Maps data.

**Goal**: Aggregate geospatial data into H3 grid cells at levels 6 and 7 for global-scale visualization on a sphere interface.

**Immediate Focus**: Proof of concept at level 6 for Panama and Liberia using Overture GeoParquet data.

**Future Scope**: Zonal statistics on raster data from Planetary Computer.

---

## Core Architecture Decisions

### 1. Skip PostGIS - Use Direct GeoParquet Processing

**Pattern**: `Overture GeoParquet → DuckDB → Aggregated GeoParquet`

**Rationale**:
- Overture data is already in GeoParquet format hosted on Azure
- Simple point→hexagon aggregations don't need PostGIS complexity
- DuckDB can query Azure-hosted parquet directly using Azure extension
- No data movement required - queries run against cloud storage
- PostGIS only needed for complex topology operations (not our use case)

### 2. H3 IS the Batching Strategy

**Key Insight**: H3's hierarchical structure provides natural geographic batching for free.

**Batching by Parent Cells**:
- **Level 6**: Use Level 3 parents (~500 globally) → ~3,000 children each
- **Level 7**: Use Level 4 parents (~3,500 globally) → ~343 children each

**Benefits**:
- Each batch naturally groups spatially proximate hexagons
- Overture's parquet files are spatially organized → better cache efficiency
- Perfect fit for 10-minute Azure Function timeout
- Natural resume-ability (each parent cell = independent unit of work)

### 3. Queue-Based Orchestration

**Workflow**:
```
HTTP Trigger (Generate H3) → HTTP Trigger (Create Batches) → 
Queue Messages → Queue Triggers (Parallel Processing) → 
Write to ADLS GeoParquet
```

**Benefits**:
- Automatic retry on failure
- Massive parallelism (thousands of concurrent executions)
- Azure handles scaling automatically
- No coordination overhead between workers

### 4. Progress Tracking with Azure Table Storage

**Not using a database** - Table Storage is perfect for this use case.

**Schema**:
- PartitionKey: `{resolution}_{dataset}` (e.g., "h3_level7_roads")
- RowKey: Parent H3 index
- Columns: Status, StartTime, EndTime, RecordsProcessed, RetryCount, ErrorMessage

**Rationale**:
- No ACID needed - batch status updates are independent
- Natural idempotency - reprocessing just overwrites status
- Cost: ~$0.045/GB/month vs $100+/month for SQL database
- No connection pool limits for serverless
- Simple queries: "how many completed?"

---

## Deep Water Exclusion Strategy

**Problem**: 75% of global H3 cells are ocean - why process them?

**Solution**: Pre-filter using Natural Earth data

**Process**:
1. Generate coarse parent cells (level 3 or 4) globally
2. Filter using Natural Earth land polygons + coastal buffer (~50km for littoral zone)
3. Only generate child cells (6 or 7) for qualifying parents
4. Store as reference parquet: `land_h3_level6.parquet` and `land_h3_level7.parquet`

**Impact**:
- Level 6 Global: Reduces from 4.8M to 1.5M hexagons (69% savings)
- Scope: ~25% of globe (all land + coastal/littoral waters)
- Deep waters outside exclusive economic zones excluded

---

## DuckDB Integration

### Required Extensions

```python
conn.execute("INSTALL azure; LOAD azure;")
conn.execute("INSTALL h3; LOAD h3;")
conn.execute("INSTALL spatial; LOAD spatial;")
```

### Memory Configuration for Azure Functions

```python
conn.execute("SET memory_limit = '8GB';")  # Premium plan
conn.execute("SET threads = 4;")
```

### Query Pattern Example

```sql
WITH filtered_features AS (
    SELECT * FROM read_parquet(
        'azure://overturemapswesteurope.blob.core.windows.net/release/*/theme=buildings/type=building/*.parquet',
        hive_partitioning=true
    )
    WHERE bbox.xmin >= ? AND bbox.xmax <= ? 
      AND bbox.ymin >= ? AND bbox.ymax <= ?
),
h3_aggregates AS (
    SELECT
        h3_latlng_to_cell(
            ST_Y(ST_Centroid(geometry)), 
            ST_X(ST_Centroid(geometry)), 
            6
        ) as h3_index,
        COUNT(*) as feature_count,
        -- Additional aggregations
    FROM filtered_features
    GROUP BY h3_index
)
SELECT * FROM h3_aggregates
```

---

## Aggregation Methods

### Vector Aggregations (Overture Data)

#### Roads/Transportation
- **Length by road class**: Total meters per H3 cell
  - Classes: motorway, trunk, primary, secondary, tertiary, residential
- **Segment counts**: Number of road segments per cell
- **Connectivity metrics**: Intersection density, network connectivity

**DuckDB Query Pattern**:
```sql
SELECT 
    h3_index,
    road_class,
    COUNT(*) as segment_count,
    SUM(ST_Length(geometry)) as total_length_m,
    AVG(ST_Length(geometry)) as avg_length_m
FROM roads_in_cells
GROUP BY h3_index, road_class
```

#### Buildings
- **Count by building class**: Residential, commercial, industrial, etc.
- **Area calculations**: Total and average footprint (sqm)
- **Height metrics**: Average, max, count of tall buildings (>10m)
- **Density**: Buildings per square kilometer

**DuckDB Query Pattern**:
```sql
SELECT
    h3_index,
    building_class,
    COUNT(*) as building_count,
    SUM(ST_Area(geometry)) as total_area_sqm,
    AVG(height) as avg_height,
    MAX(height) as max_height,
    COUNT(CASE WHEN height > 10 THEN 1 END) as tall_buildings
FROM buildings_in_cells
GROUP BY h3_index, building_class
```

#### Infrastructure Corridors (Advanced Use Case)
Identify significant infrastructure corridors by analyzing spatial patterns:

- **Connectivity index**: Road network connectivity scores
- **Centrality score**: Betweenness centrality in transport network
- **Development intensity**: Building density along corridors
- **Economic significance**: Industrial/commercial concentration
- **Multimodal score**: Presence of multiple transport modes

**Approach**:
1. Calculate infrastructure density per H3 cell
2. Compare each cell with k-ring neighbors (spatial context)
3. Identify cells with >1.5x neighbor average (primary corridors)
4. Use NetworkX to find continuous corridor components
5. Calculate corridor metrics (length, connectivity, development)

### Raster Aggregations (Zonal Statistics)

**For Future Implementation with Planetary Computer COGs**

#### Serverless Zonal Stats Pattern

**Per-Hexagon Processing**:
```python
# One function invocation = One hexagon (or small batch)
def process_h3_zonal_stats(h3_index: str, cog_url: str):
    # 1. Convert H3 to polygon
    polygon = h3.h3_to_geo_boundary(h3_index)
    
    # 2. Read JUST the COG window needed
    with rasterio.open(f'/vsicurl/{cog_url}') as src:
        # COG magic: only downloads tiles intersecting polygon!
        window = rasterio.windows.from_bounds(*polygon.bounds, src.transform)
        data = src.read(1, window=window)
    
    # 3. Calculate stats for ONE hexagon
    return {
        'h3_index': h3_index,
        'mean': data.mean(),
        'sum': data.sum(),
        'std': data.std(),
        'min': data.min(),
        'max': data.max()
    }
```

**Statistics to Calculate**:
- Mean, median, mode
- Sum, count
- Standard deviation, variance
- Min, max
- Percentiles (25th, 50th, 75th, 90th, 95th)

**Tools**:
- `rasterio` for COG reading with `/vsicurl/` for cloud access
- `rioxarray` + `stackstac` for time-series raster data
- `rasterstats` for pure Python zonal statistics (serverless-friendly)
- Note: `exactextract` (C++) is more accurate but requires persistent compute

**Economics**:
- Kenya at Level 6: ~3,000 hexagons
- 3,000 parallel functions × 10 seconds each
- Wall time: ~10 seconds (vs 1 hour on VM)
- Cost: ~$0.03 per run (Premium plan)

---

## Scaling Projections

### Global Processing Capacity

| H3 Level | Parent Level | Total Cells | Land/Coastal | Batches | Cells/Batch | Est. Time |
|----------|--------------|-------------|--------------|---------|-------------|-----------|
| 6        | 3            | 4.8M        | 1.5M         | 500     | ~3,000      | ~3 min    |
| 7        | 4            | 33.6M       | 10.5M        | 3,500   | ~343        | ~5 min    |

### Regional Examples

**Panama (Level 6)**:
- ~50 batches × 2 minutes = 100 minutes total compute
- Wall time: ~2 minutes (parallel execution)
- Cost: ~$0.50 per run

**Global (Level 7)**:
- ~3,500 batches × 5 minutes = 17,500 minutes total compute
- Wall time: ~5 minutes (parallel execution)
- Cost: ~$35 per complete global run

### Storage Costs

- **GeoParquet output**: ~$0.045/GB/month (ADLS)
- **Table Storage progress**: <$1/month for millions of records
- **Queue messages**: Negligible (<$0.01/month)

---

## Azure Functions Workflow

### Phase 1: H3 Cell Generation (HTTP Trigger)

**Endpoint**: `POST /generate-h3-cells`

**Input**:
```json
{
  "resolution": 6,
  "region": "panama",
  "include_coastal": true
}
```

**Process**:
1. Generate H3 cells for region at specified resolution
2. Filter using Natural Earth land/coastal boundaries
3. Save to ADLS: `h3_cells/land_h3_level6_panama.parquet`

**Output**: Cell count, output path, processing time

---

### Phase 2: Batch Creation (HTTP Trigger)

**Endpoint**: `POST /create-aggregation-batches`

**Input**:
```json
{
  "h3_cells_path": "h3_cells/land_h3_level6_panama.parquet",
  "aggregation_types": ["road_length", "building_count"],
  "data_theme": "transportation"
}
```

**Process**:
1. Read H3 cells from parquet
2. Group by parent cell (level 3 for res 6, level 4 for res 7)
3. Create queue message per parent with:
   - Parent H3 index
   - List of child H3 cells
   - Aggregation types to perform
   - Data theme (buildings, transportation, etc.)

**Output**: Number of batches created, estimated processing time

---

### Phase 3: Parallel Aggregation (Queue Trigger)

**Trigger**: Queue message from `h3-aggregation-queue`

**Message Format**:
```json
{
  "parent_h3": "843b1ffffffffff",
  "child_cells": ["863b1ffffffffff", "863b1fffffbffff", ...],
  "resolution": 6,
  "aggregation_types": ["road_length", "building_count"],
  "data_theme": "transportation"
}
```

**Process**:
1. Initialize DuckDB connection with extensions
2. Build spatial query for child H3 cells
3. Query Overture data directly from Azure storage
4. Perform aggregations (sum, count, avg as specified)
5. Update progress in Table Storage
6. Write results: `aggregations/roads_h3_level6_batch_0042.parquet`

**Timeout**: 10 minutes max (Premium/Dedicated plan)

---

### Phase 4: Query Interface (HTTP Trigger)

**Endpoint**: `GET /query-aggregations`

**Input**:
```json
{
  "bbox": [-83.0, 7.0, -77.0, 10.0],
  "resolution": 6,
  "metrics": ["road_length", "building_count"],
  "format": "geojson"
}
```

**Process**:
1. Query aggregated parquet files from ADLS
2. Filter by spatial bounds or H3 indices
3. Return in requested format (GeoJSON, parquet, JSON)

**Output**: Aggregated data ready for visualization

---

## Repository Pattern

### DuckDBRepository
- Base class for DuckDB operations
- Connection management with context managers
- Extension loading (Azure, H3, Spatial)
- Query execution with error handling
- Caching for repeated queries

### OvertureRepository
- Queries Overture Maps GeoParquet
- Methods for each theme (buildings, roads, places)
- Spatial filtering and H3 aggregation queries
- Release version management

### NaturalEarthRepository
- Land/water boundary data
- Coastal buffer generation
- H3 cell filtering by land/water intersection

### H3Repository
- H3 cell generation and manipulation
- Parent-child relationships
- K-ring neighbor calculations
- Geometric operations

### StorageRepository (ADLS)
- Write/read GeoParquet files
- Blob existence checks
- List operations with prefix filtering

### QueueRepository
- Send messages to queue
- Batch message operations
- Queue management

### ProgressRepository (Table Storage)
- Track batch processing status
- Calculate completion percentage
- Handle retry logic
- Query progress summaries

---

## Key Success Factors

1. **H3 Hierarchical Structure**: Provides natural batching and spatial indexing without additional infrastructure

2. **Pre-filtering Ocean Cells**: 69% reduction in global processing by excluding deep water

3. **Queue-based Orchestration**: Enables massive parallelism with built-in fault tolerance and retry

4. **DuckDB Azure Extension**: Direct cloud queries without data movement or staging

5. **Immutable Parquet Outputs**: Enable incremental processing, easy resume, and time-travel queries

6. **10-minute Timeout Design**: Batch sizes designed to comfortably complete within Azure Functions limits

7. **COG-style Reading**: Even for vector data, spatial organization of parquet files improves cache efficiency

8. **H3 Index = Spatial Index**: No need to store separate geometry - H3 index encodes all spatial information

---

## H3 Index Structure

**Not a hash** - H3 indices are hierarchical addresses (64-bit integers, displayed as 15-character hex).

**Components**:
1. **Resolution** (4 bits): Scale level 0-15
2. **Base Cell** (7 bits): One of 122 icosahedron faces
3. **Digit Sequence** (remaining bits): Path down hierarchy

**Properties**:
- Parent-child relationships are deterministic
- Siblings share same parent
- Spatial proximity = similar indices (mostly)
- Can extract resolution from index directly

**For Sphere Visualization**:
- No polygons needed! H3 library converts index → center point or boundary on-the-fly
- Just store H3 index + aggregated metrics
- Renderer converts to geometry as needed

---

## Testing Strategy

### Proof of Concept Regions

**Panama**:
- Bounds: [-83.0, 7.0, -77.0, 10.0]
- Level 6: ~50 batches
- Small enough for rapid iteration
- Complex geography (canal, coast, mountains)

**Liberia**:
- Bounds: [-11.5, 4.3, -7.4, 8.6]
- Similar size to Panama
- Different geographic context (West Africa)
- Validates generalization

### Test Progression

1. **Single H3 cell**: Validate query and aggregation logic
2. **Single batch** (1 parent cell): Test queue processing
3. **Panama region**: Prove end-to-end workflow
4. **Liberia region**: Validate generalization
5. **Global subset**: Test scaling (e.g., all of Africa)
6. **Full global**: Ultimate production test

---

## Future Enhancements

### Phase 2: Raster Integration
- Integrate Planetary Computer STAC catalog
- Implement serverless zonal statistics
- Support time-series raster analysis
- Multi-band raster aggregations

### Phase 3: Temporal Analysis
- Process multiple Overture release versions
- Change detection between time periods
- Growth trajectory calculations
- Animation frame generation

### Phase 4: Multi-Resolution Hierarchies
- Pre-compute parent-child aggregations
- Enable dynamic zoom on sphere visualization
- Drill-down from coarse to fine resolution
- Optimize for interactive exploration

### Phase 5: Cross-Theme Analysis
- Roads within 100m of buildings
- POI clustering analysis
- Land use classification
- Urban growth modeling

---

## Important Caveats

### What This Architecture Is NOT Optimized For:

- **Real-time updates**: Batch processing architecture, not streaming
- **Transactional consistency**: Eventual consistency model
- **Sub-second queries**: Parquet queries are fast but not sub-second
- **Complex spatial joins**: Better suited for simple aggregations
- **Exact pixel statistics**: Raster approach trades accuracy for speed

### When to Consider Alternatives:

- **If needing real-time data**: Consider PostGIS + change data capture
- **If needing complex topology**: Load into PostGIS for analysis
- **If needing sub-second queries**: Pre-aggregate and cache in Redis/Cosmos DB
- **If needing perfect accuracy**: Use exactextract on persistent compute

---

## Cost Optimization Tips

1. **Use Premium Plan only when processing**: Scale down to Consumption plan when idle
2. **Batch similar aggregations**: Combine multiple metrics in single pass
3. **Cache common queries**: Store frequently accessed results in CDN
4. **Use hierarchical processing**: Aggregate to coarse resolution first, generate fine-grained on demand
5. **Leverage parquet compression**: Zstd compression reduces storage and transfer costs
6. **Delete intermediate files**: Only keep final aggregated outputs

---

## Monitoring and Observability

### Key Metrics to Track

**Processing Metrics**:
- Batches completed per minute
- Average processing time per batch
- Failure rate and retry counts
- Records processed per second

**Resource Metrics**:
- Memory usage per function
- CPU utilization
- Network throughput (parquet reads)
- Queue depth

**Cost Metrics**:
- Compute costs per run
- Storage costs (ADLS + Table Storage)
- Data egress (if significant)

### Recommended Tools

- **Application Insights**: Function execution traces, exceptions
- **Azure Monitor**: Resource utilization, scaling metrics
- **Custom Dashboard**: Progress tracking, cost estimates
- **Alerts**: Long-running batches, high failure rates

---

## Quick Reference: Common Operations

### Generate H3 Cells for Region
```bash
curl -X POST https://<function-app>/generate-h3-cells \
  -H "Content-Type: application/json" \
  -d '{"resolution": 6, "region": "panama"}'
```

### Create Aggregation Batches
```bash
curl -X POST https://<function-app>/create-aggregation-batches \
  -H "Content-Type: application/json" \
  -d '{
    "h3_cells_path": "h3_cells/land_h3_level6_panama.parquet",
    "aggregation_types": ["road_length", "building_count"]
  }'
```

### Check Progress
```bash
curl https://<function-app>/progress/roads/6
```

### Query Aggregated Data
```bash
curl -X POST https://<function-app>/query-aggregations \
  -H "Content-Type: application/json" \
  -d '{
    "bbox": [-83.0, 7.0, -77.0, 10.0],
    "resolution": 6,
    "metrics": ["road_length"]
  }'
```

---

## References and Resources

### Key Technologies
- **H3**: https://h3geo.org/
- **DuckDB**: https://duckdb.org/
- **Overture Maps**: https://overturemaps.org/
- **Azure Functions**: https://docs.microsoft.com/azure/azure-functions/

### Related Projects
- **Space2Stats** (DevSeed): H3-based global statistics
- **Planetary Computer**: Microsoft's geospatial data catalog
- **Natural Earth**: Free geographic datasets

### Internal Documentation
- Architecture diagrams: `/docs/architecture/`
- API specifications: `/docs/api/`
- Deployment guides: `/docs/deployment/`

---

**Document Version**: 1.0  
**Last Updated**: October 2025  
**Maintained By**: Project Team


Great security question! **Yes, you absolutely need to worry about SQL injection with DuckDB** - it uses SQL syntax and is vulnerable to the same injection attacks as traditional databases.

## The Risk in Your Architecture

Your system accepts user input for:
- Bounding box coordinates
- H3 indices
- Resolution levels
- Region names
- Aggregation types
- File paths

All of these could be injection vectors if handled improperly.

## ✅ Safe: Parameterized Queries

DuckDB supports parameterized queries using `?` placeholders:

```python
# SAFE - uses parameters
query = """
SELECT * FROM read_parquet(
    'az://overturemapswestus2.blob.core.windows.net/release/2025-09-24.0/theme=buildings/type=building/*'
)
WHERE bbox.xmin >= ? AND bbox.xmax <= ?
  AND bbox.ymin >= ? AND bbox.ymax <= ?
"""

# Parameters are safely escaped
result = conn.execute(query, [west, east, south, north]).fetchdf()
```

You can also use named parameters:

```python
# SAFE - named parameters
query = """
SELECT h3_latlng_to_cell(lat, lon, $resolution) as h3_index
FROM my_table
WHERE id = $id
"""

result = conn.execute(query, {
    'resolution': user_resolution,
    'id': user_id
}).fetchdf()
```

## ❌ Unsafe: String Concatenation

```python
# DANGEROUS - DON'T DO THIS
resolution = request.get('resolution')
query = f"""
SELECT h3_latlng_to_cell(lat, lon, {resolution}) as h3_index
FROM my_table
"""
conn.execute(query)  # Vulnerable to injection!

# An attacker could pass: "6); DROP TABLE important_data; --"
```

## Special Concerns for Your Use Case

### 1. File Paths in `read_parquet()`

This is tricky because the path goes inside the SQL string:

```python
# VULNERABLE
theme = request.get('theme')
query = f"SELECT * FROM read_parquet('az://.../{theme}/type=building/*')"

# Attacker could inject: "buildings'); SELECT * FROM other_table; --"
```

**Solution: Whitelist validation**

```python
ALLOWED_THEMES = {
    'buildings', 'transportation', 'places', 
    'addresses', 'base', 'divisions'
}

ALLOWED_TYPES = {
    'building', 'segment', 'place', 
    'address', 'infrastructure', 'land', 'water'
}

def validate_theme(theme: str) -> str:
    if theme not in ALLOWED_THEMES:
        raise ValueError(f"Invalid theme: {theme}")
    return theme

def validate_type(type_name: str) -> str:
    if type_name not in ALLOWED_TYPES:
        raise ValueError(f"Invalid type: {type_name}")
    return type_name

# Safe usage
theme = validate_theme(request.get('theme'))
type_name = validate_type(request.get('type'))
query = f"SELECT * FROM read_parquet('az://.../{theme}/type={type_name}/*')"
```

### 2. H3 Indices

H3 indices have a specific format - validate them:

```python
import re

def validate_h3_index(h3_index: str) -> str:
    """H3 indices are 15-character hex strings"""
    if not re.match(r'^[0-9a-f]{15}$', h3_index):
        raise ValueError(f"Invalid H3 index format: {h3_index}")
    return h3_index

def validate_h3_indices(indices: List[str]) -> List[str]:
    return [validate_h3_index(idx) for idx in indices]

# Safe to use in WHERE IN clause
h3_cells = validate_h3_indices(user_provided_indices)
placeholders = ','.join(['?'] * len(h3_cells))
query = f"SELECT * FROM table WHERE h3_index IN ({placeholders})"
result = conn.execute(query, h3_cells)
```

### 3. Numeric Parameters

Even numbers should be validated:

```python
def validate_resolution(resolution: int) -> int:
    """H3 resolution must be 0-15"""
    if not (0 <= resolution <= 15):
        raise ValueError(f"Resolution must be 0-15, got: {resolution}")
    return resolution

def validate_bbox(west: float, south: float, east: float, north: float) -> tuple:
    """Validate bounding box coordinates"""
    if not (-180 <= west <= 180 and -180 <= east <= 180):
        raise ValueError("Longitude must be between -180 and 180")
    if not (-90 <= south <= 90 and -90 <= north <= 90):
        raise ValueError("Latitude must be between -90 and 90")
    if west >= east:
        raise ValueError("West must be less than East")
    if south >= north:
        raise ValueError("South must be less than North")
    return (west, south, east, north)

# Usage
resolution = validate_resolution(int(request.get('resolution')))
bbox = validate_bbox(
    float(request.get('west')),
    float(request.get('south')),
    float(request.get('east')),
    float(request.get('north'))
)
```

### 4. Dynamic Column Selection

If users can choose which metrics to return:

```python
# VULNERABLE
metrics = request.get('metrics')  # ["building_count", "road_length"]
query = f"SELECT {', '.join(metrics)} FROM table"

# SAFE - whitelist approach
ALLOWED_METRICS = {
    'building_count': 'COUNT(*)',
    'total_area': 'SUM(ST_Area(geometry))',
    'avg_height': 'AVG(height)',
    'road_length': 'SUM(ST_Length(geometry))',
    'max_height': 'MAX(height)'
}

def build_select_clause(requested_metrics: List[str]) -> str:
    validated = []
    for metric in requested_metrics:
        if metric not in ALLOWED_METRICS:
            raise ValueError(f"Invalid metric: {metric}")
        validated.append(f"{ALLOWED_METRICS[metric]} as {metric}")
    return ', '.join(validated) if validated else '*'

# Usage
select_clause = build_select_clause(request.get('metrics', ['building_count']))
query = f"SELECT h3_index, {select_clause} FROM table GROUP BY h3_index"
```

## Recommended Repository Pattern

Create a safe query builder in your DuckDB repository:

```python
class OvertureRepository(DuckDBRepository):
    
    ALLOWED_THEMES = {'buildings', 'transportation', 'places', 'addresses', 'base', 'divisions'}
    ALLOWED_TYPES = {
        'building', 'segment', 'connector', 'place', 
        'address', 'infrastructure', 'land', 'water', 'division', 'division_area'
    }
    
    def _validate_theme(self, theme: str) -> str:
        if theme not in self.ALLOWED_THEMES:
            raise ValueError(f"Invalid theme: {theme}")
        return theme
    
    def _validate_type(self, type_name: str) -> str:
        if type_name not in self.ALLOWED_TYPES:
            raise ValueError(f"Invalid type: {type_name}")
        return type_name
    
    def _validate_resolution(self, resolution: int) -> int:
        if not (0 <= resolution <= 15):
            raise ValueError(f"Resolution must be 0-15, got: {resolution}")
        return resolution
    
    def _validate_bbox(self, bbox: tuple) -> tuple:
        west, south, east, north = bbox
        if not (-180 <= west <= 180 and -180 <= east <= 180):
            raise ValueError("Invalid longitude")
        if not (-90 <= south <= 90 and -90 <= north <= 90):
            raise ValueError("Invalid latitude")
        if west >= east or south >= north:
            raise ValueError("Invalid bbox bounds")
        return (west, south, east, north)
    
    def aggregate_to_h3(
        self, 
        theme: str,
        type_name: str,
        resolution: int,
        bbox: tuple,
        release: str = "2025-09-24.0"
    ) -> pd.DataFrame:
        """
        Safely aggregate Overture data to H3 cells.
        All inputs are validated before use.
        """
        # Validate all inputs
        theme = self._validate_theme(theme)
        type_name = self._validate_type(type_name)
        resolution = self._validate_resolution(resolution)
        west, south, east, north = self._validate_bbox(bbox)
        
        # Release version validation (date format)
        if not re.match(r'^\d{4}-\d{2}-\d{2}\.\d+$', release):
            raise ValueError(f"Invalid release format: {release}")
        
        # Build safe query with validated strings in the file path
        # and parameterized query for bbox coordinates
        base_url = "az://overturemapswestus2.blob.core.windows.net"
        path = f"{base_url}/release/{release}/theme={theme}/type={type_name}/*"
        
        query = f"""
        SELECT
            h3_latlng_to_cell(
                ST_Y(ST_Centroid(geometry)), 
                ST_X(ST_Centroid(geometry)), 
                ?
            ) as h3_index,
            COUNT(*) as feature_count
        FROM read_parquet(?, hive_partitioning=1)
        WHERE bbox.xmin >= ? AND bbox.xmax <= ?
          AND bbox.ymin >= ? AND bbox.ymax <= ?
        GROUP BY h3_index
        """
        
        # Execute with parameters
        return self.execute_query(
            query, 
            [resolution, path, west, east, south, north]
        )
```

## Additional Security Measures

### 1. Rate Limiting

Prevent abuse of your aggregation endpoints:

```python
from functools import wraps
from datetime import datetime, timedelta

request_counts = {}  # In production, use Redis

def rate_limit(max_requests: int, window_seconds: int):
    def decorator(func):
        @wraps(func)
        def wrapper(req: func.HttpRequest):
            client_id = req.headers.get('X-Client-ID') or req.remote_addr
            now = datetime.utcnow()
            
            # Clean old entries
            cutoff = now - timedelta(seconds=window_seconds)
            if client_id in request_counts:
                request_counts[client_id] = [
                    ts for ts in request_counts[client_id] if ts > cutoff
                ]
            else:
                request_counts[client_id] = []
            
            # Check limit
            if len(request_counts[client_id]) >= max_requests:
                return func.HttpResponse(
                    "Rate limit exceeded",
                    status_code=429
                )
            
            request_counts[client_id].append(now)
            return func(req)
        return wrapper
    return decorator

@app.route(route="create-aggregation-batches")
@rate_limit(max_requests=10, window_seconds=60)
def create_batches(req: func.HttpRequest):
    # Your handler code
    pass
```

### 2. Input Size Limits

Prevent resource exhaustion:

```python
def validate_batch_size(h3_cells: List[str], max_cells: int = 5000) -> List[str]:
    if len(h3_cells) > max_cells:
        raise ValueError(f"Batch too large: {len(h3_cells)} cells (max: {max_cells})")
    return h3_cells

def validate_bbox_size(bbox: tuple, max_area_sq_degrees: float = 100.0) -> tuple:
    west, south, east, north = bbox
    area = (east - west) * (north - south)
    if area > max_area_sq_degrees:
        raise ValueError(f"Bounding box too large: {area} sq degrees (max: {max_area_sq_degrees})")
    return bbox
```

## Summary: Best Practices for Your Architecture

1. **Always use parameterized queries** for user-provided values
2. **Whitelist validation** for themes, types, metrics, and enums
3. **Regex validation** for H3 indices (15-char hex)
4. **Range validation** for numeric inputs (resolution, coordinates)
5. **Size limits** on batch sizes and bounding boxes
6. **Never concatenate user input** into SQL strings
7. **Sanitize file paths** even though they're in trusted storage
8. **Rate limiting** on HTTP endpoints
9. **Logging** of all validation failures for security monitoring

The good news: DuckDB's attack surface is smaller than a traditional database (no users, no network access, no stored procedures), but **input validation is still critical**.

