{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Platform API - B2B Integration Guide\n\n**Last Updated**: 01 FEB 2026\n**Version**: 0.8.6.2 (API Version 1.5.0)\n\nComplete workflow for submitting, approving, and accessing geospatial data through the Platform API.\n\n## Workflow Overview\n\n```\n0. Validate    POST /api/platform/submit?dry_run=true   Check lineage, get suggested params\n       ‚Üì       (or POST /api/platform/validate)\n1. Submit      POST /api/platform/submit        Submit raster or vector for processing\n       ‚Üì                                        (include previous_version_id for v2+)\n2. Poll        GET /api/platform/status/{id}    Monitor until completed\n       ‚Üì\n3. Approve     POST /api/platform/approve       Set clearance_level (ouo/public), approve dataset\n   (or Reject) POST /api/platform/reject        Reject with reason if data fails review\n       ‚Üì\n4. Access      {TITILER_URL}/stac/...           Query STAC catalog, get tiles\n       ‚Üì\n5. Revoke      POST /api/platform/revoke        Unapprove (required before unpublish)\n       ‚Üì                                        (or use force_approved=true in unpublish)\n6. Unpublish   POST /api/platform/unpublish     Delete data (STAC item, table/COG)\n\nRecovery:\n   Resubmit    POST /api/platform/resubmit      Clean up failed job and retry\n```\n\n## Asset State Model\n\nAssets have **three orthogonal state dimensions**:\n\n### 1. Processing Status (workflow execution)\n| Status | Description |\n|--------|-------------|\n| `pending` | Request received, job not started |\n| `processing` | Job running |\n| `completed` | Job finished successfully |\n| `failed` | Job failed (may retry via resubmit) |\n\n### 2. Approval State (data quality review)\n| State | Description | Next Actions |\n|-------|-------------|--------------|\n| `pending_review` | Job completed, awaiting approval | Approve or Reject |\n| `approved` | Data quality confirmed | Access data, Revoke, or Unpublish |\n| `rejected` | Rejected with reason | Resubmit after fixes |\n\n### 3. Clearance State (access control)\n| Level | Description |\n|-------|-------------|\n| `uncleared` | Default - same access as OUO, awaiting confirmation |\n| `ouo` | Official Use Only (internal access) |\n| `public` | Triggers ADF export to external zone |\n\n**Note:** `uncleared` and `ouo` have identical access behavior. The distinction is for B2B workflow confirmation.\n\n## Approval Record Status (Separate from Asset)\n\nThe `DatasetApproval` record tracks the approval workflow:\n\n| Status | Transition From | Description |\n|--------|-----------------|-------------|\n| `pending` | (initial) | Awaiting reviewer |\n| `approved` | `pending` | Approved for access |\n| `rejected` | `pending` | Rejected with reason |\n| `revoked` | `approved` | Unapproved (before unpublish) |\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": "## Setup & Configuration\n\n### Before You Start\n\n1. **Verify your test files exist** in Azure Storage:\n   - Vector: GeoJSON, Shapefile, GeoPackage, FlatGeobuf\n   - Raster: GeoTIFF (.tif)\n\n2. **Update the DDH identifiers** (dataset_id, resource_id, version_id) for your test data\n\n---\n\n### Configuration Steps\n\n**Step 1: Verify Environment URLs** (lines 11-17 below)\n\nThe QA environment URLs are pre-configured:\n- Platform API: `https://gddatahubetlqa-qa.ocappsaseqa2.appserviceenvironment.net`\n- Service Layer: `https://gddatahubpyfeqa.ocappsaseqa2.appserviceenvironment.net`\n\n**Step 2: Verify Storage Container** (line 23)\n\nQA bronze container: `gddathub-bronze-qa`\n\n**Step 3: Configure Test Data** (lines 29-46)\n\nUpdate the identifiers to match YOUR test files:\n\n```python\nTEST_VECTOR = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": \"roads.gpkg\",             # ‚Üê Your file path\n    \"dataset_id\": \"my-project\",            # ‚Üê Replace with your dataset ID\n    \"resource_id\": \"roads\",                # ‚Üê Replace with your resource ID\n    \"version_id\": \"v1\"                     # ‚Üê Replace with your version ID\n}\n```\n\n---\n\n### Quick Verification\n\nAfter running the setup cell, you should see:\n```\n============================================================\nPLATFORM API - QA ENVIRONMENT\n============================================================\n\nEndpoints:\n  Platform API:     https://gddatahubetlqa-qa...\n  Service Layer:    https://gddatahubpyfeqa...\n...\n```\n\n---\n\n### Run This Cell ‚Üì"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\nimport time\nfrom IPython.display import Markdown, display\n\n# =============================================================================\n# STEP 1: ENVIRONMENT URLS (QA)\n# =============================================================================\n\nENVIRONMENT = \"QA\"\n\n# Platform API - where you submit jobs and check status\nBASE_URL = \"https://gddatahubetlqa-qa.ocappsaseqa2.appserviceenvironment.net\"\n\n# Service Layer - where you access published data (STAC, tiles, features)\nTITILER_URL = \"https://gddatahubpyfeqa.ocappsaseqa2.appserviceenvironment.net\"\n\n# =============================================================================\n# STEP 2: STORAGE CONTAINER\n# =============================================================================\n\nBRONZE_CONTAINER = \"gddathub-bronze-qa\"\n\n# =============================================================================\n# STEP 3: TEST DATA FILES\n# =============================================================================\n# Update the identifier placeholders to match your test data\n\n# Vector test file - GeoPackage\nTEST_VECTOR = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": \"roads.gpkg\",\n    \"dataset_id\": \"your-dataset-id\",      # ‚Üê Replace with your dataset ID\n    \"resource_id\": \"your-resource-id\",    # ‚Üê Replace with your resource ID\n    \"version_id\": \"v1\"                    # ‚Üê Replace with your version ID\n}\n\n# Raster test file - GeoTIFF\nTEST_RASTER = {\n    \"container_name\": BRONZE_CONTAINER,\n    \"file_name\": \"dctest.tif\",\n    \"dataset_id\": \"your-dataset-id\",      # ‚Üê Replace with your dataset ID\n    \"resource_id\": \"your-resource-id\",    # ‚Üê Replace with your resource ID\n    \"version_id\": \"v1\"                    # ‚Üê Replace with your version ID\n}\n\n# =============================================================================\n# HELPER FUNCTIONS (Don't modify)\n# =============================================================================\n\ndef api_call(method, endpoint, data=None, params=None, timeout=30):\n    \"\"\"Make API call and return formatted response.\"\"\"\n    url = f\"{BASE_URL}{endpoint}\"\n    headers = {\"Content-Type\": \"application/json\"}\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"{method} {endpoint}\")\n    print(f\"{'='*60}\")\n    \n    if data:\n        print(f\"\\nRequest Body:\")\n        print(json.dumps(data, indent=2))\n    \n    try:\n        if method == \"GET\":\n            response = requests.get(url, params=params, timeout=timeout)\n        elif method == \"POST\":\n            response = requests.post(url, json=data, headers=headers, timeout=timeout)\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n        \n        print(f\"\\nStatus: {response.status_code}\")\n        \n        try:\n            result = response.json()\n            print(f\"\\nResponse:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        except:\n            print(f\"\\nResponse (text): {response.text[:500]}\")\n            return {\"error\": response.text, \"status_code\": response.status_code}\n            \n    except requests.exceptions.Timeout:\n        print(f\"\\nRequest timed out (timeout={timeout}s)\")\n        return None\n    except requests.exceptions.ConnectionError:\n        print(f\"\\n‚ùå CONNECTION ERROR - Check your network connection\")\n        print(f\"   URL: {url}\")\n        return None\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        return None\n\n\ndef poll_status(request_id, max_polls=30, poll_interval=5):\n    \"\"\"Poll platform status until job completes.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Polling request: {request_id}\")\n    print(f\"{'='*60}\")\n    \n    for i in range(max_polls):\n        try:\n            result = requests.get(\n                f\"{BASE_URL}/api/platform/status/{request_id}\", \n                timeout=30\n            ).json()\n        except:\n            print(f\"  [{i+1}/{max_polls}] Connection error - retrying...\")\n            time.sleep(poll_interval)\n            continue\n        \n        job_status = result.get(\"job_status\", \"unknown\")\n        job_stage = result.get(\"job_stage\", \"?\")\n        job_id = result.get(\"job_id\", \"N/A\")[:16] if result.get(\"job_id\") else \"N/A\"\n        \n        print(f\"  [{i+1}/{max_polls}] Status: {job_status}, Stage: {job_stage}, Job: {job_id}...\")\n        \n        if job_status in [\"completed\", \"failed\"]:\n            print(f\"\\nFinal Result:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        \n        time.sleep(poll_interval)\n    \n    print(f\"\\nPolling timeout after {max_polls * poll_interval}s\")\n    return result\n\n\n# =============================================================================\n# CONFIGURATION CHECK\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(f\"PLATFORM API - {ENVIRONMENT} ENVIRONMENT\")\nprint(\"=\" * 60)\n\nprint(f\"\\nEndpoints:\")\nprint(f\"  Platform API:     {BASE_URL}\")\nprint(f\"  Service Layer:    {TITILER_URL}\")\nprint(f\"  API Docs:         {TITILER_URL}/docs\")\nprint(f\"\\nStorage:\")\nprint(f\"  Bronze Container: {BRONZE_CONTAINER}\")\nprint(f\"\\nTest Data:\")\nprint(f\"  Vector: {TEST_VECTOR['file_name']}\")\nprint(f\"  Raster: {TEST_RASTER['file_name']}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Platform Health\n",
    "\n",
    "Check if the platform is ready to accept jobs.\n",
    "\n",
    "**Endpoint:** `GET /api/platform/health`\n",
    "\n",
    "Returns simplified health status for external applications (no internal details exposed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform Health Check\n",
    "result = api_call(\"GET\", \"/api/platform/health\")\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PLATFORM STATUS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Ready for Jobs: {result.get('ready_for_jobs', 'unknown')}\")\n",
    "    print(f\"Queue Backlog:  {result.get('queue_backlog', 'unknown')}\")\n",
    "    print(f\"Version:        {result.get('version', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Complete Platform Workflow\n",
    "\n",
    "Full lifecycle: Validate ‚Üí Submit ‚Üí Poll ‚Üí Approve ‚Üí Unpublish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validate-header",
   "metadata": {},
   "source": "## 2.1 Pre-flight Validation & Version Lineage\n\nValidate request before submitting and check version lineage state.\n\n**Two equivalent options:**\n- `POST /api/platform/validate` - Standalone validation endpoint\n- `POST /api/platform/submit?dry_run=true` - Submit with dry_run parameter\n\nBoth return identical responses including **lineage state** for version tracking.\n\n### Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `dataset_id` | body | Yes | DDH dataset identifier |\n| `resource_id` | body | Yes | DDH resource identifier |\n| `version_id` | body | Yes | Version identifier (e.g., `v2.0`) |\n| `container_name` | body | Yes | Storage container |\n| `file_name` | body | Yes | Path to file |\n| `previous_version_id` | body | For v2+ | Must match current latest version |\n\n### Response includes:\n\n- `valid`: Whether submission would succeed\n- `lineage_state`: Current version chain info\n- `suggested_params`: Recommended parameters (e.g., `previous_version_id`)\n- `warnings`: Validation messages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PRE-FLIGHT VALIDATION WITH DRY_RUN\n# =============================================================================\n# Uses POST /api/platform/submit?dry_run=true to check lineage and validate\n# This returns the same response as POST /api/platform/validate\n\ntest_file = TEST_RASTER  # or TEST_VECTOR\n\nvalidate_request = {\n    \"dataset_id\": test_file[\"dataset_id\"],\n    \"resource_id\": test_file[\"resource_id\"],\n    \"version_id\": test_file[\"version_id\"],\n    \"container_name\": test_file[\"container_name\"],\n    \"file_name\": test_file[\"file_name\"]\n    # Note: previous_version_id NOT included - let validation tell us what to set\n}\n\n# Use dry_run=true query parameter\nresult = api_call(\"POST\", \"/api/platform/submit?dry_run=true\", validate_request)\n\nif result:\n    is_valid = result.get(\"valid\", False)\n    lineage = result.get(\"lineage_state\", {})\n    suggested = result.get(\"suggested_params\", {})\n    warnings = result.get(\"warnings\", [])\n    \n    print(f\"\\n{'='*60}\")\n    print(\"VALIDATION RESULT\")\n    print(f\"{'='*60}\")\n    print(f\"Valid: {is_valid}\")\n    print(f\"Would create job type: {result.get('would_create_job_type', 'N/A')}\")\n    \n    if lineage:\n        print(f\"\\nLineage State:\")\n        print(f\"  Lineage ID: {lineage.get('lineage_id', 'N/A')[:16]}...\")\n        print(f\"  Lineage exists: {lineage.get('lineage_exists', False)}\")\n        if lineage.get(\"current_latest\"):\n            latest = lineage[\"current_latest\"]\n            print(f\"  Current latest version: {latest.get('version_id')}\")\n            print(f\"  Version ordinal: {latest.get('version_ordinal')}\")\n    \n    if suggested:\n        print(f\"\\nSuggested Parameters:\")\n        for key, value in suggested.items():\n            print(f\"  {key}: {value}\")\n    \n    if warnings:\n        print(f\"\\nWarnings:\")\n        for w in warnings:\n            print(f\"  ‚ö†Ô∏è {w}\")\n    \n    if is_valid:\n        print(f\"\\n‚úÖ Ready to submit\")\n    else:\n        print(f\"\\n‚ùå Not valid - check warnings/errors above\")"
  },
  {
   "cell_type": "markdown",
   "id": "submit-header",
   "metadata": {},
   "source": "## 2.2 Submit Data\n\nSubmit raster or vector data for processing.\n\n**Endpoint:** `POST /api/platform/submit`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `dataset_id` | body | Yes | Top-level identifier (e.g., `aerial-imagery`) |\n| `resource_id` | body | Yes | Resource identifier (e.g., `site-alpha`) |\n| `version_id` | body | Yes | Version identifier (e.g., `v1.0`) |\n| `container_name` | body | Yes | Source container with raw file |\n| `file_name` | body | Yes | Blob path to file |\n| `previous_version_id` | body | **For v2+** | Must match current latest version |\n| `title` | body | No | Human-readable name |\n| `access_level` | body | No | `ouo` (default) or `public` |\n\n### Version Lineage Rules\n\n| Scenario | `previous_version_id` | Result |\n|----------|----------------------|--------|\n| First version (v1.0) | Omit | ‚úÖ Creates lineage |\n| New version (v2.0) after v1.0 | `\"v1.0\"` | ‚úÖ Advances lineage |\n| New version without prev | Omit | ‚ùå Error: specify previous |\n| Wrong previous_version_id | `\"v0.5\"` | ‚ùå Error: doesn't match latest |\n\n**Note:** Data type is auto-detected from file extension."
  },
  {
   "cell_type": "markdown",
   "id": "pa4b28trjj",
   "source": "## 2.1b Version Lineage Example: v1.0 ‚Üí v2.0\n\nThis example demonstrates submitting a new version when a previous version exists.\n\n**Workflow:**\n1. Validate to check lineage state (discovers v1.0 exists)\n2. Submit v2.0 with `previous_version_id=\"v1.0\"`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z9mb0tcbug",
   "source": "# =============================================================================\n# VERSION LINEAGE EXAMPLE: Submit v2.0 after v1.0\n# =============================================================================\n# This example shows how to properly advance a version\n\n# Step 1: Define v2.0 submission (same dataset/resource, new version)\nv2_request = {\n    \"dataset_id\": TEST_RASTER[\"dataset_id\"],\n    \"resource_id\": TEST_RASTER[\"resource_id\"],\n    \"version_id\": \"v2.0\",  # New version\n    \"container_name\": TEST_RASTER[\"container_name\"],\n    \"file_name\": TEST_RASTER[\"file_name\"]  # Could be different file\n}\n\n# Step 2: Validate first to check lineage\nprint(\"Step 1: Checking lineage state...\")\nresult = api_call(\"POST\", \"/api/platform/submit?dry_run=true\", v2_request)\n\nif result:\n    is_valid = result.get(\"valid\", False)\n    suggested = result.get(\"suggested_params\", {})\n    \n    if is_valid:\n        print(f\"\\n‚úÖ Valid! Ready to submit v2.0\")\n    else:\n        # Get suggested previous_version_id from response\n        prev_version = suggested.get(\"previous_version_id\")\n        if prev_version:\n            print(f\"\\n‚ö†Ô∏è Lineage exists - need to specify previous_version_id\")\n            print(f\"   Suggested: previous_version_id='{prev_version}'\")\n            \n            # Step 3: Add previous_version_id and submit\n            print(f\"\\nStep 2: Adding previous_version_id and submitting...\")\n            v2_request[\"previous_version_id\"] = prev_version\n            \n            # Uncomment to actually submit:\n            # result = api_call(\"POST\", \"/api/platform/submit\", v2_request)\n            # if result and result.get(\"request_id\"):\n            #     print(f\"‚úÖ v2.0 submitted! Request ID: {result['request_id']}\")\n            \n            print(f\"\\n   To submit, uncomment the code above and run again\")\n        else:\n            print(f\"\\n‚ùå Validation failed: {result.get('warnings', [])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "submit",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# SUBMIT DATA - Choose ONE: Raster OR Vector\n# =============================================================================\n\n# Option A: Submit RASTER\nsubmit_request = {\n    \"dataset_id\": TEST_RASTER[\"dataset_id\"],\n    \"resource_id\": TEST_RASTER[\"resource_id\"],\n    \"version_id\": TEST_RASTER[\"version_id\"],\n    \"container_name\": TEST_RASTER[\"container_name\"],\n    \"file_name\": TEST_RASTER[\"file_name\"],\n    \"access_level\": \"ouo\"\n    # For subsequent versions, add:\n    # \"previous_version_id\": \"v1.0\"  # Must match current latest\n}\n\n# Option B: Submit VECTOR (uncomment to use instead)\n# submit_request = {\n#     \"dataset_id\": TEST_VECTOR[\"dataset_id\"],\n#     \"resource_id\": TEST_VECTOR[\"resource_id\"],\n#     \"version_id\": TEST_VECTOR[\"version_id\"],\n#     \"container_name\": TEST_VECTOR[\"container_name\"],\n#     \"file_name\": TEST_VECTOR[\"file_name\"],\n#     \"access_level\": \"ouo\"\n#     # For subsequent versions, add:\n#     # \"previous_version_id\": \"v1.0\"  # Must match current latest\n# }\n\n# =============================================================================\n\nresult = api_call(\"POST\", \"/api/platform/submit\", submit_request)\n\n# Store request_id for subsequent steps\nREQUEST_ID = result.get(\"request_id\") if result else None\n\nif REQUEST_ID:\n    print(f\"\\n‚úÖ Submitted successfully\")\n    print(f\"   Request ID: {REQUEST_ID}\")\n    print(f\"   Job ID:     {result.get('job_id', 'N/A')[:16]}...\")\n    print(f\"   Job Type:   {result.get('job_type', 'N/A')}\")\n    print(f\"\\n   Use REQUEST_ID for all subsequent operations\")\nelse:\n    # Check if this was a validation error (lineage issue)\n    if result and result.get(\"error_type\") == \"ValidationError\":\n        print(f\"\\n‚ùå Validation failed: {result.get('error')}\")\n        print(f\"\\n   Hint: Run the validation cell first to check lineage state\")\n    else:\n        print(f\"\\n‚ùå Submit failed\")"
  },
  {
   "cell_type": "markdown",
   "id": "poll-header",
   "metadata": {},
   "source": [
    "## 2.3 Poll Status\n",
    "\n",
    "Monitor job progress using the request ID.\n",
    "\n",
    "**Endpoint:** `GET /api/platform/status/{request_id}`\n",
    "\n",
    "Returns job status, stage progress, and result data when completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll Status by Request ID\n",
    "if REQUEST_ID:\n",
    "    result = poll_status(REQUEST_ID, max_polls=30, poll_interval=5)\n",
    "    \n",
    "    if result and result.get(\"job_status\") == \"completed\":\n",
    "        print(f\"\\n‚úÖ Job completed successfully\")\n",
    "        # Store for approval step\n",
    "        JOB_ID = result.get(\"job_id\")\n",
    "    elif result and result.get(\"job_status\") == \"failed\":\n",
    "        print(f\"\\n‚ùå Job failed\")\n",
    "        JOB_ID = None\n",
    "else:\n",
    "    print(\"No REQUEST_ID - run Submit cell first\")\n",
    "    JOB_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approve-header",
   "metadata": {},
   "source": "## 2.4 Approve or Reject Dataset\n\nAfter a job completes, approve it for publication or reject if it fails review.\n\n### Approve\n\n**Endpoint:** `POST /api/platform/approve`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `job_id` | body | Yes* | Job ID from completed job |\n| `clearance_level` | body | **Yes** | `ouo` or `public` |\n| `reviewer` | body | Yes | Email of approver |\n| `notes` | body | No | Approval notes |\n\n*Can also use `request_id` instead of `job_id`\n\n### Reject (Alternative - No Live Example)\n\nIf the data fails review, reject it instead:\n\n**Endpoint:** `POST /api/platform/reject`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `job_id` | body | Yes* | Job ID from completed job |\n| `reviewer` | body | Yes | Email of reviewer |\n| `reason` | body | **Yes** | Why rejected (audit trail) |\n\n```python\n# Example reject request (not executed)\nreject_request = {\n    \"job_id\": JOB_ID,\n    \"reviewer\": \"qa-tester@example.com\",\n    \"reason\": \"Data quality issue: missing CRS metadata\"\n}\n# result = api_call(\"POST\", \"/api/platform/reject\", reject_request)\n```\n\nRejected datasets can be resubmitted after fixes via `POST /api/platform/resubmit`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approve Dataset\n",
    "if JOB_ID:\n",
    "    approve_request = {\n",
    "        \"job_id\": JOB_ID,\n",
    "        \"clearance_level\": \"ouo\",  # REQUIRED: \"ouo\" or \"public\"\n",
    "        \"reviewer\": \"qa-tester@example.com\",\n",
    "        \"notes\": \"QA test approval\"\n",
    "    }\n",
    "    \n",
    "    result = api_call(\"POST\", \"/api/platform/approve\", approve_request)\n",
    "    \n",
    "    if result and result.get(\"success\"):\n",
    "        print(f\"\\n‚úÖ Dataset approved\")\n",
    "        print(f\"   Approval ID: {result.get('approval_id', 'N/A')}\")\n",
    "        print(f\"   Status: {result.get('status', 'N/A')}\")\n",
    "        print(f\"   Clearance: {result.get('clearance_level', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Approval failed\")\n",
    "else:\n",
    "    print(\"No JOB_ID - run Poll cell first (job must be completed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unpublish-header",
   "metadata": {},
   "source": "## 2.5 Revoke and Unpublish Dataset\n\nTo remove an **approved** dataset, you must either revoke first OR use `force_approved=true`.\n\n### Option 1: Two-Step (Explicit Audit Trail)\n\n```\nStep 1: POST /api/platform/revoke     ‚Üí Unapproves the dataset\nStep 2: POST /api/platform/unpublish  ‚Üí Deletes the data\n```\n\n### Option 2: Force Unpublish (Convenience)\n\n```\nPOST /api/platform/unpublish with force_approved=true\n```\nThis auto-revokes approved data before deleting.\n\n---\n\n### Step 1: Revoke Approval (Optional)\n\n**Endpoint:** `POST /api/platform/revoke`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `approval_id` | body | Yes* | Approval ID |\n| `revoker` | body | Yes | Email of person revoking |\n| `reason` | body | **Yes** | Why revoking (audit trail) |\n\n*Can also use `stac_item_id` or `job_id`\n\n### Step 2: Unpublish Data\n\n**Endpoint:** `POST /api/platform/unpublish`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `dataset_id` | body | Yes* | Original dataset_id |\n| `resource_id` | body | Yes* | Original resource_id |\n| `version_id` | body | Yes* | Original version_id |\n| `dry_run` | body | No | `true` (default) = preview only |\n| `force_approved` | body | No | `true` = auto-revoke approved data |\n\n*Can also use `request_id` or `job_id` instead of DDH identifiers.\n\n**Safety:** Always defaults to `dry_run=true`. Set to `false` to actually delete."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unpublish",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# STEP 1: REVOKE APPROVAL (Required before unpublish)\n# =============================================================================\n# This unapproves the dataset and creates an audit trail\n# Must be called before unpublish for approved datasets\n\nif JOB_ID:\n    revoke_request = {\n        \"job_id\": JOB_ID,\n        \"revoker\": \"qa-tester@example.com\",\n        \"reason\": \"QA test cleanup - removing test data\"  # Required for audit\n    }\n    \n    result = api_call(\"POST\", \"/api/platform/revoke\", revoke_request)\n    \n    if result and result.get(\"success\"):\n        print(f\"\\n‚úÖ Approval revoked\")\n        print(f\"   Approval ID: {result.get('approval_id', 'N/A')}\")\n        print(f\"   Status: {result.get('status', 'N/A')}\")\n        print(f\"   Asset deleted: {result.get('asset_deleted', 'N/A')}\")\n        if result.get(\"warning\"):\n            print(f\"   ‚ö†Ô∏è  {result.get('warning')}\")\n        print(f\"\\n   ‚û°Ô∏è  Now run the Unpublish cell below to delete data\")\n    else:\n        error = result.get('error', 'Unknown error') if result else 'No response'\n        print(f\"\\n‚ùå Revoke failed: {error}\")\n        if \"not found\" in str(error).lower():\n            print(\"   Hint: Check that the job was approved first\")\n        elif \"approved\" in str(error).lower():\n            print(\"   Hint: Only approved datasets can be revoked\")\nelse:\n    print(\"No JOB_ID available\")\n    print(\"\\nTo revoke manually, use one of these options:\")\n    print(\"\"\"\n    # Option 1: By Job ID\n    revoke_request = {\n        \"job_id\": \"your-job-id-here\",\n        \"revoker\": \"your-email@example.com\",\n        \"reason\": \"Reason for revocation\"\n    }\n    \n    # Option 2: By Approval ID\n    revoke_request = {\n        \"approval_id\": \"apr-xxxxx\",\n        \"revoker\": \"your-email@example.com\", \n        \"reason\": \"Reason for revocation\"\n    }\n    \n    # Option 3: By Request ID\n    revoke_request = {\n        \"request_id\": \"your-request-id\",\n        \"revoker\": \"your-email@example.com\",\n        \"reason\": \"Reason for revocation\"\n    }\n    \"\"\")"
  },
  {
   "cell_type": "code",
   "id": "mewrq6x4vt",
   "source": "# =============================================================================\n# STEP 2: UNPUBLISH DATA (Run after revoking)\n# =============================================================================\n# Deletes the actual data: STAC item, PostGIS table (vector), or COG files (raster)\n# Only run this after successfully revoking the approval\n\ntest_file = TEST_RASTER  # or TEST_VECTOR\n\nunpublish_request = {\n    \"dataset_id\": test_file[\"dataset_id\"],\n    \"resource_id\": test_file[\"resource_id\"],\n    \"version_id\": test_file[\"version_id\"],\n    \"dry_run\": True  # SAFETY: Set to False to actually delete\n}\n\nresult = api_call(\"POST\", \"/api/platform/unpublish\", unpublish_request)\n\nif result:\n    if result.get(\"dry_run\"):\n        print(f\"\\nüîç DRY RUN - No changes made\")\n        print(f\"   Would delete: {result.get('would_delete', {})}\")\n        print(f\"\\n   Set dry_run=False to actually delete\")\n    else:\n        print(f\"\\n‚úÖ Dataset unpublished\")\n        print(f\"   Deleted: {result.get('deleted', {})}\")\nelse:\n    print(f\"\\n‚ùå Unpublish failed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": "---\n# Part 3: Catalog Discovery\n\nQuery published assets and STAC items through the Platform API.\n\n## Catalog Endpoints\n\n| Endpoint | Purpose |\n|----------|---------|\n| `GET /api/platform/catalog/lookup` | Lookup by DDH IDs (dataset/resource/version) |\n| `GET /api/platform/catalog/assets` | List all assets with filters |\n| `GET /api/platform/catalog/item/{col}/{item}` | Get full STAC item |\n| `GET /api/platform/catalog/assets/{col}/{item}` | Get asset URLs with TiTiler links |\n| `GET /api/platform/catalog/dataset/{id}` | List all items for a dataset |"
  },
  {
   "cell_type": "markdown",
   "id": "catalog-header",
   "metadata": {},
   "source": "## 3.1 List Assets\n\n**Endpoint:** `GET /api/platform/catalog/assets`\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `limit` | query | Max results (default: 100) |\n| `approval_state` | query | Filter: `pending_review`, `approved`, `rejected` |\n| `clearance_state` | query | Filter: `uncleared`, `ouo`, `public` |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog-list",
   "metadata": {},
   "outputs": [],
   "source": "# List Assets\nresult = api_call(\"GET\", \"/api/platform/catalog/assets\", params={\"limit\": 10})\n\nif result and result.get(\"assets\"):\n    assets = result[\"assets\"]\n    print(f\"\\nFound {len(assets)} assets:\")\n    for asset in assets[:5]:\n        print(f\"  - {asset.get('dataset_id')}/{asset.get('resource_id')}/{asset.get('version_id')}\")\n        print(f\"    Approval: {asset.get('approval_state')}, Clearance: {asset.get('clearance_state')}\")"
  },
  {
   "cell_type": "markdown",
   "id": "lookup-header",
   "metadata": {},
   "source": [
    "## 3.2 Lookup by DDH Identifiers\n",
    "\n",
    "**Endpoint:** `GET /api/platform/catalog/lookup`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | query | Yes | Dataset identifier |\n",
    "| `resource_id` | query | Yes | Resource identifier |\n",
    "| `version_id` | query | No | Specific version (omit for latest) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catalog-lookup",
   "metadata": {},
   "outputs": [],
   "source": "# Lookup Asset by DDH Identifiers\ntest_file = TEST_RASTER  # or TEST_VECTOR\n\nparams = {\n    \"dataset_id\": test_file[\"dataset_id\"],\n    \"resource_id\": test_file[\"resource_id\"],\n    \"version_id\": test_file[\"version_id\"]\n}\n\nresult = api_call(\"GET\", \"/api/platform/catalog/lookup\", params=params)\n\nif result and result.get(\"found\"):\n    print(f\"\\n‚úÖ Asset found\")\n    print(f\"   STAC Item: {result.get('stac_item_id', 'N/A')}\")\n    print(f\"   Collection: {result.get('stac_collection_id', 'N/A')}\")\n    print(f\"   Approval: {result.get('approval_state', 'N/A')}\")\n    print(f\"   Clearance: {result.get('clearance_state', 'N/A')}\")\nelse:\n    print(f\"\\n‚ùå Asset not found\")"
  },
  {
   "cell_type": "markdown",
   "id": "avqmypbtrhu",
   "source": "## 3.3 Get STAC Item Details\n\n**Endpoint:** `GET /api/platform/catalog/item/{collection_id}/{item_id}`\n\nReturns the full STAC item as GeoJSON Feature.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jll789zkh2",
   "source": "# Get STAC Item Details\n# Requires knowing collection_id and item_id from lookup or asset list\n\n# Replace with your actual IDs (from lookup results)\nCOLLECTION_ID = \"your-dataset-id\"  # Usually same as dataset_id\nITEM_ID = \"your-dataset-id-resource-id-v1\"  # Format: {dataset}-{resource}-{version}\n\nresult = api_call(\"GET\", f\"/api/platform/catalog/item/{COLLECTION_ID}/{ITEM_ID}\")\n\nif result and result.get(\"type\") == \"Feature\":\n    props = result.get(\"properties\", {})\n    print(f\"\\n‚úÖ STAC Item retrieved\")\n    print(f\"   ID: {result.get('id')}\")\n    print(f\"   Collection: {result.get('collection')}\")\n    print(f\"   Datetime: {props.get('datetime', 'N/A')}\")\n    print(f\"   BBox: {result.get('bbox', 'N/A')}\")\n    assets = result.get(\"assets\", {})\n    print(f\"   Assets: {list(assets.keys())}\")\nelif result:\n    print(f\"\\n‚ùå Item not found or error: {result.get('error', 'Unknown')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rq57viescml",
   "source": "## 3.4 Get Asset URLs with TiTiler\n\n**Endpoint:** `GET /api/platform/catalog/assets/{collection_id}/{item_id}`\n\nReturns asset URLs with pre-built TiTiler tile/preview links.\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `include_titiler` | query | `true` = include TiTiler URLs for rasters |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "n1fdrmlptuj",
   "source": "# Get Asset URLs with TiTiler Links\n# Useful for raster data - provides ready-to-use tile URLs\n\n# Replace with your actual IDs\nCOLLECTION_ID = \"your-dataset-id\"\nITEM_ID = \"your-dataset-id-resource-id-v1\"\n\nresult = api_call(\n    \"GET\", \n    f\"/api/platform/catalog/assets/{COLLECTION_ID}/{ITEM_ID}\",\n    params={\"include_titiler\": \"true\"}\n)\n\nif result and result.get(\"assets\"):\n    print(f\"\\n‚úÖ Asset URLs retrieved\")\n    print(f\"   Item: {result.get('item_id')}\")\n    print(f\"   BBox: {result.get('bbox', 'N/A')}\")\n    \n    # Show asset links\n    assets = result.get(\"assets\", {})\n    for name, asset in assets.items():\n        print(f\"\\n   Asset '{name}':\")\n        print(f\"     href: {asset.get('href', 'N/A')[:60]}...\")\n        print(f\"     type: {asset.get('type', 'N/A')}\")\n    \n    # Show TiTiler links (for rasters)\n    titiler = result.get(\"titiler\", {})\n    if titiler:\n        print(f\"\\n   TiTiler Links:\")\n        print(f\"     preview: {titiler.get('preview', 'N/A')[:60]}...\")\n        print(f\"     tiles:   {titiler.get('tiles', 'N/A')[:60]}...\")\nelif result:\n    print(f\"\\n‚ùå Error: {result.get('error', 'Unknown')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3h3d3333opl",
   "source": "## 3.5 List Items for Dataset\n\n**Endpoint:** `GET /api/platform/catalog/dataset/{dataset_id}`\n\nReturns all STAC items for a given dataset (across all resources and versions).\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `limit` | query | Max results (default: 50) |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gax0yvgz0k",
   "source": "# List All Items for a Dataset\nDATASET_ID = TEST_RASTER[\"dataset_id\"]  # or TEST_VECTOR[\"dataset_id\"]\n\nresult = api_call(\n    \"GET\", \n    f\"/api/platform/catalog/dataset/{DATASET_ID}\",\n    params={\"limit\": 50}\n)\n\nif result and result.get(\"items\"):\n    items = result[\"items\"]\n    print(f\"\\n‚úÖ Found {result.get('count', len(items))} items for dataset '{DATASET_ID}'\")\n    \n    for item in items[:10]:  # Show first 10\n        print(f\"\\n   {item.get('item_id')}:\")\n        print(f\"     Resource: {item.get('resource_id')}\")\n        print(f\"     Version:  {item.get('version_id')}\")\n        print(f\"     Datetime: {item.get('datetime', 'N/A')}\")\nelif result:\n    if result.get(\"count\") == 0:\n        print(f\"\\n‚ö†Ô∏è No items found for dataset '{DATASET_ID}'\")\n    else:\n        print(f\"\\n‚ùå Error: {result.get('error', 'Unknown')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rodpnrtmx0p",
   "source": "---\n# Part 3.5: Platform Registry\n\nQuery supported B2B platforms and their identifier requirements.\n\n## Endpoints\n\n| Endpoint | Purpose |\n|----------|---------|\n| `GET /api/platforms` | List all supported platforms |\n| `GET /api/platforms/{id}` | Get specific platform details |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ni7ooluq7us",
   "source": "# List Supported Platforms\nresult = api_call(\"GET\", \"/api/platforms\")\n\nif result and result.get(\"platforms\"):\n    platforms = result[\"platforms\"]\n    print(f\"\\n‚úÖ Found {result.get('count', len(platforms))} platform(s)\")\n    \n    for p in platforms:\n        print(f\"\\n   Platform: {p.get('platform_id')}\")\n        print(f\"     Name: {p.get('display_name')}\")\n        print(f\"     Required refs: {p.get('required_refs')}\")\n        print(f\"     Optional refs: {p.get('optional_refs')}\")\n        print(f\"     Active: {p.get('is_active')}\")\nelif result:\n    print(f\"\\n‚ùå Error: {result.get('error', 'Unknown')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Data Access\n",
    "\n",
    "Access published data through the consolidated service layer.\n",
    "\n",
    "**Base URL:** `TITILER_URL`\n",
    "\n",
    "**Full API Documentation:** `{TITILER_URL}/docs`\n",
    "\n",
    "## Service Layer Endpoints\n",
    "\n",
    "| Path Prefix | Service | Purpose |\n",
    "|-------------|---------|---------- |\n",
    "| `/stac/` | STAC API | Catalog search, collections, items |\n",
    "| `/vector/` | TiPG (OGC Features) | Vector collections, features, vector tiles |\n",
    "| `/cog/` | TiTiler COG | Raster tiles, info, statistics |\n",
    "| `/searches/` | Mosaic Search | Dynamic mosaic tiles from STAC searches |\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# STAC Catalog\n",
    "f\"{TITILER_URL}/stac/collections\"                    # List collections\n",
    "f\"{TITILER_URL}/stac/search\"                         # Search items (POST)\n",
    "f\"{TITILER_URL}/stac/collections/{id}/items/{item}\"  # Get item\n",
    "\n",
    "# Vector Features (OGC API)\n",
    "f\"{TITILER_URL}/vector/collections\"                  # List vector collections\n",
    "f\"{TITILER_URL}/vector/collections/{id}/items\"       # Get features\n",
    "f\"{TITILER_URL}/vector/collections/{id}/tiles/...\"   # Vector tiles (MVT)\n",
    "\n",
    "# Raster Tiles (COG)\n",
    "f\"{TITILER_URL}/cog/info?url={cog_url}\"              # COG metadata\n",
    "f\"{TITILER_URL}/cog/tiles/{z}/{x}/{y}?url={cog_url}\" # XYZ tiles\n",
    "f\"{TITILER_URL}/cog/statistics?url={cog_url}\"        # Band statistics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Access Quick Links\n",
    "print(f\"{'='*60}\")\n",
    "print(\"DATA ACCESS ENDPOINTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nAPI Documentation:\")\n",
    "print(f\"  {TITILER_URL}/docs\")\n",
    "print(f\"\\nSTAC Catalog:\")\n",
    "print(f\"  {TITILER_URL}/stac/collections\")\n",
    "print(f\"  {TITILER_URL}/stac/search\")\n",
    "print(f\"\\nVector Features:\")\n",
    "print(f\"  {TITILER_URL}/vector/collections\")\n",
    "print(f\"\\nHealth Checks:\")\n",
    "print(f\"  {TITILER_URL}/health\")\n",
    "print(f\"  {TITILER_URL}/stac/_mgmt/ping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix-header",
   "metadata": {},
   "source": "---\n# Appendix: Job Recovery\n\n## A.1 Resubmit Failed Job\n\nClean up and resubmit a failed job with the same parameters. Useful for retrying jobs that failed due to transient errors.\n\n**Endpoint:** `POST /api/platform/resubmit`\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `dataset_id` | body | - | DDH dataset identifier (Option 1) |\n| `resource_id` | body | - | DDH resource identifier (Option 1) |\n| `version_id` | body | - | DDH version identifier (Option 1) |\n| `request_id` | body | - | Platform request ID (Option 2) |\n| `job_id` | body | - | CoreMachine job ID (Option 3) |\n| `dry_run` | body | `false` | Preview cleanup plan without executing |\n| `delete_blobs` | body | `false` | Also delete COG files from storage |\n| `force` | body | `false` | Resubmit even if job is processing |\n\n**Identifier Options (choose one):**\n1. DDH Identifiers: `dataset_id` + `resource_id` + `version_id` (preferred)\n2. Request ID: `request_id` from original submit\n3. Job ID: `job_id` from CoreMachine\n\n**Response includes:**\n- `original_job_id`: The job that was cleaned up\n- `new_job_id`: The newly submitted job\n- `cleanup_summary`: What was deleted (tasks, STAC items, tables)\n- `monitor_url`: URL to poll for new job status"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resubmit",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# RESUBMIT FAILED JOB\n# =============================================================================\n# Choose ONE identifier option below\n\n# Option 1: By DDH Identifiers (Preferred)\n# Use the same identifiers from the original submit\ntest_file = TEST_RASTER  # or TEST_VECTOR\n\nresubmit_request = {\n    \"dataset_id\": test_file[\"dataset_id\"],\n    \"resource_id\": test_file[\"resource_id\"],\n    \"version_id\": test_file[\"version_id\"],\n    \"dry_run\": True,        # Preview first\n    \"delete_blobs\": False   # Keep COG files\n}\n\n# Option 2: By Request ID (uncomment to use)\n# resubmit_request = {\n#     \"request_id\": REQUEST_ID,  # From original submit\n#     \"dry_run\": True\n# }\n\n# Option 3: By Job ID (uncomment to use)\n# resubmit_request = {\n#     \"job_id\": \"YOUR_JOB_ID_HERE\",\n#     \"dry_run\": True\n# }\n\n# =============================================================================\n\nresult = api_call(\"POST\", \"/api/platform/resubmit\", resubmit_request)\n\nif result and result.get(\"success\"):\n    if result.get(\"dry_run\"):\n        print(f\"\\nüîç DRY RUN - Cleanup Plan:\")\n        plan = result.get(\"cleanup_plan\", {})\n        print(f\"   Tasks to delete: {plan.get('tasks_to_delete', 0)}\")\n        print(f\"   STAC items: {plan.get('stac_items_to_delete', [])}\")\n        print(f\"   Tables to drop: {plan.get('tables_to_drop', [])}\")\n        print(f\"\\n   Set dry_run=False to execute resubmit\")\n    else:\n        print(f\"\\n‚úÖ Job resubmitted successfully\")\n        print(f\"   Original Job: {result.get('original_job_id', 'N/A')[:16]}...\")\n        print(f\"   New Job:      {result.get('new_job_id', 'N/A')[:16]}...\")\n        print(f\"   Monitor URL:  {result.get('monitor_url', 'N/A')}\")\n        \n        # Store new job for polling\n        NEW_JOB_ID = result.get(\"new_job_id\")\nelif result:\n    print(f\"\\n‚ùå Resubmit failed: {result.get('error', 'Unknown error')}\")"
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": "---\n\n## Additional Resources\n\n- **Platform API Health:** `GET /api/platform/health`\n- **Platform Failures:** `GET /api/platform/failures?hours=24`\n- **Data Lineage:** `GET /api/platform/lineage/{request_id}`\n- **Pending Approvals:** `GET /api/platform/approvals`\n- **Reject Dataset:** `POST /api/platform/reject`\n- **Revoke Approval:** `POST /api/platform/revoke`\n- **Resubmit Failed Job:** `POST /api/platform/resubmit`\n- **Platform Registry:** `GET /api/platforms`\n\n## Version Lineage Quick Reference\n\n| Scenario | `previous_version_id` | Result |\n|----------|----------------------|--------|\n| First version | Omit | ‚úÖ Creates new lineage |\n| New version | Set to current latest | ‚úÖ Advances lineage |\n| New version | Omitted when lineage exists | ‚ùå Error with suggestion |\n| Wrong previous | Doesn't match latest | ‚ùå Error with correct value |\n\n**Tip:** Always run `POST /api/platform/submit?dry_run=true` first to check lineage state.\n\n## State Reference\n\n### Asset State Dimensions (GeospatialAsset)\n\n| Dimension | Values | Set By |\n|-----------|--------|--------|\n| Processing Status | `pending`, `processing`, `completed`, `failed` | System (automatic) |\n| Approval State | `pending_review`, `approved`, `rejected` | Reviewer (approve/reject) |\n| Clearance State | `uncleared`, `ouo`, `public` | Reviewer (during approval) |\n\n### Approval Record Status (DatasetApproval)\n\n| Status | Description |\n|--------|-------------|\n| `pending` | Awaiting reviewer decision |\n| `approved` | Approved for access |\n| `rejected` | Rejected with reason |\n| `revoked` | Previously approved, now unapproved (for unpublish) |\n\n**Note:** `revoked` is only reached from `approved` via `/api/platform/revoke` or `force_approved=true` during unpublish.\n\n---\n\n*Generated for Platform API v0.8.6.2 (API Version 1.5.0)*\n*Last Updated: 01 FEB 2026*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}