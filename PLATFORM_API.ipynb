{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": "# Platform API Testing Notebook\n\n**Last Updated**: 14 JAN 2026\n\nComplete workflow examples for publishing, accessing, and unpublishing geospatial data.\n\n## Table of Contents\n\n### Part 1: Diagnostics\n- 1.1 Liveness Probe (`/livez`)\n- 1.2 Readiness Probe (`/readyz`)\n- 1.3 Full Health Check (`/health`)\n- 1.4 Database Health\n- 1.5 Container Check\n\n### Part 2: Vector Workflow\n- 2.1 Publish Vector\n- 2.2 Access Vector (OGC Features API)\n- 2.3 Unpublish Vector\n\n### Part 3: Single Raster Workflow\n- 3.1 Publish Single Raster\n- 3.2 Access Raster (STAC API)\n- 3.3 Unpublish Raster\n\n### Part 4: Raster Collection Workflow\n- 4.1 Publish Raster Collection\n- 4.2 Access Collection (STAC API)\n- 4.3 Unpublish Collection\n\n### Part 5: Large Raster Workflow\n- 5.1 Publish Large Raster (>800 MB)\n\n### Part 6: Validation & Error Handling\n- 6.1 Size/Count Limit Rejections\n\n### Part 7: TiTiler Tile Server\n- 7.1 COG Metadata & Info\n- 7.2 Interactive Viewer\n- 7.3 TileJSON & XYZ Tiles\n- 7.4 Rescale & Colormap\n- 7.5 Multi-Band Selection (bidx)\n- 7.6 Statistics & Point Queries\n\n### Appendices\n- Appendix A: Manual Job Status Check\n- Appendix B: Platform Request Status Check\n- Appendix C: Job Resubmit (Recovery)\n\n---\n\n## Size Routing Summary\n\n| Data Type | Size/Count | Job Type | Notes |\n|-----------|------------|----------|-------|\n| Single Raster | ≤800 MB | `process_raster_v2` | Standard COG conversion |\n| Single Raster | 100 MB - 30 GB | `process_large_raster_v2` | Tiled COG workflow |\n| Raster Collection | ≤20 files, each ≤800 MB | `process_raster_collection_v2` | MosaicJSON |\n| Vector | Any size | `process_vector` | PostGIS + STAC |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Configuration\n",
    "\n",
    "**All environment and component names are defined here.** Update this cell to point to different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\nimport time\nimport urllib.parse\nfrom IPython.display import Markdown, display\n\n# =============================================================================\n# ENVIRONMENT CONFIGURATION\n# =============================================================================\n# Update these values to point to different environments (DEV/QA/UAT/PROD)\n\nENVIRONMENT = \"DEV\"  # DEV, QA, UAT, PROD\n\n# Function App Base URLs\nBASE_URL = \"https://rmhazuregeoapi-a3dma3ctfdgngwf6.eastus-01.azurewebsites.net\"\nOGC_STAC_URL = \"https://rmhogcstac-b4f5ccetf0a7hwe9.eastus-01.azurewebsites.net\"\n\n# TiTiler Tile Server (Vanilla TiTiler with /vsiaz/ direct access)\nTITILER_URL = \"https://rmhtitiler-ghcyd7g0bxdvc2hc.eastus-01.azurewebsites.net\"\n\n# =============================================================================\n# STORAGE CONFIGURATION\n# =============================================================================\n\n# Bronze tier: Raw input data\nBRONZE_CONTAINER = \"rmhazuregeobronze\"\n\n# Silver tier: Processed outputs\nSILVER_COGS_CONTAINER = \"silver-cogs\"\nSILVER_FATHOM_CONTAINER = \"silver-fathom\"\n\n# Azure Storage Account (for /vsiaz/ paths)\nSTORAGE_ACCOUNT = \"rmhazuregeo\"\n\n# =============================================================================\n# DATABASE CONFIGURATION\n# =============================================================================\n\n# PostGIS schemas\nGEO_SCHEMA = \"geo\"        # Vector data\nAPP_SCHEMA = \"app\"        # Jobs, tasks, metadata\nPGSTAC_SCHEMA = \"pgstac\"  # STAC catalog\nH3_SCHEMA = \"h3\"          # H3 analytics\n\n# =============================================================================\n# STAC CONFIGURATION\n# =============================================================================\n\n# Default collections\nRASTER_COLLECTION = \"system-rasters\"   # Single rasters\nVECTOR_COLLECTION = \"system-vectors\"   # Vectors\n\n# =============================================================================\n# TEST DATA CONFIGURATION\n# =============================================================================\n\n# Test vector file\nTEST_VECTOR_FILE = \"8.geojson\"\nTEST_VECTOR_DATASET = \"test-vectors\"\nTEST_VECTOR_RESOURCE = \"geojson-8\"\nTEST_VECTOR_VERSION = \"v1\"\n\n# Test raster file (small, <800 MB)\nTEST_RASTER_FILE = \"dctest.tif\"\nTEST_RASTER_DATASET = \"test-raster-notebook\"\nTEST_RASTER_RESOURCE = \"dctest\"\nTEST_RASTER_VERSION = \"v1\"\n\n# Test large raster (>800 MB)\nTEST_LARGE_RASTER_FILE = \"antigua.tif\"\nTEST_LARGE_RASTER_DATASET = \"test-large-raster\"\nTEST_LARGE_RASTER_RESOURCE = \"antigua\"\nTEST_LARGE_RASTER_VERSION = \"v1\"\n\n# Test raster collection\nTEST_COLLECTION_FILES = [\n    \"namangan/namangan14aug2019_R1C1cog.tif\",\n    \"namangan/namangan14aug2019_R1C2cog.tif\",\n    \"namangan/namangan14aug2019_R2C1cog.tif\",\n    \"namangan/namangan14aug2019_R2C2cog.tif\"\n]\nTEST_COLLECTION_DATASET = \"namangan-imagery\"\nTEST_COLLECTION_RESOURCE = \"aug2019\"\nTEST_COLLECTION_VERSION = \"v1\"\n\n# Test COG for TiTiler (already processed, in silver-cogs)\nTEST_COG_FILE = \"05APR13082706_cog_analysis.tif\"\n\n# =============================================================================\n# HELPER FUNCTIONS\n# =============================================================================\n\ndef api_call(method, endpoint, data=None, params=None, timeout=30, base_url=None):\n    \"\"\"Make API call and return formatted response.\"\"\"\n    url = f\"{base_url or BASE_URL}{endpoint}\"\n    headers = {\"Content-Type\": \"application/json\"}\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"{method} {endpoint}\")\n    print(f\"{'='*60}\")\n    \n    if data:\n        print(f\"\\nRequest Body:\")\n        print(json.dumps(data, indent=2))\n    \n    try:\n        if method == \"GET\":\n            response = requests.get(url, params=params, timeout=timeout)\n        elif method == \"POST\":\n            response = requests.post(url, json=data, headers=headers, timeout=timeout)\n        elif method == \"DELETE\":\n            response = requests.delete(url, params=params, timeout=timeout)\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n        \n        print(f\"\\nStatus: {response.status_code}\")\n        \n        try:\n            result = response.json()\n            print(f\"\\nResponse:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        except:\n            print(f\"\\nResponse (text): {response.text[:500]}\")\n            return response.text\n            \n    except requests.exceptions.Timeout:\n        print(f\"\\nRequest timed out (timeout={timeout}s)\")\n        return None\n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        return None\n\n\ndef check_job_status(job_id, max_polls=20, poll_interval=5):\n    \"\"\"Poll job status until completion or timeout.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Polling job: {job_id}\")\n    print(f\"{'='*60}\")\n    \n    for i in range(max_polls):\n        result = requests.get(f\"{BASE_URL}/api/jobs/status/{job_id}\", timeout=30).json()\n        status = result.get(\"status\", \"unknown\")\n        stage = result.get(\"stage\", \"?\")\n        total = result.get(\"totalStages\", \"?\")\n        \n        print(f\"  [{i+1}/{max_polls}] Status: {status}, Stage: {stage}/{total}\")\n        \n        if status in [\"completed\", \"failed\"]:\n            print(f\"\\nFinal Result:\")\n            print(json.dumps(result, indent=2, default=str))\n            return result\n        \n        time.sleep(poll_interval)\n    \n    print(f\"\\nPolling timeout after {max_polls * poll_interval}s\")\n    return result\n\n\ndef derive_table_name(dataset_id, resource_id, version_id):\n    \"\"\"Derive PostGIS table name from DDH identifiers.\"\"\"\n    return f\"{dataset_id}_{resource_id}_{version_id}\".replace(\"-\", \"_\")\n\n\ndef derive_stac_item_id(dataset_id, resource_id, version_id):\n    \"\"\"Derive STAC item ID from DDH identifiers.\"\"\"\n    return f\"{dataset_id}-{resource_id}-{version_id}\".replace(\"_\", \"-\")\n\n\ndef show_links(links_dict):\n    \"\"\"Display dynamic links as clickable markdown.\n    \n    Usage: show_links({\"Submit Vector\": \"/api/interface/submit-vector\", ...})\n    \"\"\"\n    md = \"**Quick Links:**\\n\"\n    for name, path in links_dict.items():\n        if path.startswith(\"http\"):\n            md += f\"- [{name}]({path})\\n\"\n        else:\n            md += f\"- [{name}]({BASE_URL}{path})\\n\"\n    display(Markdown(md))\n\n\ndef vsiaz_path(container, blob_path):\n    \"\"\"Build /vsiaz/ path for GDAL access to Azure Blob Storage.\"\"\"\n    return f\"/vsiaz/{container}/{blob_path}\"\n\n\ndef titiler_url(endpoint, container, blob_path, **params):\n    \"\"\"Build TiTiler URL with properly encoded /vsiaz/ path.\n    \n    Args:\n        endpoint: TiTiler endpoint (e.g., '/cog/info', '/cog/tiles/{z}/{x}/{y}')\n        container: Azure container name\n        blob_path: Path to blob within container\n        **params: Additional query parameters (rescale, colormap_name, bidx, etc.)\n    \n    Returns:\n        Full TiTiler URL with encoded parameters\n    \"\"\"\n    vsiaz = vsiaz_path(container, blob_path)\n    encoded_url = urllib.parse.quote(vsiaz, safe='')\n    \n    url = f\"{TITILER_URL}{endpoint}?url={encoded_url}\"\n    \n    for key, value in params.items():\n        if value is not None:\n            url += f\"&{key}={value}\"\n    \n    return url\n\n\n# =============================================================================\n# DISPLAY CONFIGURATION\n# =============================================================================\n\nprint(\"=\" * 60)\nprint(f\"PLATFORM API TESTING - {ENVIRONMENT} ENVIRONMENT\")\nprint(\"=\" * 60)\nprint(f\"\\nEndpoints:\")\nprint(f\"  Platform API:    {BASE_URL}\")\nprint(f\"  OGC/STAC API:    {OGC_STAC_URL}\")\nprint(f\"  TiTiler:         {TITILER_URL}\")\nprint(f\"\\nStorage:\")\nprint(f\"  Bronze:          {BRONZE_CONTAINER}\")\nprint(f\"  Silver COGs:     {SILVER_COGS_CONTAINER}\")\nprint(f\"  Account:         {STORAGE_ACCOUNT}\")\nprint(f\"\\nSchemas:\")\nprint(f\"  Geo:             {GEO_SCHEMA}\")\nprint(f\"  STAC:            {PGSTAC_SCHEMA}\")\nprint(f\"\\nTest Data:\")\nprint(f\"  Vector:          {TEST_VECTOR_FILE}\")\nprint(f\"  Raster:          {TEST_RASTER_FILE}\")\nprint(f\"  Large Raster:    {TEST_LARGE_RASTER_FILE}\")\nprint(f\"  Collection:      {len(TEST_COLLECTION_FILES)} files\")\nprint(f\"  Test COG:        {TEST_COG_FILE}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-part1-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Diagnostics\n",
    "\n",
    "## Probe Comparison\n",
    "\n",
    "| Probe | Endpoint | Speed | Checks | Use Case |\n",
    "|-------|----------|-------|--------|----------|\n",
    "| **Liveness** | `/api/livez` | ~1ms | Process alive | Platform restart decisions |\n",
    "| **Readiness** | `/api/readyz` | ~100-500ms | DB + Service Bus | Load balancer routing |\n",
    "| **Health** | `/api/health` | ~30-60s | Everything | Manual debugging |\n",
    "\n",
    "System health checks and diagnostic endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5tcs7dqikn",
   "metadata": {},
   "outputs": [],
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"System Health\": \"/api/interface/health\",\n    \"Pipelines\": \"/api/interface/pipeline\",\n    \"Service Bus Queues\": \"/api/interface/queues\",\n    \"Databases\": \"/api/interface/database\",\n    \"Storage Browser\": \"/api/interface/storage\",\n    \"OGC Feature Collections Gallery\": \"/api/interface/vector\",\n    \"STAC Collections Gallery\": \"/api/interface/stac\"\n})"
  },
  {
   "cell_type": "markdown",
   "id": "idu6qyvlfjg",
   "metadata": {},
   "source": [
    "## 1.1 Liveness Probe (`/livez`)\n",
    "\n",
    "**Purpose:** Is the application process alive and running?\n",
    "\n",
    "This is the **fastest** probe (~1ms). Azure and Kubernetes use this to determine if the container needs to be **restarted**. It only checks that the Python process is responding - no database, no storage, no external dependencies.\n",
    "\n",
    "**When it fails:** The container/instance should be killed and replaced.\n",
    "\n",
    "**Use case:** Continuous monitoring by the platform (every 10-30 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gqo2cwjv04u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liveness Probe - Is the process alive? (~1ms)\n",
    "result = api_call(\"GET\", \"/api/livez\", timeout=5)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    status = result.get('status', 'unknown')\n",
    "    message = result.get('message', '')\n",
    "    print(f\"\\nLiveness: {'ALIVE' if status == 'alive' else 'DEAD'}\")\n",
    "    print(f\"Message: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dwgkxyxrwr",
   "metadata": {},
   "source": [
    "## 1.2 Readiness Probe (`/readyz`)\n",
    "\n",
    "**Purpose:** Is the application ready to handle requests?\n",
    "\n",
    "This is a **fast** probe (~100-500ms). It verifies that critical dependencies are available:\n",
    "- Database connection (can we reach PostgreSQL?)\n",
    "- Service Bus connection (can we queue jobs?)\n",
    "\n",
    "**When it fails:** The instance should be **removed from the load balancer** but NOT restarted. Traffic is routed to other healthy instances while this one recovers.\n",
    "\n",
    "**Use case:** Load balancer health checks, deployment validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6mlq4vxhr9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readiness Probe - Are dependencies available? (~100-500ms)\n",
    "result = api_call(\"GET\", \"/api/readyz\", timeout=10)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    status = result.get('status', 'unknown')\n",
    "    message = result.get('message', '')\n",
    "    summary = result.get('summary', {})\n",
    "    \n",
    "    print(f\"\\nReadiness: {'READY' if status == 'ready' else 'NOT READY'}\")\n",
    "    print(f\"Message: {message}\")\n",
    "    \n",
    "    if summary:\n",
    "        passed = summary.get('checks_passed', 0)\n",
    "        failed = summary.get('checks_failed', 0)\n",
    "        print(f\"Checks: {passed} passed, {failed} failed\")\n",
    "        \n",
    "        if summary.get('failed_check_names'):\n",
    "            print(f\"Failed: {', '.join(summary['failed_check_names'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcjuzin99i",
   "metadata": {},
   "source": [
    "## 1.3 Full Health Check (`/health`)\n",
    "\n",
    "**Purpose:** Comprehensive system diagnostics for operators and debugging.\n",
    "\n",
    "This is a **slow** probe (~30-60s). It performs exhaustive checks of ALL system components:\n",
    "- Database connectivity AND query execution\n",
    "- Service Bus queue status (all 4 queues)\n",
    "- Storage account access\n",
    "- PgSTAC catalog health\n",
    "- Schema inspection\n",
    "- PostgreSQL function tests\n",
    "\n",
    "**When to use:** Manual debugging, deployment verification, incident investigation. NOT for automated health checks (too slow, too much load).\n",
    "\n",
    "**Use case:** \"Why isn't my job processing?\" - run this to see what's broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Check (takes ~60s for full check)\n",
    "result = api_call(\"GET\", \"/api/health\", timeout=90)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"HEALTH SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overall Status: {result.get('status', 'unknown').upper()}\")\n",
    "    print(f\"Version: {result.get('version', 'unknown')}\")\n",
    "    \n",
    "    components = result.get('components', {})\n",
    "    print(f\"\\nComponents ({len(components)}):\")\n",
    "    for name, component in components.items():\n",
    "        # Each component is a dict with 'status' field\n",
    "        if isinstance(component, dict):\n",
    "            status = component.get('status', 'unknown')\n",
    "            icon = \"OK\" if status == \"healthy\" else (\"WARN\" if status == \"disabled\" else \"FAIL\")\n",
    "        else:\n",
    "            icon = \"?\"\n",
    "        print(f\"  {name}: {icon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-stats-header",
   "metadata": {},
   "source": [
    "## 1.4 Database Health\n",
    "\n",
    "Database connection pool, size, and maintenance status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Health\n",
    "result = api_call(\"GET\", \"/api/dbadmin/health\")\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATABASE HEALTH\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Status: {result.get('status', 'unknown').upper()}\")\n",
    "    \n",
    "    # Connection pool\n",
    "    pool = result.get('connection_pool', {})\n",
    "    if pool:\n",
    "        print(f\"\\nConnection Pool:\")\n",
    "        print(f\"  Active: {pool.get('active', 0)}/{pool.get('total', 0)}\")\n",
    "        print(f\"  Utilization: {pool.get('utilization_percent', 0)}%\")\n",
    "    \n",
    "    # Database size\n",
    "    size = result.get('database_size', {})\n",
    "    if size:\n",
    "        print(f\"\\nDatabase Size:\")\n",
    "        print(f\"  Total: {size.get('total', 'unknown')}\")\n",
    "        print(f\"  App Schema: {size.get('app_schema', 'unknown')}\")\n",
    "        print(f\"  Geo Schema: {size.get('geo_schema', 'unknown')}\")\n",
    "    \n",
    "    # Health checks\n",
    "    checks = result.get('checks', [])\n",
    "    if checks:\n",
    "        print(f\"\\nHealth Checks:\")\n",
    "        for check in checks:\n",
    "            status = check.get('status', 'unknown')\n",
    "            icon = \"OK\" if status == \"healthy\" else (\"WARN\" if status == \"warning\" else \"FAIL\")\n",
    "            print(f\"  {check.get('name', '?')}: {icon} - {check.get('message', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-failed-jobs-header",
   "metadata": {},
   "source": [
    "### 1.4b Query Jobs\n",
    "\n",
    "Query jobs from the database with optional filters.\n",
    "\n",
    "**Endpoint:** `GET /api/dbadmin/jobs`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `status` | query | No | Filter by status: `queued`, `processing`, `completed`, `failed` |\n",
    "| `job_type` | query | No | Filter by job type (e.g., `process_vector`, `process_raster_v2`) |\n",
    "| `limit` | query | No | Max jobs to return (default: 50) |\n",
    "| `hours` | query | No | Only jobs from last N hours |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-failed-jobs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query recent failed jobs\n",
    "result = api_call(\"GET\", \"/api/dbadmin/jobs\", params={\"status\": \"failed\", \"limit\": 5})\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    jobs = result.get('jobs', [])\n",
    "    print(f\"\\nFound {len(jobs)} failed jobs\")\n",
    "    for job in jobs:\n",
    "        job_id = job.get('job_id', 'unknown')[:16]\n",
    "        job_type = job.get('job_type', 'unknown')\n",
    "        error = job.get('error_details', 'No error details')\n",
    "        if error and len(error) > 60:\n",
    "            error = error[:60] + \"...\"\n",
    "        print(f\"  - {job_id}... ({job_type})\")\n",
    "        print(f\"    Error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-container-header",
   "metadata": {},
   "source": [
    "## 1.5 Container Check\n",
    "\n",
    "Synchronous endpoint to list blobs in a container. Returns immediately (no job queue).\n",
    "\n",
    "**Endpoint:** `GET /api/containers/{container_name}/blobs`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `container_name` | path | Yes | Storage container name (e.g., `rmhazuregeobronze`) |\n",
    "| `suffix` | query | No | Filter by file extension (e.g., `.tif`, `.geojson`) |\n",
    "| `prefix` | query | No | Filter by blob path prefix (e.g., `namangan/`) |\n",
    "| `metadata` | query | No | `true` (default) returns full blob info, `false` returns just names |\n",
    "| `limit` | query | No | Max blobs to return (default: 500, max: 10000) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List TIF files in bronze container\n",
    "result = api_call(\"GET\", f\"/api/containers/{BRONZE_CONTAINER}/blobs\", \n",
    "                  params={\"suffix\": \".tif\", \"limit\": 10, \"metadata\": \"true\"})\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    count = result.get(\"count\", 0)\n",
    "    blobs = result.get(\"blobs\", [])\n",
    "    print(f\"\\nFound {count} TIF files\")\n",
    "    if blobs:\n",
    "        total_mb = sum(b.get(\"size_mb\", 0) for b in blobs)\n",
    "        print(f\"Total size (first {len(blobs)}): {total_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-container-geojson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List GeoJSON files in bronze container\n",
    "result = api_call(\"GET\", f\"/api/containers/{BRONZE_CONTAINER}/blobs\", \n",
    "                  params={\"suffix\": \".geojson\", \"limit\": 10, \"metadata\": \"true\"})\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    count = result.get(\"count\", 0)\n",
    "    print(f\"\\nFound {count} GeoJSON files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part2-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Vector Workflow\n",
    "\n",
    "Complete publish → access → unpublish cycle for vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881s8be1x58",
   "metadata": {},
   "outputs": [],
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"Submit Vector (Web UI)\": \"/api/interface/submit-vector\",\n    \"Unpublish Vector (Web UI)\": \"/api/interface/unpublish-vector\",\n    \"OGC Features Collections\": f\"{OGC_STAC_URL}/api/features/collections\",\n    \"Swagger UI\": \"/api/docs\"\n})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-vector-publish-header",
   "metadata": {},
   "source": [
    "## 2.1 Publish Vector\n",
    "\n",
    "Submit a vector file (GeoJSON, Shapefile, GeoPackage) for ingestion into PostGIS.\n",
    "\n",
    "**Endpoint:** `POST /api/platform/submit`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Top-level identifier (e.g., `world-borders`) |\n",
    "| `resource_id` | body | Yes | Resource identifier (e.g., `admin-boundaries`) |\n",
    "| `version_id` | body | Yes | Version identifier (e.g., `v1`, `2024-01`) |\n",
    "| `container_name` | body | Yes | Source container with raw file |\n",
    "| `file_name` | body | Yes | Blob path to vector file (`.geojson`, `.shp.zip`, `.gpkg`) |\n",
    "| `service_name` | body | No | Human-readable name for the dataset |\n",
    "| `description` | body | No | Dataset description |\n",
    "| `access_level` | body | No | Access classification (default: `OUO`) |\n",
    "\n",
    "**Output Naming:** Table name derived as `{dataset_id}_{resource_id}_{version_id}` (hyphens → underscores)\n",
    "\n",
    "**Workflow:**\n",
    "1. Stage 1: Parse and chunk the file\n",
    "2. Stage 2: Upload chunks to PostGIS (parallel)\n",
    "3. Stage 3: Create STAC item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-publish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Vector via Platform API\n",
    "vector_request = {\n",
    "    \"dataset_id\": TEST_VECTOR_DATASET,\n",
    "    \"resource_id\": TEST_VECTOR_RESOURCE,\n",
    "    \"version_id\": TEST_VECTOR_VERSION,\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"file_name\": TEST_VECTOR_FILE,\n",
    "    \"service_name\": \"Test Vector Data\"\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/submit\", vector_request)\n",
    "vector_job_id = result.get(\"job_id\") if result else None\n",
    "vector_request_id = result.get(\"request_id\") if result else None\n",
    "\n",
    "print(f\"\\nRequest ID: {vector_request_id}\")\n",
    "print(f\"Job ID: {vector_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll Vector Job Status\n",
    "if vector_job_id:\n",
    "    vector_result = check_job_status(vector_job_id)\n",
    "else:\n",
    "    print(\"No job_id - run previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-vector-access-header",
   "metadata": {},
   "source": [
    "## 2.2 Access Vector (OGC Features API)\n",
    "\n",
    "Query the published vector data using OGC API - Features.\n",
    "\n",
    "**Base URL:** `OGC_STAC_URL` (dedicated OGC/STAC app)\n",
    "\n",
    "### List Collections\n",
    "**Endpoint:** `GET /api/features/collections`\n",
    "\n",
    "### Query Features\n",
    "**Endpoint:** `GET /api/features/collections/{collection}/items`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `collection` | path | Yes | Table name (e.g., `test_vectors_geojson_8_v1`) |\n",
    "| `limit` | query | No | Max features to return (default: 10, max: 10000) |\n",
    "| `offset` | query | No | Skip first N features for pagination |\n",
    "| `bbox` | query | No | Bounding box filter: `minx,miny,maxx,maxy` (WGS84) |\n",
    "| `datetime` | query | No | Temporal filter (ISO 8601) |\n",
    "| `properties` | query | No | Filter by property values |\n",
    "\n",
    "**Example bbox:** `-71.0,-56.0,-70.0,-55.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-list-collections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List OGC Feature Collections (via dedicated OGC/STAC app)\n",
    "result = api_call(\"GET\", \"/api/features/collections\", base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    collections = result.get(\"collections\", [])\n",
    "    print(f\"\\nFound {len(collections)} vector collections\")\n",
    "    for c in collections[:10]:\n",
    "        print(f\"  - {c.get('id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query features from published vector\n",
    "vector_table = derive_table_name(TEST_VECTOR_DATASET, TEST_VECTOR_RESOURCE, TEST_VECTOR_VERSION)\n",
    "print(f\"Querying table: {vector_table}\")\n",
    "\n",
    "result = api_call(\"GET\", f\"/api/features/collections/{vector_table}/items\",\n",
    "                  params={\"limit\": 5}, base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    features = result.get(\"features\", [])\n",
    "    print(f\"\\nReturned {len(features)} features\")\n",
    "    print(f\"Total matched: {result.get('numberMatched', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-bbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial query with bounding box\n",
    "# Format: minx,miny,maxx,maxy (WGS84)\n",
    "bbox = \"-71.0,-56.0,-70.0,-55.0\"\n",
    "\n",
    "result = api_call(\"GET\", f\"/api/features/collections/{vector_table}/items\",\n",
    "                  params={\"bbox\": bbox, \"limit\": 10}, base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    features = result.get(\"features\", [])\n",
    "    print(f\"\\nFeatures in bbox: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-vector-unpublish-header",
   "metadata": {},
   "source": [
    "## 2.3 Unpublish Vector\n",
    "\n",
    "Remove vector data from the platform (PostGIS table + STAC item).\n",
    "\n",
    "**Endpoint:** `POST /api/platform/unpublish/vector`\n",
    "\n",
    "### Option 1: By DDH Identifiers (Preferred)\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Original dataset_id used during publish |\n",
    "| `resource_id` | body | Yes | Original resource_id |\n",
    "| `version_id` | body | Yes | Original version_id |\n",
    "| `dry_run` | body | No | `true` (default) = preview only, `false` = actually delete |\n",
    "\n",
    "### Option 2: By Request ID\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `request_id` | body | Yes | Original request_id from publish response |\n",
    "| `dry_run` | body | No | `true` (default) = preview only |\n",
    "\n",
    "### Option 3: Cleanup Mode (Direct table_name)\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `table_name` | body | Yes | Direct PostGIS table name |\n",
    "| `schema_name` | body | No | Schema name (default: `geo`) |\n",
    "| `dry_run` | body | No | `true` (default) = preview only |\n",
    "\n",
    "**Important:** Always defaults to `dry_run=true` for safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-unpublish-dry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Vector - DRY RUN (shows what would be deleted)\n",
    "unpublish_request = {\n",
    "    \"dataset_id\": TEST_VECTOR_DATASET,\n",
    "    \"resource_id\": TEST_VECTOR_RESOURCE,\n",
    "    \"version_id\": TEST_VECTOR_VERSION,\n",
    "    \"dry_run\": True  # SAFETY: Set to False to actually delete\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/unpublish/vector\", unpublish_request)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nDry Run: {result.get('dry_run', 'N/A')}\")\n",
    "    print(f\"Table: {result.get('table_name', 'N/A')}\")\n",
    "    print(f\"Would delete: {result.get('would_delete', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-vector-unpublish-execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Vector - EXECUTE (uncomment to actually delete)\n",
    "# unpublish_request = {\n",
    "#     \"dataset_id\": TEST_VECTOR_DATASET,\n",
    "#     \"resource_id\": TEST_VECTOR_RESOURCE,\n",
    "#     \"version_id\": TEST_VECTOR_VERSION,\n",
    "#     \"dry_run\": False  # DANGER: Will actually delete!\n",
    "# }\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/vector\", unpublish_request)\n",
    "print(\"Uncomment and run to actually delete the vector data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part3-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Single Raster Workflow\n",
    "\n",
    "Complete publish → access → unpublish cycle for single raster files (≤800 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j7s3huyu6xe",
   "metadata": {},
   "outputs": [],
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"Submit Single Raster (Web UI)\": \"/api/interface/submit-raster\",\n    \"Unpublish Raster (Web UI)\": \"/api/interface/unpublish-raster\",\n    \"STAC Collections\": f\"{OGC_STAC_URL}/api/collections\",\n    \"STAC Search\": f\"{OGC_STAC_URL}/api/search\",\n    \"Swagger UI\": \"/api/docs\"\n})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-raster-publish-header",
   "metadata": {},
   "source": [
    "## 3.1 Publish Single Raster\n",
    "\n",
    "Submit a single raster file for COG conversion and STAC cataloging.\n",
    "\n",
    "**Endpoint:** `POST /api/platform/raster`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Top-level identifier (e.g., `satellite-imagery`) |\n",
    "| `resource_id` | body | Yes | Resource identifier (e.g., `nairobi-2024`) |\n",
    "| `version_id` | body | Yes | Version identifier (e.g., `v1`) |\n",
    "| `container_name` | body | Yes | Source container with raw raster |\n",
    "| `file_name` | body | Yes | Blob path to raster file (`.tif`, `.img`, etc.) |\n",
    "| `service_name` | body | No | Human-readable name |\n",
    "| `description` | body | No | Dataset description |\n",
    "| `access_level` | body | No | Access classification (default: `OUO`) |\n",
    "\n",
    "**Output Naming:** STAC item ID = `{dataset_id}-{resource_id}-{version_id}`\n",
    "\n",
    "**Size Routing:**\n",
    "- ≤800 MB → Standard COG conversion (`process_raster_v2`)\n",
    "- >800 MB → Auto-routes to tiled workflow (`process_large_raster_v2`)\n",
    "\n",
    "**Workflow:**\n",
    "1. Stage 1: Download and validate\n",
    "2. Stage 2: Convert to COG\n",
    "3. Stage 3: Create STAC item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-raster-publish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Single Raster via Platform API\n",
    "raster_request = {\n",
    "    \"dataset_id\": TEST_RASTER_DATASET,\n",
    "    \"resource_id\": TEST_RASTER_RESOURCE,\n",
    "    \"version_id\": TEST_RASTER_VERSION,\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"file_name\": TEST_RASTER_FILE,\n",
    "    \"service_name\": \"Test Raster Data\",\n",
    "    \"access_level\": \"OUO\"\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/raster\", raster_request)\n",
    "raster_job_id = result.get(\"job_id\") if result else None\n",
    "raster_request_id = result.get(\"request_id\") if result else None\n",
    "\n",
    "print(f\"\\nRequest ID: {raster_request_id}\")\n",
    "print(f\"Job ID: {raster_job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-raster-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll Raster Job Status\n",
    "if raster_job_id:\n",
    "    raster_result = check_job_status(raster_job_id)\n",
    "else:\n",
    "    print(\"No job_id - run previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-raster-access-header",
   "metadata": {},
   "source": [
    "## 3.2 Access Raster (STAC API)\n",
    "\n",
    "Query the published raster data using STAC API.\n",
    "\n",
    "**Base URL:** `OGC_STAC_URL` (dedicated OGC/STAC app)\n",
    "\n",
    "### List Collections\n",
    "**Endpoint:** `GET /api/collections`\n",
    "\n",
    "### Get Specific Item\n",
    "**Endpoint:** `GET /api/collections/{collection_id}/items/{item_id}`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `collection_id` | path | Yes | STAC collection (e.g., `system-rasters`) |\n",
    "| `item_id` | path | Yes | STAC item ID (e.g., `test-raster-notebook-dctest-v1`) |\n",
    "\n",
    "### Search Items\n",
    "**Endpoint:** `POST /api/search`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `collections` | body | No | Array of collection IDs to search |\n",
    "| `bbox` | body | No | Bounding box: `[minx, miny, maxx, maxy]` |\n",
    "| `datetime` | body | No | Temporal filter (ISO 8601 interval) |\n",
    "| `limit` | body | No | Max items to return (default: 10) |\n",
    "| `ids` | body | No | Array of specific item IDs |\n",
    "| `intersects` | body | No | GeoJSON geometry for spatial filter |\n",
    "\n",
    "**Example search:**\n",
    "```json\n",
    "{\"collections\": [\"system-rasters\"], \"bbox\": [-180, -90, 180, 90], \"limit\": 10}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stac-collections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List STAC Collections\n",
    "result = api_call(\"GET\", \"/api/collections\", base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    collections = result.get(\"collections\", [])\n",
    "    print(f\"\\nFound {len(collections)} STAC collections\")\n",
    "    for c in collections[:10]:\n",
    "        print(f\"  - {c.get('id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stac-item",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific STAC item\n",
    "stac_item_id = derive_stac_item_id(TEST_RASTER_DATASET, TEST_RASTER_RESOURCE, TEST_RASTER_VERSION)\n",
    "print(f\"Looking for STAC item: {stac_item_id}\")\n",
    "\n",
    "result = api_call(\"GET\", f\"/api/collections/{RASTER_COLLECTION}/items/{stac_item_id}\",\n",
    "                  base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    print(f\"\\nItem found: {result.get('id', 'unknown')}\")\n",
    "    assets = result.get('assets', {})\n",
    "    print(f\"Assets: {list(assets.keys())}\")\n",
    "    if 'data' in assets:\n",
    "        print(f\"COG URL: {assets['data'].get('href', 'N/A')[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stac-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC Search\n",
    "search_params = {\n",
    "    \"collections\": [RASTER_COLLECTION],\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/search\", search_params, base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    features = result.get(\"features\", [])\n",
    "    print(f\"\\nFound {len(features)} items\")\n",
    "    for f in features[:5]:\n",
    "        print(f\"  - {f.get('id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-raster-unpublish-header",
   "metadata": {},
   "source": [
    "## 3.3 Unpublish Raster\n",
    "\n",
    "Remove raster data from the platform (COG blobs + STAC item).\n",
    "\n",
    "**Endpoint:** `POST /api/platform/unpublish/raster`\n",
    "\n",
    "### Option 1: By DDH Identifiers (Preferred)\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Original dataset_id used during publish |\n",
    "| `resource_id` | body | Yes | Original resource_id |\n",
    "| `version_id` | body | Yes | Original version_id |\n",
    "| `dry_run` | body | No | `true` (default) = preview only, `false` = actually delete |\n",
    "\n",
    "### Option 2: By Request ID\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `request_id` | body | Yes | Original request_id from publish response |\n",
    "| `dry_run` | body | No | `true` (default) = preview only |\n",
    "\n",
    "### Option 3: Cleanup Mode (Direct STAC identifiers)\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `stac_item_id` | body | Yes | STAC item ID to delete |\n",
    "| `collection_id` | body | Yes | STAC collection containing the item |\n",
    "| `dry_run` | body | No | `true` (default) = preview only |\n",
    "\n",
    "**Important:** Always defaults to `dry_run=true` for safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-raster-unpublish-dry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Raster - DRY RUN\n",
    "unpublish_request = {\n",
    "    \"dataset_id\": TEST_RASTER_DATASET,\n",
    "    \"resource_id\": TEST_RASTER_RESOURCE,\n",
    "    \"version_id\": TEST_RASTER_VERSION,\n",
    "    \"dry_run\": True  # SAFETY: Set to False to actually delete\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_request)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nDry Run: {result.get('dry_run', 'N/A')}\")\n",
    "    print(f\"STAC Item: {result.get('stac_item_id', 'N/A')}\")\n",
    "    print(f\"Collection: {result.get('collection_id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-raster-unpublish-execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Raster - EXECUTE (uncomment to actually delete)\n",
    "# unpublish_request = {\n",
    "#     \"dataset_id\": TEST_RASTER_DATASET,\n",
    "#     \"resource_id\": TEST_RASTER_RESOURCE,\n",
    "#     \"version_id\": TEST_RASTER_VERSION,\n",
    "#     \"dry_run\": False  # DANGER: Will actually delete!\n",
    "# }\n",
    "# result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_request)\n",
    "print(\"Uncomment and run to actually delete the raster data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part4-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Raster Collection Workflow\n",
    "\n",
    "Complete publish → access → unpublish cycle for multi-file raster collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z4v0fuxgizj",
   "metadata": {},
   "outputs": [],
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"Submit Raster Collection (Web UI)\": \"/api/interface/submit-raster-collection\",\n    \"Unpublish Raster (Web UI)\": \"/api/interface/unpublish-raster\",\n    \"STAC Collections\": f\"{OGC_STAC_URL}/api/collections\",\n    \"STAC Search\": f\"{OGC_STAC_URL}/api/search\",\n    \"Swagger UI\": \"/api/docs\"\n})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-collection-publish-header",
   "metadata": {},
   "source": [
    "## 4.1 Publish Raster Collection\n",
    "\n",
    "Submit multiple raster files as a unified collection with MosaicJSON.\n",
    "\n",
    "**Endpoint:** `POST /api/platform/raster-collection`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Top-level identifier (e.g., `namangan-imagery`) |\n",
    "| `resource_id` | body | Yes | Resource identifier (e.g., `aug2019`) |\n",
    "| `version_id` | body | Yes | Version identifier (e.g., `v1`) |\n",
    "| `container_name` | body | Yes | Source container with raw rasters |\n",
    "| `file_name` | body | Yes | **Array** of blob paths (2-20 files) |\n",
    "| `service_name` | body | No | Human-readable name |\n",
    "| `description` | body | No | Dataset description |\n",
    "| `access_level` | body | No | Access classification (default: `OUO`) |\n",
    "\n",
    "**Limits:**\n",
    "- Min 2 files, Max 20 files per collection\n",
    "- Each file must be ≤800 MB\n",
    "\n",
    "**Output:** Creates a STAC collection with multiple items + MosaicJSON for unified tile access.\n",
    "\n",
    "**Workflow:**\n",
    "1. Stage 1: Validate all files (size, existence)\n",
    "2. Stage 2: Convert each to COG (parallel)\n",
    "3. Stage 3: Create MosaicJSON\n",
    "4. Stage 4: Create STAC collection + items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-collection-publish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Raster Collection via Platform API\n",
    "collection_request = {\n",
    "    \"dataset_id\": TEST_COLLECTION_DATASET,\n",
    "    \"resource_id\": TEST_COLLECTION_RESOURCE,\n",
    "    \"version_id\": TEST_COLLECTION_VERSION,\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"file_name\": TEST_COLLECTION_FILES,\n",
    "    \"service_name\": \"Test Raster Collection\",\n",
    "    \"access_level\": \"OUO\"\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/raster-collection\", collection_request)\n",
    "collection_job_id = result.get(\"job_id\") if result else None\n",
    "collection_request_id = result.get(\"request_id\") if result else None\n",
    "\n",
    "print(f\"\\nRequest ID: {collection_request_id}\")\n",
    "print(f\"Job ID: {collection_job_id}\")\n",
    "print(f\"File Count: {result.get('file_count', 'N/A')}\" if result else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-collection-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll Collection Job Status (longer timeout for multi-file)\n",
    "if collection_job_id:\n",
    "    collection_result = check_job_status(collection_job_id, max_polls=30, poll_interval=10)\n",
    "else:\n",
    "    print(\"No job_id - run previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-collection-access-header",
   "metadata": {},
   "source": [
    "## 4.2 Access Collection (STAC API)\n",
    "\n",
    "Query the published collection using STAC API.\n",
    "\n",
    "**Base URL:** `OGC_STAC_URL` (dedicated OGC/STAC app)\n",
    "\n",
    "### List Items in Collection\n",
    "**Endpoint:** `GET /api/collections/{collection_id}/items`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `collection_id` | path | Yes | Collection ID (e.g., `namangan-imagery-aug2019-v1`) |\n",
    "| `limit` | query | No | Max items to return (default: 10) |\n",
    "| `bbox` | query | No | Bounding box filter: `minx,miny,maxx,maxy` |\n",
    "\n",
    "### Search Within Collection\n",
    "**Endpoint:** `POST /api/search`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `collections` | body | Yes | Array with collection ID |\n",
    "| `bbox` | body | No | Bounding box: `[minx, miny, maxx, maxy]` |\n",
    "| `limit` | body | No | Max items to return |\n",
    "\n",
    "**Note:** Collections have MosaicJSON for unified tile serving via TiTiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-collection-items",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List items in the collection\n",
    "collection_id = f\"{TEST_COLLECTION_DATASET}-{TEST_COLLECTION_RESOURCE}-{TEST_COLLECTION_VERSION}\"\n",
    "print(f\"Looking for collection: {collection_id}\")\n",
    "\n",
    "result = api_call(\"GET\", f\"/api/collections/{collection_id}/items\",\n",
    "                  params={\"limit\": 10}, base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    features = result.get(\"features\", [])\n",
    "    print(f\"\\nItems in collection: {len(features)}\")\n",
    "    for f in features:\n",
    "        print(f\"  - {f.get('id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-collection-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search within collection by bbox\n",
    "search_params = {\n",
    "    \"collections\": [collection_id],\n",
    "    \"bbox\": [70.0, 40.0, 72.0, 42.0],  # Namangan area\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/search\", search_params, base_url=OGC_STAC_URL)\n",
    "\n",
    "if result and isinstance(result, dict):\n",
    "    features = result.get(\"features\", [])\n",
    "    print(f\"\\nItems in bbox: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-collection-unpublish-header",
   "metadata": {},
   "source": [
    "## 4.3 Unpublish Collection\n",
    "\n",
    "Remove entire collection (all COGs + MosaicJSON + STAC collection + items).\n",
    "\n",
    "**Endpoint:** `POST /api/platform/unpublish/raster`\n",
    "\n",
    "Uses same endpoint as single raster unpublish. Parameters:\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Original dataset_id |\n",
    "| `resource_id` | body | Yes | Original resource_id |\n",
    "| `version_id` | body | Yes | Original version_id |\n",
    "| `dry_run` | body | No | `true` (default) = preview only |\n",
    "\n",
    "**Note:** Unpublishing a collection removes ALL items and the MosaicJSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-collection-unpublish-dry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpublish Collection - DRY RUN\n",
    "unpublish_request = {\n",
    "    \"dataset_id\": TEST_COLLECTION_DATASET,\n",
    "    \"resource_id\": TEST_COLLECTION_RESOURCE,\n",
    "    \"version_id\": TEST_COLLECTION_VERSION,\n",
    "    \"dry_run\": True\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/unpublish/raster\", unpublish_request)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nDry Run: {result.get('dry_run', 'N/A')}\")\n",
    "    print(f\"Collection: {result.get('collection_id', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part5-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Large Raster Workflow\n",
    "\n",
    "Processing rasters larger than 800 MB (up to 30 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhhufkztc4e",
   "metadata": {},
   "outputs": [],
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"Submit Raster (Web UI)\": \"/api/interface/submit-raster\",\n    \"Unpublish Raster (Web UI)\": \"/api/interface/unpublish-raster\",\n    \"STAC Collections\": f\"{OGC_STAC_URL}/api/collections\",\n    \"Container Contents\": f\"/api/containers/{BRONZE_CONTAINER}/blobs?suffix=.tif&limit=20\",\n    \"Swagger UI\": \"/api/docs\"\n})"
  },
  {
   "cell_type": "markdown",
   "id": "cell-large-raster-header",
   "metadata": {},
   "source": [
    "## 5.1 Publish Large Raster (>800 MB)\n",
    "\n",
    "Large rasters use a tiled processing workflow for files 100 MB - 30 GB.\n",
    "\n",
    "**Endpoint:** `POST /api/platform/raster` (same as single raster - auto-routes based on size)\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `dataset_id` | body | Yes | Top-level identifier |\n",
    "| `resource_id` | body | Yes | Resource identifier |\n",
    "| `version_id` | body | Yes | Version identifier |\n",
    "| `container_name` | body | Yes | Source container with raw raster |\n",
    "| `file_name` | body | Yes | Blob path to large raster file |\n",
    "| `service_name` | body | No | Human-readable name |\n",
    "| `access_level` | body | No | Access classification (default: `OUO`) |\n",
    "\n",
    "**Size Limits:**\n",
    "- Min: 100 MB (smaller files use standard `process_raster_v2`)\n",
    "- Max: 30 GB\n",
    "\n",
    "**Processing Time:** Large rasters take 30+ minutes to process.\n",
    "\n",
    "**Workflow:**\n",
    "1. Stage 1: Generate tiling scheme\n",
    "2. Stage 2: Extract tiles (sequential)\n",
    "3. Stage 3: Create tile COGs (parallel)\n",
    "4. Stage 4: Create MosaicJSON\n",
    "5. Stage 5: Create STAC collection\n",
    "\n",
    "**Output:** Creates a STAC collection with tile items + MosaicJSON (same as raster collection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-large-raster-publish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Large Raster via Platform API\n",
    "# Platform auto-routes to process_large_raster_v2 based on file size\n",
    "\n",
    "large_raster_request = {\n",
    "    \"dataset_id\": TEST_LARGE_RASTER_DATASET,\n",
    "    \"resource_id\": TEST_LARGE_RASTER_RESOURCE,\n",
    "    \"version_id\": TEST_LARGE_RASTER_VERSION,\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"file_name\": TEST_LARGE_RASTER_FILE,\n",
    "    \"service_name\": \"Test Large Raster\",\n",
    "    \"access_level\": \"OUO\"\n",
    "}\n",
    "\n",
    "result = api_call(\"POST\", \"/api/platform/raster\", large_raster_request)\n",
    "large_raster_job_id = result.get(\"job_id\") if result else None\n",
    "\n",
    "print(f\"\\nJob ID: {large_raster_job_id}\")\n",
    "print(f\"Job Type: {result.get('job_type', 'N/A')}\" if result else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-large-raster-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poll Large Raster Job Status (long timeout)\n",
    "if large_raster_job_id:\n",
    "    large_raster_result = check_job_status(large_raster_job_id, max_polls=60, poll_interval=30)\n",
    "else:\n",
    "    print(\"No job_id - run previous cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-part6-header",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Validation & Error Handling\n",
    "\n",
    "Examples of pre-flight validation rejecting invalid requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-validation-header",
   "metadata": {},
   "source": [
    "## 6.1 Size/Count Limit Rejections\n",
    "\n",
    "The platform validates requests before processing:\n",
    "\n",
    "- Single raster >800 MB → Rejected (use large raster endpoint)\n",
    "- Collection >20 files → Rejected\n",
    "- Collection with file >800 MB → Rejected\n",
    "- Missing files → Rejected with list of missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-reject-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Single raster too large for standard processing\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Single raster >800 MB via standard endpoint\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = api_call(\"POST\", \"/api/jobs/submit/process_raster_v2\", {\n",
    "    \"blob_name\": TEST_LARGE_RASTER_FILE,\n",
    "    \"container_name\": BRONZE_CONTAINER\n",
    "})\n",
    "\n",
    "if result and \"error\" in str(result).lower():\n",
    "    print(\"\\nCorrectly rejected - use process_large_raster_v2 instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-reject-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Collection with too many files\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Collection exceeding 20 file limit\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = api_call(\"POST\", \"/api/jobs/submit/process_raster_collection_v2\", {\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"blob_list\": [f\"file{i}.tif\" for i in range(21)],\n",
    "    \"collection_id\": \"test-too-many\"\n",
    "})\n",
    "\n",
    "if result and \"error\" in str(result).lower():\n",
    "    print(\"\\nCorrectly rejected - max 20 files per collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-reject-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Collection with non-existent file\n",
    "print(\"=\"*60)\n",
    "print(\"TEST: Collection with missing file\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = api_call(\"POST\", \"/api/jobs/submit/process_raster_collection_v2\", {\n",
    "    \"container_name\": BRONZE_CONTAINER,\n",
    "    \"blob_list\": [\n",
    "        TEST_COLLECTION_FILES[0],  # exists\n",
    "        \"nonexistent_file_xyz123.tif\"  # does not exist\n",
    "    ],\n",
    "    \"collection_id\": \"test-missing\"\n",
    "})\n",
    "\n",
    "if result and \"error\" in str(result).lower():\n",
    "    print(\"\\nCorrectly rejected - lists missing files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-manual-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix A: Manual Job Status Check\n",
    "\n",
    "Check status of any CoreMachine job by job_id.\n",
    "\n",
    "**Endpoint:** `GET /api/jobs/status/{job_id}`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `job_id` | path | Yes | 64-character SHA256 job hash |\n",
    "\n",
    "**Response includes:**\n",
    "- `status`: `queued`, `processing`, `completed`, `failed`\n",
    "- `stage` / `totalStages`: Current progress\n",
    "- `taskSummary`: Count of tasks by status\n",
    "- `resultData`: Output data (when completed)\n",
    "- `error_details`: Error message (when failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-manual-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Job Status Check\n",
    "# Replace with your job_id\n",
    "manual_job_id = \"YOUR_JOB_ID_HERE\"\n",
    "\n",
    "if manual_job_id != \"YOUR_JOB_ID_HERE\":\n",
    "    check_job_status(manual_job_id)\n",
    "else:\n",
    "    print(\"Replace 'YOUR_JOB_ID_HERE' with an actual job_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-platform-status-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix B: Platform Request Status Check\n",
    "\n",
    "Check status of any Platform request by request_id (DDH-friendly).\n",
    "\n",
    "**Endpoint:** `GET /api/platform/status/{request_id}`\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `request_id` | path | Yes | 32-character request hash from Platform submission |\n",
    "\n",
    "**Response includes:**\n",
    "- `job_status`: Current job status\n",
    "- `job_stage`: Current stage number\n",
    "- `data_type`: `vector` or `raster`\n",
    "- `task_summary`: Task completion counts\n",
    "- `result_data`: Output URLs and metadata (when completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-platform-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platform Status Check\n",
    "# Replace with your request_id\n",
    "manual_request_id = \"YOUR_REQUEST_ID_HERE\"\n",
    "\n",
    "if manual_request_id != \"YOUR_REQUEST_ID_HERE\":\n",
    "    result = api_call(\"GET\", f\"/api/platform/status/{manual_request_id}\")\n",
    "    if result and result.get(\"success\"):\n",
    "        print(f\"\\nStatus: {result.get('job_status', 'N/A')}\")\n",
    "        print(f\"Stage: {result.get('job_stage', 'N/A')}\")\n",
    "else:\n",
    "    print(\"Replace 'YOUR_REQUEST_ID_HERE' with an actual request_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nzblzj2thz",
   "source": "---\n## Appendix C: Job Resubmit (Recovery)\n\nClean reset and resubmit a job with the same parameters. Useful for:\n- Failed jobs that need retry\n- Orphaned jobs stuck in processing\n- Jobs that completed but need to be re-run\n\n**Endpoint:** `POST /api/jobs/{job_id}/resubmit`\n\n### Path Parameters\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `job_id` | path | Yes | 64-character SHA256 job hash to resubmit |\n\n### Request Body (Optional)\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `dry_run` | boolean | `false` | Preview cleanup plan without executing |\n| `delete_blobs` | boolean | `false` | Also delete COG files from storage |\n| `force` | boolean | `false` | Resubmit even if job is currently processing |\n\n### Cleanup Actions Performed\n| Job Type | Actions |\n|----------|---------|\n| **Vector** | Delete tasks → Drop PostGIS table → Delete STAC item → Delete job |\n| **Raster** | Delete tasks → Delete STAC item → (optionally) Delete COGs → Delete job |\n\n### Response\n```json\n{\n    \"success\": true,\n    \"original_job_id\": \"abc123...\",\n    \"new_job_id\": \"abc123...\",\n    \"job_type\": \"process_raster_v2\",\n    \"cleanup_summary\": {\n        \"tasks_deleted\": 5,\n        \"job_deleted\": true,\n        \"tables_dropped\": [],\n        \"stac_items_deleted\": [\"item-123\"],\n        \"blobs_deleted\": []\n    },\n    \"message\": \"Job resubmitted successfully\"\n}\n```\n\n**Note:** Job IDs are deterministic (SHA256 of job_type + params), so resubmitting with same parameters generates the same job_id.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2zmu1h79yy5",
   "source": "# Job Resubmit - DRY RUN (preview cleanup plan)\n# Replace with your job_id\nresubmit_job_id = \"YOUR_JOB_ID_HERE\"\n\nif resubmit_job_id != \"YOUR_JOB_ID_HERE\":\n    result = api_call(\"POST\", f\"/api/jobs/{resubmit_job_id}/resubmit\", \n                      {\"dry_run\": True})\n    \n    if result and result.get(\"success\"):\n        print(f\"\\n{'='*60}\")\n        print(\"DRY RUN - Cleanup Plan\")\n        print(f\"{'='*60}\")\n        print(f\"Job Type: {result.get('job_type', 'N/A')}\")\n        print(f\"Job Status: {result.get('job_status', 'N/A')}\")\n        \n        plan = result.get(\"cleanup_plan\", {})\n        print(f\"\\nWould cleanup:\")\n        print(f\"  Tasks to delete: {plan.get('tasks_to_delete', 0)}\")\n        print(f\"  Tables to drop: {plan.get('tables_to_drop', [])}\")\n        print(f\"  STAC items to delete: {plan.get('stac_items_to_delete', [])}\")\n        print(f\"  Blobs to delete: {plan.get('blobs_to_delete', [])}\")\nelse:\n    print(\"Replace 'YOUR_JOB_ID_HERE' with an actual job_id\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pdvnb57wpj",
   "source": "# Job Resubmit - EXECUTE (uncomment to actually resubmit)\n# This will:\n#   1. Delete all tasks for the job\n#   2. Drop PostGIS tables (vector jobs)\n#   3. Delete STAC items\n#   4. Delete job record\n#   5. Resubmit with same parameters\n\n# resubmit_job_id = \"YOUR_JOB_ID_HERE\"\n# result = api_call(\"POST\", f\"/api/jobs/{resubmit_job_id}/resubmit\", \n#                   {\"dry_run\": False})\n# \n# if result and result.get(\"success\"):\n#     print(f\"\\nJob resubmitted!\")\n#     print(f\"Original Job ID: {result.get('original_job_id', 'N/A')[:16]}...\")\n#     print(f\"New Job ID: {result.get('new_job_id', 'N/A')[:16]}...\")\n#     print(f\"\\nCleanup Summary:\")\n#     summary = result.get(\"cleanup_summary\", {})\n#     print(f\"  Tasks deleted: {summary.get('tasks_deleted', 0)}\")\n#     print(f\"  Tables dropped: {summary.get('tables_dropped', [])}\")\n#     print(f\"  STAC items deleted: {summary.get('stac_items_deleted', [])}\")\n\nprint(\"Uncomment and run to actually resubmit the job\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8kz4d0o4nm",
   "source": "# Job Resubmit - FORCE (for stuck/processing jobs)\n# Use force=True to resubmit a job that is currently in 'processing' status\n# WARNING: This may cause duplicate work if the original job is still running\n\n# resubmit_job_id = \"YOUR_JOB_ID_HERE\"\n# result = api_call(\"POST\", f\"/api/jobs/{resubmit_job_id}/resubmit\", \n#                   {\"dry_run\": False, \"force\": True})\n\nprint(\"Uncomment and run to force resubmit a stuck job\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "piqahtd8mrq",
   "source": "---\n# Part 7: TiTiler Tile Server\n\nDynamic tile serving for Cloud-Optimized GeoTIFFs (COGs) stored in Azure Blob Storage.\n\n## Architecture\n\nTiTiler uses GDAL's `/vsiaz/` virtual filesystem for direct Azure Blob access:\n\n```\nClient Request → TiTiler → GDAL /vsiaz/ → Azure Blob Storage → COG Data → PNG/JPEG Tile\n```\n\n**No database required** - TiTiler can serve ANY COG in blob storage (cataloged or not).\n\n## URL Encoding\n\nThe `/vsiaz/{container}/{blob}` path must be URL-encoded:\n\n```python\n# Raw path\n/vsiaz/silver-cogs/myfile.tif\n\n# URL-encoded\n%2Fvsiaz%2Fsilver-cogs%2Fmyfile.tif\n```\n\nUse the `titiler_url()` helper function defined in the setup cell to build URLs automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lm74f8d7oji",
   "source": "# Execute cell for links to UI Dashboard\nshow_links({\n    \"TiTiler API Docs\": f\"{TITILER_URL}/docs\",\n    \"TiTiler Landing Page\": f\"{TITILER_URL}/\",\n    \"COG Viewer (Test File)\": titiler_url(\"/cog/viewer\", SILVER_COGS_CONTAINER, TEST_COG_FILE),\n    \"STAC Collections Gallery\": \"/api/interface/stac\"\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h5l48f0i20b",
   "source": "## 7.1 COG Metadata & Info\n\nGet metadata about a COG including bounds, CRS, band information, and data types.\n\n**Endpoint:** `GET /cog/info`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `url` | query | Yes | URL-encoded `/vsiaz/{container}/{blob}` path |\n\n**Response includes:**\n- `bounds`: Geographic extent [minx, miny, maxx, maxy]\n- `crs`: Coordinate reference system (e.g., \"EPSG:4326\")\n- `band_metadata`: List of band info (dtype, nodata, colorinterp)\n- `width` / `height`: Raster dimensions in pixels\n- `overviews`: Available overview levels",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2s0efiejw8w",
   "source": "# Get COG Info - metadata about the raster\ninfo_url = titiler_url(\"/cog/info\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\nprint(f\"Request URL:\\n{info_url}\\n\")\n\nresponse = requests.get(info_url, timeout=30)\nprint(f\"Status: {response.status_code}\")\n\nif response.status_code == 200:\n    info = response.json()\n    print(f\"\\n{'='*60}\")\n    print(\"COG INFO\")\n    print(f\"{'='*60}\")\n    print(f\"Bounds: {info.get('bounds', 'N/A')}\")\n    print(f\"CRS: {info.get('crs', 'N/A')}\")\n    print(f\"Size: {info.get('width', '?')} x {info.get('height', '?')} pixels\")\n    print(f\"Bands: {info.get('count', '?')}\")\n    print(f\"Data Type: {info.get('dtype', 'N/A')}\")\n    print(f\"Overviews: {info.get('overviews', [])}\")\n    \n    # Band details\n    band_meta = info.get('band_metadata', [])\n    if band_meta:\n        print(f\"\\nBand Metadata:\")\n        for i, band in enumerate(band_meta, 1):\n            print(f\"  Band {i}: {band}\")\nelse:\n    print(f\"Error: {response.text[:500]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zafcypcxat",
   "source": "## 7.2 Interactive Viewer\n\nOpen a COG in TiTiler's built-in interactive map viewer.\n\n**Endpoint:** `GET /cog/viewer`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `url` | query | Yes | URL-encoded `/vsiaz/{container}/{blob}` path |\n\nThe viewer automatically loads tiles and displays a Leaflet map with the COG data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5o8cc6ilor",
   "source": "# Generate Interactive Viewer URL\nviewer_url = titiler_url(\"/cog/viewer\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\nprint(\"Interactive Viewer URL:\")\nprint(viewer_url)\nprint(\"\\nOpen this URL in your browser to view the COG on an interactive map.\")\n\n# Display as clickable link\ndisplay(Markdown(f\"**[Open Viewer]({viewer_url})**\"))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4g3ces800tb",
   "source": "## 7.3 TileJSON & XYZ Tiles\n\nGet TileJSON specification or individual map tiles.\n\n### TileJSON\n**Endpoint:** `GET /cog/WebMercatorQuad/tilejson.json`\n\nReturns tile URL template, zoom levels, and bounds for use in mapping libraries (Leaflet, MapLibre, OpenLayers).\n\n### XYZ Tiles\n**Endpoint:** `GET /cog/tiles/WebMercatorQuad/{z}/{x}/{y}`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `url` | query | Yes | URL-encoded `/vsiaz/` path |\n| `z` | path | Yes | Zoom level (0-22) |\n| `x` | path | Yes | Tile column |\n| `y` | path | Yes | Tile row |\n| `scale` | query | No | Tile scale (`1` = 256px, `2` = 512px) |\n| `format` | query | No | Output format: `png`, `jpeg`, `webp` |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gaeiqdzu1m",
   "source": "# Get TileJSON specification\ntilejson_url = titiler_url(\"/cog/WebMercatorQuad/tilejson.json\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\nprint(f\"TileJSON URL:\\n{tilejson_url}\\n\")\n\nresponse = requests.get(tilejson_url, timeout=30)\nprint(f\"Status: {response.status_code}\")\n\nif response.status_code == 200:\n    tilejson = response.json()\n    print(f\"\\n{'='*60}\")\n    print(\"TILEJSON\")\n    print(f\"{'='*60}\")\n    print(f\"Name: {tilejson.get('name', 'N/A')}\")\n    print(f\"Min Zoom: {tilejson.get('minzoom', '?')}\")\n    print(f\"Max Zoom: {tilejson.get('maxzoom', '?')}\")\n    print(f\"Bounds: {tilejson.get('bounds', 'N/A')}\")\n    print(f\"Center: {tilejson.get('center', 'N/A')}\")\n    print(f\"\\nTile URL Template:\")\n    tiles = tilejson.get('tiles', [])\n    if tiles:\n        print(f\"  {tiles[0][:100]}...\")\nelse:\n    print(f\"Error: {response.text[:500]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ultkbwyhlmn",
   "source": "## 7.4 Rescale & Colormap\n\nControl how raster values are mapped to colors.\n\n### Rescale Parameter\n**Format:** `rescale=min,max`\n\nMaps raster values to 0-255 range for display:\n- Values ≤ min → 0 (black)\n- Values ≥ max → 255 (white/color max)\n- Values between → linearly interpolated\n\n### Colormap Parameter\n**Format:** `colormap_name={name}`\n\nAvailable colormaps: `viridis`, `plasma`, `inferno`, `magma`, `terrain`, `RdYlBu`, `Blues`, `Greens`, `Reds`, etc.\n\n### Common Use Cases\n\n| Data Type | Recommended Settings |\n|-----------|---------------------|\n| RGB Imagery | No rescale needed (uint8) |\n| 16-bit Satellite | `rescale=0,2000` |\n| DEM/Elevation | `rescale=0,3000&colormap_name=terrain` |\n| NDVI | `rescale=-1,1&colormap_name=RdYlGn` |\n| Temperature | `rescale=250,320&colormap_name=RdYlBu` |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5fivx3n6qkm",
   "source": "# Example: Viewer with rescale and colormap\n# Useful for single-band data like DEMs, temperature, or indices\n\n# Default viewer (auto-detect)\ndefault_viewer = titiler_url(\"/cog/viewer\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\n\n# With explicit rescale (for 16-bit data)\nrescaled_viewer = titiler_url(\"/cog/viewer\", SILVER_COGS_CONTAINER, TEST_COG_FILE,\n                               rescale=\"0,2000\")\n\n# With colormap (for single-band visualization)\ncolormap_viewer = titiler_url(\"/cog/viewer\", SILVER_COGS_CONTAINER, TEST_COG_FILE,\n                               rescale=\"0,255\",\n                               colormap_name=\"viridis\")\n\nprint(\"Viewer URL Examples:\")\nprint(f\"\\n1. Default (auto):\")\nprint(f\"   {default_viewer}\")\nprint(f\"\\n2. With rescale (16-bit data):\")\nprint(f\"   {rescaled_viewer}\")\nprint(f\"\\n3. With colormap (single-band):\")\nprint(f\"   {colormap_viewer}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c96st7y2gp",
   "source": "## 7.5 Multi-Band Selection (bidx)\n\nSelect which bands to display from multi-band rasters.\n\n### Band Index Parameter\n**Format:** `bidx=N` (repeat for multiple bands)\n\nBand indices are **1-based** (first band = 1).\n\n### Common Patterns\n\n| Sensor | Bands | Natural Color | False Color |\n|--------|-------|---------------|-------------|\n| 3-band RGB | 3 | `bidx=1&bidx=2&bidx=3` | N/A |\n| 4-band RGBA | 4 | `bidx=1&bidx=2&bidx=3` | (skip alpha) |\n| WorldView-2/3 | 8 | `bidx=5&bidx=3&bidx=2` | `bidx=7&bidx=5&bidx=3` |\n| Sentinel-2 | 10+ | `bidx=4&bidx=3&bidx=2` | `bidx=8&bidx=4&bidx=3` |\n| Landsat 8 | 11 | `bidx=4&bidx=3&bidx=2` | `bidx=5&bidx=4&bidx=3` |\n\n**Note:** Our ETL pipeline automatically selects optimal bands and stores them in STAC item properties.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "87nuv83rl5i",
   "source": "# Example: Band selection for multi-band imagery\n# Note: bidx parameters need to be added manually since titiler_url() \n# doesn't handle repeated parameters\n\nvsiaz = vsiaz_path(SILVER_COGS_CONTAINER, TEST_COG_FILE)\nencoded = urllib.parse.quote(vsiaz, safe='')\n\n# Standard RGB (bands 1, 2, 3)\nrgb_url = f\"{TITILER_URL}/cog/viewer?url={encoded}&bidx=1&bidx=2&bidx=3\"\n\n# Example for 8-band WorldView: Natural color (R=5, G=3, B=2)\nwv_natural = f\"{TITILER_URL}/cog/viewer?url={encoded}&bidx=5&bidx=3&bidx=2&rescale=0,2000\"\n\n# Example for 8-band WorldView: False color NIR (NIR1=7, R=5, G=3)\nwv_false = f\"{TITILER_URL}/cog/viewer?url={encoded}&bidx=7&bidx=5&bidx=3&rescale=0,2000\"\n\nprint(\"Band Selection Examples:\")\nprint(f\"\\n1. Standard RGB (bands 1,2,3):\")\nprint(f\"   {rgb_url}\")\nprint(f\"\\n2. WorldView Natural Color (bands 5,3,2 + rescale):\")\nprint(f\"   {wv_natural}\")\nprint(f\"\\n3. WorldView False Color NIR (bands 7,5,3 + rescale):\")\nprint(f\"   {wv_false}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gubshxirbqk",
   "source": "## 7.6 Statistics & Point Queries\n\nGet band statistics or extract values at specific coordinates.\n\n### Statistics\n**Endpoint:** `GET /cog/statistics`\n\nReturns min, max, mean, stddev, histogram for each band.\n\n### Point Query\n**Endpoint:** `GET /cog/point/{lon},{lat}`\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `url` | query | Yes | URL-encoded `/vsiaz/` path |\n| `lon` | path | Yes | Longitude (WGS84) |\n| `lat` | path | Yes | Latitude (WGS84) |\n\nReturns pixel values at the specified coordinate for all bands.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "uv87x0msdf",
   "source": "# Get COG Statistics\nstats_url = titiler_url(\"/cog/statistics\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\nprint(f\"Statistics URL:\\n{stats_url}\\n\")\n\nresponse = requests.get(stats_url, timeout=60)\nprint(f\"Status: {response.status_code}\")\n\nif response.status_code == 200:\n    stats = response.json()\n    print(f\"\\n{'='*60}\")\n    print(\"BAND STATISTICS\")\n    print(f\"{'='*60}\")\n    \n    for band_name, band_stats in stats.items():\n        print(f\"\\n{band_name}:\")\n        print(f\"  Min: {band_stats.get('min', 'N/A')}\")\n        print(f\"  Max: {band_stats.get('max', 'N/A')}\")\n        print(f\"  Mean: {band_stats.get('mean', 'N/A'):.2f}\" if band_stats.get('mean') else \"  Mean: N/A\")\n        print(f\"  Std Dev: {band_stats.get('std', 'N/A'):.2f}\" if band_stats.get('std') else \"  Std Dev: N/A\")\nelse:\n    print(f\"Error: {response.text[:500]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vlf3aduishf",
   "source": "# Point Query - Extract pixel value at specific coordinate\n# First get bounds from info to find a valid point\ninfo_url = titiler_url(\"/cog/info\", SILVER_COGS_CONTAINER, TEST_COG_FILE)\ninfo_response = requests.get(info_url, timeout=30)\n\nif info_response.status_code == 200:\n    info = info_response.json()\n    bounds = info.get('bounds', [-180, -90, 180, 90])\n    \n    # Get center point of the raster\n    center_lon = (bounds[0] + bounds[2]) / 2\n    center_lat = (bounds[1] + bounds[3]) / 2\n    \n    # Build point query URL\n    vsiaz = vsiaz_path(SILVER_COGS_CONTAINER, TEST_COG_FILE)\n    encoded = urllib.parse.quote(vsiaz, safe='')\n    point_url = f\"{TITILER_URL}/cog/point/{center_lon},{center_lat}?url={encoded}\"\n    \n    print(f\"Point Query: ({center_lon:.4f}, {center_lat:.4f})\")\n    print(f\"URL: {point_url}\\n\")\n    \n    response = requests.get(point_url, timeout=30)\n    print(f\"Status: {response.status_code}\")\n    \n    if response.status_code == 200:\n        point_data = response.json()\n        print(f\"\\n{'='*60}\")\n        print(\"POINT VALUES\")\n        print(f\"{'='*60}\")\n        print(f\"Coordinates: {point_data.get('coordinates', 'N/A')}\")\n        print(f\"Values: {point_data.get('values', 'N/A')}\")\n        \n        band_names = point_data.get('band_names', [])\n        values = point_data.get('values', [])\n        if band_names and values:\n            print(f\"\\nBy Band:\")\n            for name, val in zip(band_names, values):\n                print(f\"  {name}: {val}\")\n    else:\n        print(f\"Error: {response.text[:500]}\")\nelse:\n    print(\"Could not get COG info to determine valid point\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}