# ADO Work Items - Geospatial API for DDH

**Created**: 30 JAN 2026
**Revised**: 31 JAN 2026 (Consolidated US 1.1 DAG orchestration)
**Purpose**: Review/revise before ADO import
**Sprint Cadence**: 2-week sprints

---

## Standards Compliance

This platform implements **OGC (Open Geospatial Consortium)** standards throughout. OGC is the international consortium that defines how geospatial data is shared and accessed. Members include **Microsoft, AWS, Google, NASA, ESA, USGS**, and 500+ organizations worldwide.

**Why this matters**: OGC-compliant APIs work with any standards-based client—no vendor lock-in, no proprietary formats. Data published here can be consumed by any OGC-compatible tool.

| Standard | Purpose | Used By |
|----------|---------|---------|
| **OGC API - Features** | Query vector data via REST | ArcGIS, QGIS, MapLibre |
| **OGC API - Tiles** | Serve map tiles (raster & vector) | Google Maps, Mapbox, Leaflet |
| **STAC** | Catalog & discover datasets | Microsoft Planetary Computer, AWS Earth |
| **Cloud Optimized GeoTIFF** | Stream raster data without download | All major cloud platforms |

---

## Reading Guide

```
Structure:
  Epic (this project)
  └── Feature (business capability)
        └── User Story (deliverable)
              └── Task (implementation work) - only for in-progress items
```

**Conventions**:
- `[CLOSED]` = Complete, import as Closed
- `[ACTIVE]` = In Progress, import as Active
- `[NEW]` = Planned, import as New

---

# EPIC: Geospatial API for DDH

**Description**: Cloud-native geospatial data platform enabling DDH (Data Hub Dashboard) to publish and consume vector/raster datasets via modern APIs.

### Epic Requirements

These architectural requirements apply to all Features:

| Requirement | Standard | Description |
|-------------|----------|-------------|
| **Zero Trust Security** | [Microsoft Zero Trust](https://learn.microsoft.com/en-us/security/zero-trust/) | Azure Entra ID + Managed Identity only. No passwords, connection strings, or API keys. Verify explicitly, least privilege, assume breach. |
| **OGC Standards** | [OGC API](https://ogcapi.ogc.org/) | All APIs implement OGC specifications (see Standards Compliance above) |
| **Horizontally Scalable** | Cloud-Native | Containerized workloads scale out automatically under load |
| **Stateless Workers** | [12-Factor App](https://12factor.net/) | All processing nodes are stateless; state lives in PostgreSQL and Azure Storage |
| **Queue-Based Orchestration** | Event-Driven | Async job processing via Azure Service Bus; no synchronous long-running requests |
| **Infrastructure as Code** | [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/) | All Azure resources deployable via ARM/Bicep; no portal-only configuration |

---

## FEATURE 1: ETL Pipeline Orchestration `[ACTIVE]`

**What**: Job orchestration engine powering all data processing
**Why**: Enables reliable, scalable ETL without manual intervention
**Includes**: Standard pipelines + specialized large dataset processing (FATHOM, VirtualiZarr)

### User Stories

#### US 1.1: DAG-Based Job Orchestration `[CLOSED]`
**As a** platform operator
**I want** directed acyclic graph (DAG) workflows for job processing
**So that** complex multi-stage pipelines execute reliably with parallel task execution

**Acceptance Criteria**:
- [x] Node-based DAG workflow engine (jobs spawn stages, stages spawn parallel tasks)
- [x] Queue-based workload separation (jobs, container-tasks, functionapp-tasks)
- [x] Docker worker for heavy GDAL operations with checkpoint/resume

---

#### US 1.2: Metadata Architecture `[CLOSED]`
**As a** data steward
**I want** consistent metadata tracking
**So that** I can audit data provenance

**Acceptance Criteria**:
- [x] VectorMetadata and RasterMetadata models
- [x] geo.table_catalog and app.cog_metadata populated

---

#### US 1.3: Job Lifecycle `[CLOSED]`
**As a** platform operator
**I want** complete job status tracking
**So that** I can monitor pipeline health

**Acceptance Criteria**:
- [x] Job/task status in database
- [x] Approval record auto-created on completion
- [x] Job resubmit capability

---

#### US 1.4: FATHOM Flood Data Pipeline `[ACTIVE]`
**As a** climate data manager
**I want** FATHOM flood data processed, hosted, and queryable
**So that** analysts can access global flood risk via API

**Acceptance Criteria**:
- [x] Band stacking (8 return periods → 1 COG)
- [x] Spatial merge (N tiles → regional COG)
- [x] Proven on Rwanda test region
- [ ] STAC collection with datacube extension
- [ ] Full FATHOM inventory processed
- [ ] Query API: /api/fathom/point, /api/fathom/profile

---

#### US 1.5: VirtualiZarr Pipeline `[NEW]`
**As a** climate data manager
**I want** NetCDF files served without conversion
**So that** I avoid duplicating TB of data

**Acceptance Criteria**:
- [ ] Kerchunk JSON references generated
- [ ] TiTiler-xarray can read references

---

## FEATURE 2: Raster Data Pipeline `[CLOSED]`

**What**: Transform uploaded GeoTIFFs into cloud-optimized format for serving
**Why**: Enables raster data hosting via modern tile and data APIs
**Note**: Consumption APIs (tiles, point queries, statistics) are in Feature 5

### User Stories

#### US 2.1: Raster ETL Pipeline `[CLOSED]`
**As a** data publisher
**I want** to upload a GeoTIFF and have it converted to cloud-native format
**So that** it can be served as web map tiles and accessed directly

**Acceptance Criteria**:
- [x] Converts to OGC Cloud Optimized GeoTIFF (COG) standard
- [x] Handles large rasters (>1GB) via tiling pipeline
- [x] Enables HTTP range requests (no full download required)

---

#### US 2.2: Raster STAC Registration `[CLOSED]`
**As a** data consumer
**I want** raster datasets in the STAC catalog
**So that** I can discover and access them programmatically

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog with COG asset links
- [x] Discoverable via standard STAC API queries

---

#### US 2.3: Raster Unpublish `[CLOSED]`
**As a** data manager
**I want** to remove published raster datasets
**So that** storage is reclaimed and outdated data removed

**Acceptance Criteria**:
- [x] Deletes blob, STAC item, and metadata records

---

## FEATURE 3: Vector Data Pipeline `[CLOSED]`

**What**: Transform uploaded vector files into PostGIS tables ready for serving
**Why**: Enables data publishers to ingest vector data for API consumption
**Supported Formats**: CSV (lat/lon or WKT), KML, KMZ, Shapefile (zipped), GeoJSON, GeoPackage (.gpkg)
**Future**: GeoParquet support planned
**Note**: Consumption APIs (OGC Features, Vector Tiles) are in Feature 5

### User Stories

#### US 3.1: Vector ETL Pipeline `[CLOSED]`
**As a** data publisher
**I want** to upload vector files and have them processed automatically
**So that** they become available as queryable datasets

**Acceptance Criteria**:
- [x] Accepts CSV, KML, KMZ, Shapefile, GeoJSON, GeoPackage
- [x] Corrects geometry and topology errors for PostGIS data model
- [x] Transforms to EPSG:4326 and bulk loads to PostGIS

---

#### US 3.2: Vector STAC Registration `[CLOSED]`
**As a** data consumer
**I want** vector datasets registered in the STAC catalog
**So that** I can discover them using standard tools

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog (same standard used by AWS, Microsoft Planetary Computer)

---

#### US 3.3: Vector Unpublish `[CLOSED]`
**As a** data manager
**I want** to remove published vector datasets
**So that** outdated data doesn't remain accessible

**Acceptance Criteria**:
- [x] Drops PostGIS table, deletes STAC item, removes catalog entry

---

## FEATURE 4: Data Governance & Classification `[ACTIVE]`

**What**: Enforce data classification and provide auditable external delivery
**Why**: Prevents accidental exposure of OUO data; ensures compliance via audit trails
**Principle**: Data classification determines routing—only PUBLIC-approved data can exit to external systems

### User Stories

#### US 4.1: Classification Enforcement `[ACTIVE]`
**As a** data steward
**I want** data classification enforced at submission
**So that** miscategorized data doesn't leak externally

**Acceptance Criteria**:
- [x] Platform API validates access_level parameter
- [x] Rejects invalid classification values
- [ ] Pipeline tasks fail-fast if classification missing (Phase 2 - optional)

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.1.1 | Create AccessLevel enum | Done |
| T4.1.2 | Add Pydantic validator to PlatformRequest | Done |
| T4.1.3 | Add fail-fast check in pipeline tasks | Optional/Future |

---

#### US 4.2: Approval Workflow `[CLOSED]`
**As a** data steward
**I want** to approve datasets before they're published
**So that** I can review data quality and classification

**Acceptance Criteria**:
- [x] Approval record created when job completes
- [x] Approve/reject endpoints functional
- [x] STAC item updated on approval (published=true)
- [x] Revocation supported for approved items

---

#### US 4.3: Governed External Delivery `[ACTIVE]`
**As a** data steward
**I want** PUBLIC data delivered externally via Azure Data Factory
**So that** external delivery is auditable and OUO data cannot be accidentally exposed

**Why ADF**: Azure Data Factory provides enterprise audit trails, RBAC, and activity logging required for compliance. Classification-based routing ensures only explicitly PUBLIC-approved data triggers external delivery—OUO data never enters the external pipeline.

**Acceptance Criteria**:
- [x] ADF repository code complete with classification gate
- [ ] Environment variables configured
- [ ] ADF pipeline created in Azure with audit logging enabled
- [ ] End-to-end test verifying PUBLIC routes externally, OUO blocked

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.3.1 | Create AzureDataFactoryRepository | Done |
| T4.3.2 | Wire to ApprovalService with classification check | Done |
| T4.3.3 | Submit eService for ADF instance | Not started |
| T4.3.4 | Configure environment variables | Blocked by T4.3.3 |
| T4.3.5 | Create ADF pipeline with audit logging | Blocked by T4.3.3 |

---

## FEATURE 5: Service Layer (TiTiler/TiPG) `[ACTIVE]`

**What**: OGC-compliant APIs for consuming geospatial data
**Why**: Any OGC-compatible client can access data—no proprietary tools required
**Deployment**: Separate Docker application (`rmhtitiler` repository)
**Components**: TiTiler (raster), TiPG (vector), pgSTAC (catalog)

### User Stories

#### Raster APIs (TiTiler)

#### US 5.1: COG Tile Serving `[CLOSED]`
**As a** web developer
**I want** server-side rendered map tiles from COGs
**So that** I can build interactive web maps

**Acceptance Criteria**:
- [x] XYZ tile endpoint (/cog/tiles/{z}/{x}/{y})
- [x] Supports rescaling and colormap parameters
- [x] Preview/thumbnail generation

---

#### US 5.2: COG Data Access `[CLOSED]`
**As a** data analyst
**I want** to extract data from COGs without downloading files
**So that** I can analyze raster data efficiently via API

**Acceptance Criteria**:
- [x] Point queries: GET /cog/point/{lon}/{lat} returns pixel values for all bands
- [x] Image statistics: GET /cog/statistics returns band min/max/mean/std
- [x] Zonal statistics: POST /cog/statistics with GeoJSON polygon returns stats within area
- [x] HTTP range requests enable partial reads

---

#### US 5.3: Multidimensional Data `[CLOSED]`
**As a** climate analyst
**I want** tiles from Zarr/NetCDF data
**So that** I can visualize time-series climate data

**Acceptance Criteria**:
- [x] TiTiler-xarray integration working
- [x] Zarr tiles functional
- [x] Time dimension selectable

---

#### US 5.4: pgSTAC Mosaic Searches `[ACTIVE]`
**As a** data analyst
**I want** dynamic mosaics from STAC queries
**So that** I can view multiple datasets as one layer

**Acceptance Criteria**:
- [x] Mosaic search registration endpoint
- [x] Tiles from search results
- [x] Works with temporal queries
- [ ] Dynamic CQL queries for filtered mosaics

---

#### Vector APIs (TiPG)

#### US 5.5: OGC Features API `[CLOSED]`
**As a** data scientist
**I want** to query vector data via OGC Features API
**So that** I can use any OGC-compatible tool (QGIS, ArcGIS, Python, R)

**Acceptance Criteria**:
- [x] Implements OGC API - Features standard
- [x] GeoJSON responses compatible with all major clients
- [x] Supports bbox and property filters

---

#### US 5.6: Vector Tiles `[CLOSED]`
**As a** web developer
**I want** vector tiles for web maps
**So that** I can render large datasets efficiently

Compatible with MapLibre, Leaflet, OpenLayers, and any MVT-consuming client.

**Acceptance Criteria**:
- [x] Implements OGC API - Tiles standard
- [x] Serves Mapbox Vector Tiles (MVT) format

---

#### Catalog (pgSTAC)

#### US 5.7: STAC API `[CLOSED]`
**As a** data consumer
**I want** to search the STAC catalog
**So that** I can discover available datasets

Compatible with STAC Browser, pystac, and other standard clients.

**Acceptance Criteria**:
- [x] Implements OGC STAC API specification

---

#### Service Operations

#### US 5.8: Service Infrastructure `[CLOSED]`
**As a** platform operator
**I want** the service layer deployed and monitored
**So that** consumers can reliably access data

**Acceptance Criteria**:
- [x] TiTiler, TiPG, pgSTAC deployed as Docker application
- [x] Health probes (/livez, /readyz)
- [x] Azure Managed Identity authentication
- [x] Azure Blob Storage SAS token for COG access
- [x] OpenTelemetry observability

---

## FEATURE 6: Admin & Developer Portal `[ACTIVE]`

**What**: Web interfaces for operators and integrators
**Why**: Enables self-service operations and accelerates B2B integration

### User Stories

#### US 6.1: Admin Portal `[ACTIVE]`
**As a** platform administrator
**I want** web UI access to manage the system
**So that** I can monitor health, submit jobs, and manage data without CLI

**Acceptance Criteria**:
- [x] System health dashboard (Function App, Docker Worker, queues, database)
- [x] Job submission and monitoring interface
- [x] STAC browser with collection/item management
- [x] Vector data browser with map preview
- [x] Approval workflow interface
- [ ] DAG workflow visualization

---

#### US 6.2: Developer Integration Portal `[ACTIVE]`
**As a** B2B application developer
**I want** comprehensive API documentation and integration guides
**So that** I can integrate with the platform without support calls

**Acceptance Criteria**:
- [x] Swagger UI for interactive API exploration
- [x] ReDoc for readable API reference
- [x] OpenAPI 3.0 specification downloadable
- [x] CURL examples for common operations
- [ ] Integration quick-start guide

---

## FEATURE 7: DDH Platform Integration `[ACTIVE]`

**What**: Enable DDH application to consume geospatial services
**Why**: DDH is the primary consumer of this platform

### User Stories

#### US 7.1: API Contract Documentation `[ACTIVE]`
**As a** DDH developer
**I want** formal API documentation
**So that** I can integrate with the geospatial platform

**Acceptance Criteria**:
- [x] OpenAPI 3.0 specification published
- [x] Swagger UI available at /api/interface/swagger
- [ ] Request/response examples reviewed with DDH team

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T7.1.1 | Generate OpenAPI spec | Done |
| T7.1.2 | Deploy Swagger UI | Done |
| T7.1.3 | Review with DDH team | In Progress |

---

#### US 7.2: Identity & Access Configuration `[CLOSED]`
**As a** platform administrator
**I want** DDH to authenticate via Managed Identity
**So that** no secrets are shared between teams

**Acceptance Criteria**:
- [x] DDH Managed Identity granted Bronze Storage write access
- [x] DDH Managed Identity granted Platform API access
- [x] DDH Managed Identity granted Service Layer read access

---

#### US 7.3: Environment Provisioning `[NEW]`
**As a** platform administrator
**I want** configuration replicated to QA/UAT/Prod
**So that** DDH can test in each environment

**Acceptance Criteria**:
- [ ] QA environment configuration documented
- [ ] UAT environment provisioned
- [ ] Production environment provisioned

---

#### US 7.4: Integration Verification `[NEW]`
**As a** DDH developer
**I want** an end-to-end test suite
**So that** I can verify integration works correctly

**Acceptance Criteria**:
- [ ] Vector publish round-trip test
- [ ] Raster publish round-trip test
- [ ] OGC Features query test
- [ ] Job status polling test

---

# SUMMARY

| Feature | Status | User Stories | Complete |
|---------|--------|--------------|----------|
| F1: ETL Pipeline Orchestration | Active | 5 | 3/5 |
| F2: Raster Data Pipeline | Closed | 3 | 3/3 |
| F3: Vector Data Pipeline | Closed | 3 | 3/3 |
| F4: Data Governance & Classification | Active | 3 | 1/3 |
| F5: Service Layer (TiTiler/TiPG) | Active | 8 | 7/8 |
| F6: Admin & Developer Portal | Active | 2 | 0/2 |
| F7: DDH Platform Integration | Active | 4 | 1/4 |
| **TOTAL** | | **28** | **18/28 (64%)** |

---

*End of ADO Work Items*
