# ADO Work Items - Geospatial API for DDH

**Created**: 30 JAN 2026
**Revised**: 31 JAN 2026 (Value streams, tier classification, enablers)
**Purpose**: Review/revise before ADO import
**Sprint Cadence**: 2-week sprints

---

## Standards Compliance

This platform implements **OGC (Open Geospatial Consortium)** standards throughout. OGC is the international consortium that defines how geospatial data is shared and accessed. Members include **Microsoft, AWS, Google, NASA, ESA, USGS**, and 500+ organizations worldwide.

**Why this matters**: OGC-compliant APIs work with any standards-based clientâ€”no vendor lock-in, no proprietary formats. Data published here can be consumed by any OGC-compatible tool.

| Standard | Purpose | Used By |
|----------|---------|---------|
| **OGC API - Features** | Query vector data via REST | ArcGIS, QGIS, MapLibre |
| **OGC API - Tiles** | Serve map tiles (raster & vector) | Google Maps, Mapbox, Leaflet |
| **STAC** | Catalog & discover datasets | Microsoft Planetary Computer, AWS Earth |
| **Cloud Optimized GeoTIFF** | Stream raster data without download | All major cloud platforms |

---

## Reading Guide

```
Structure:
  Epic (this project)
  â””â”€â”€ Feature (business capability)
        â”œâ”€â”€ User Story (deliverable) - delivers value to named persona
        â”œâ”€â”€ Enabler (foundation) - technical work enabling future capabilities
        â””â”€â”€ Spike (exploration) - research/proof-of-concept
              â””â”€â”€ Task (implementation work) - only for in-progress items
```

**Conventions**:
- `[CLOSED]` = Complete, import as Closed
- `[ACTIVE]` = In Progress, import as Active
- `[NEW]` = Planned, import as New
- `[ENABLER: Type]` = Architecture, Infrastructure, Exploration, or Compliance
- `[EXPLORATION]` = Spike/proof-of-concept work

**User Tiers**:
- **Tier 1 (Platform Ops)**: Platform administrators managing infrastructure
- **Tier 2 (B2B)**: DDH team, DEC developers, data stewards integrating with platform
- **Tier 3 (B2C)**: Internal WB staff (OUO data), external researchers/public (PUBLIC data)

---

## Value Streams

This platform supports four primary value streams that cross multiple features and user tiers:

### VS1: Vector Data Publishing & Consumption
**Journey**: Data Publisher (B2B) â†’ Governance Approval (B2B) â†’ End User Access (B2C - Internal/Public)

**Actors**:
- **Tier 2 (B2B)**: DDH team, DEC developers, data stewards upload vector files
- **Tier 2 (B2B)**: Data stewards review and approve datasets  
- **Tier 3 (B2C)**: Internal WB staff (OUO data), external researchers/public (PUBLIC data) query via OGC Features API

**Flow**:
1. B2B user uploads CSV/GeoJSON/Shapefile/KML via Platform API (F3: US 3.1)
2. Platform validates, transforms to EPSG:4326, loads to PostGIS (F3: US 3.1)
3. STAC item created, pending approval (F3: US 3.2)
4. Data steward approves via governance workflow (F4: US 4.2)
5. B2C users query via OGC Features API in QGIS/Python/web maps (F5: US 5.5)
6. B2C users consume vector tiles in web applications (F5: US 5.6)
7. PUBLIC-approved data routed to external delivery via ADF (F4: US 4.3)

**Features**: F3 (Vector Pipeline) â†’ F4 (Governance) â†’ F5 (TiPG APIs)  
**Status**: âœ“ Complete end-to-end  
**Dependencies**: F1 (Job Orchestration)

**Service Layer Routing**:
- OUO data â†’ Internal service layer (WB staff only)
- PUBLIC data â†’ Internal service layer + External service layer (Cloudflare CDN â†’ public)

---

### VS2: Raster Data Publishing & Consumption  
**Journey**: Data Publisher (B2B) â†’ Governance Approval (B2B) â†’ End User Access (B2C - Internal/Public)

**Actors**:
- **Tier 2 (B2B)**: DDH team, climate data managers upload raster files
- **Tier 2 (B2B)**: Data stewards review and approve datasets
- **Tier 3 (B2C)**: Internal WB staff (OUO data), external researchers/public (PUBLIC data) access tiles and pixel data

**Flow**:
1. B2B user uploads GeoTIFF via Platform API (F2: US 2.1)
2. Platform converts to Cloud Optimized GeoTIFF (COG) (F2: US 2.1)
3. STAC item created with COG asset links (F2: US 2.2)
4. Data steward approves via governance workflow (F4: US 4.2)
5. B2C users view tiles in web maps (F5: US 5.1)
6. B2C users extract pixel values and statistics via API (F5: US 5.2)
7. PUBLIC-approved data routed to external delivery via ADF (F4: US 4.3)

**Features**: F2 (Raster Pipeline) â†’ F4 (Governance) â†’ F5 (TiTiler APIs)  
**Status**: âœ“ Complete end-to-end  
**Dependencies**: F1 (Job Orchestration)

**Service Layer Routing**:
- OUO data â†’ Internal service layer (WB staff only)
- PUBLIC data â†’ Internal service layer + External service layer (Cloudflare CDN â†’ public)

---

### VS3: B2B Platform Integration
**Journey**: B2B Developer â†’ Working Integration â†’ B2C Users Access Geospatial via B2B Application

**Actors**:
- **Tier 2 (B2B)**: DDH development team (ITSDA), other WB application teams integrating with geospatial platform
- **Tier 1 (Platform Ops)**: Platform administrators (Robert, Dimitar) configuring access and environments
- **Tier 3 (B2C)**: End users of B2B applications (DDH users, other WB app users) viewing geospatial data

**Flow**:
1. Platform admin grants B2B app Managed Identity access (F7: US 7.2) âœ“
2. B2B developers review API documentation via Developer Portal (F7: US 7.1, F6: US 6.2) - In Progress
3. Platform team provisions UAT/Prod environments (F7: US 7.3) - Planned
4. B2B developers integrate OGC APIs into their application
5. B2C users access geospatial features within B2B application interface

**Features**: F7 (Integration) â†’ F2/F3 (Pipelines) â†’ F5 (Service APIs) + F6 (Developer Portal)  
**Status**: âš  In Progress - Blocked by eService RITMs (see below)  
**Dependencies**: 
- RITM00009453692: ADF Contributor access for external delivery
- RITM00009453723: Entra Auth for API (replace client/secret with MI)
- RITM00009447599: Docker deployment ACN approval

**Note**: Developer Portal (F6: US 6.2) serves any B2B integrator, not limited to DDH

---

### VS4: NetCDF/Zarr Data Publishing (VirtualiZarr)
**Journey**: Climate Data Request (B2B) â†’ VirtualiZarr Processing â†’ Hosted Multidimensional Dataset (B2C Access)

**Actors**:
- **Tier 2 (B2B)**: DEC climate analysts with large NetCDF collections
- **Tier 1 (Platform Ops)**: Platform operators configuring VirtualiZarr pipelines
- **Tier 3 (B2C)**: Researchers accessing multidimensional climate data via TiTiler-xarray

**Flow**:
1. Climate analyst requests NetCDF hosting (previously planning Zarr conversion)
2. Platform generates Kerchunk JSON references (F1: US 1.5)
3. TiTiler-xarray serves NetCDF directly via JSON references (F5: US 5.3)
4. Data steward approves dataset (F4: US 4.2)
5. B2C users access time-series climate data via tiles and queries
6. **No data duplication**: Original NetCDF files served directly, avoiding TB-scale storage costs

**Features**: F1 (VirtualiZarr Pipeline) â†’ F4 (Governance) â†’ F5 (TiTiler-xarray)  
**Status**: ðŸ”„ Planned - Critical deliverable  
**Dependencies**: F5 (TiTiler-xarray integration - COMPLETE)

**Business Value**: Eliminates NetCDF â†’ Zarr conversion work, saves TB-scale storage duplication, enables immediate access to existing NetCDF collections

---

## Integration Points & Cross-Feature Dependencies

All value streams share common platform foundations:

| Foundation Feature | Enables | Used By |
|-------------------|---------|---------|
| **F1: Job Orchestration** | Async pipeline execution | VS1, VS2, VS4 |
| **F4: Governance** | Approval workflow & classification-based routing | VS1, VS2, VS3, VS4 |
| **F5: Service Layer** | OGC-compliant data consumption | VS1, VS2, VS3, VS4 |
| **F6: Admin Portal** | Platform operations & monitoring | Platform Ops only (Tier 1) |
| **F6: Developer Portal** | API documentation & integration guides | B2B integrators (Tier 2) |

**Service Layer Architecture**:
- **Internal Service Layer**: All data (OUO + PUBLIC) accessible to authenticated WB staff
- **External Service Layer**: Only PUBLIC-approved data routed via ADF to Cloudflare CDN for public access
- **No REST APIs for confidential data**: Access control via Entra ID, not API layer

**Critical Path for DDH Go-Live (VS3)**:
1. âœ“ Identity & Access (US 7.2) - COMPLETE
2. ðŸ”„ API Documentation (US 7.1) - Review with ITSDA team pending
3. âš  Docker Deployment (RITM00009447599) - ACN approval required
4. âš  ADF External Delivery (RITM00009453692) - Contributor access required
5. âš  Entra Auth Migration (RITM00009453723) - MI authentication required

---

# EPIC: Geospatial API for DDH

**Description**: Cloud-native geospatial data platform enabling DDH (Data Hub Dashboard) to publish and consume vector/raster datasets via modern APIs.

### Epic Requirements

These architectural requirements apply to all Features:

| Requirement | Standard | Description |
|-------------|----------|-------------|
| **Zero Trust Security** | [Microsoft Zero Trust](https://learn.microsoft.com/en-us/security/zero-trust/) | Azure Entra ID + Managed Identity only. No passwords, connection strings, or API keys. Verify explicitly, least privilege, assume breach. |
| **OGC Standards** | [OGC API](https://ogcapi.ogc.org/) | All APIs implement OGC specifications (see Standards Compliance above) |
| **Horizontally Scalable** | Cloud-Native | Containerized workloads scale out automatically under load |
| **Stateless Workers** | [12-Factor App](https://12factor.net/) | All processing nodes are stateless; state lives in PostgreSQL and Azure Storage |
| **Queue-Based Orchestration** | Event-Driven | Async job processing via Azure Service Bus; no synchronous long-running requests |
| **Infrastructure as Code** | [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/) | All Azure resources deployable via ARM/Bicep; no portal-only configuration |

---

## FEATURE 1: ETL Pipeline Orchestration `[ACTIVE]`

**What**: Job orchestration engine powering all data processing
**Why**: Enables reliable, scalable ETL without manual intervention
**Includes**: Standard pipelines + specialized large dataset processing

**User Journey**:
1. **Tier 2 (B2B - Data publishers)**: Submit data processing requests via Platform API
2. **Tier 1 (Platform - Job orchestration)**: Queue tasks, execute pipelines, track status
3. **Tier 2 (B2B - Data stewards)**: Monitor job progress, approve completed datasets
4. **Tier 3 (B2C)**: Access processed data via OGC APIs

**Consumed By**: Features 2, 3 (data pipelines), Feature 4 (governance)  
**Depends On**: Azure Service Bus, PostgreSQL, Docker worker infrastructure

### User Stories

#### US 1.1: Serverless Job Orchestration `[CLOSED]`
**As a** platform operator **(Platform Ops - Tier 1)**
**I want** serverless job orchestration via Azure Functions and Service Bus
**So that** ETL pipelines execute reliably without infrastructure management

**Acceptance Criteria**:
- [x] Jobâ†’Stageâ†’Task workflow pattern
- [x] Queue-based workload separation (jobs, container-tasks, functionapp-tasks)
- [x] Docker worker for heavy GDAL operations with checkpoint/resume

---

#### US 1.3: Job Lifecycle `[CLOSED]`
**As a** platform operator **(Platform Ops - Tier 1)**
**I want** complete job status tracking
**So that** I can monitor pipeline health

**Acceptance Criteria**:
- [x] Job/task status in database
- [x] Approval record auto-created on completion
- [x] Job resubmit capability

---

#### US 1.5: VirtualiZarr NetCDF Pipeline `[NEW]`
**As a** climate data manager **(B2B - DEC climate analysts)**
**I want** NetCDF files served without conversion
**So that** I avoid duplicating TB of data and can immediately access existing NetCDF collections

**Acceptance Criteria**:
- [ ] Kerchunk JSON references generated for NetCDF files
- [ ] TiTiler-xarray can read references and serve tiles
- [ ] STAC collection created with VirtualiZarr asset links
- [ ] Performance validation vs. COG conversion approach

**Business Value**: Eliminates planned NetCDF â†’ Zarr conversion work, saves TB-scale storage costs

---

### Enablers

#### EN 1.2: Metadata Architecture `[ENABLER: Architecture]` `[CLOSED]`
**Purpose**: Foundation for data provenance tracking and audit trails

**Deliverables**:
- VectorMetadata and RasterMetadata schemas
- geo.table_catalog and app.cog_metadata tables populated
- Metadata populated on all ETL operations

**Enables**: Future provenance audit UI (not yet scoped)

**Status**: Complete

---

#### EN 1.6: DAG Orchestration Migration `[ENABLER: Architecture]` `[ACTIVE]`
**Purpose**: Refactor orchestration engine to support complex pipeline dependencies

**Rationale**: Current linear stage progression limits pipeline expressiveness. DAG-based execution enables conditional branching and parallel paths required for future advanced workflows.

**Acceptance Criteria**:
- [ ] DAG workflow engine replacing linear stage progression
- [ ] Conditional branching based on task results
- [ ] Parallel execution paths within single job

**Enables**: Future complex pipelines (multi-source joins, conditional processing)

**Status**: In Progress (self-initiated refactor, not client-requested)

---

### Spikes

#### SPIKE 1.4: FATHOM Complex Raster Processing `[EXPLORATION]`
**Purpose**: Demonstrate platform capability for complex multi-band raster processing

**Context**: DEC climate analysts requested FATHOM flood data hosting. Requirements satisfied by standard raster pipeline (F2) + TiTiler endpoints (F5). This spike explores advanced processing patterns (band stacking, spatial merge) applicable to similar large datasets.

**Outcomes**:
- [x] Band stacking (8 return periods â†’ 1 COG) - Proven on Rwanda
- [x] Spatial merge (N tiles â†’ regional COG) - Proven on Rwanda
- [ ] STAC collection with datacube extension - If pursued
- [ ] Full FATHOM inventory processed - If client funds

**Access Method**: Standard TiTiler endpoints (tiles, point queries, statistics)  
**Custom Endpoints**: Out of scope (e.g., `/api/fathom/point`)

**Status**: Proof-of-concept complete, further work deemphasized

---

## FEATURE 2: Raster Data Pipeline `[CLOSED]`

**What**: Transform uploaded GeoTIFFs into cloud-optimized format for serving
**Why**: Enables raster data hosting via modern tile and data APIs
**Note**: Consumption APIs (tiles, point queries, statistics) are in Feature 5

**User Journey**:
1. **Tier 2 (B2B - DDH team, climate data managers)**: Upload GeoTIFF via Platform API
2. **Tier 1 (Platform)**: Job orchestration executes COG conversion pipeline
3. **Tier 2 (B2B - Data steward)**: Review and approve dataset
4. **Tier 3 (B2C - Internal/external users)**: Access tiles and pixel data via TiTiler
5. **Tier 3 (B2C - Public only)**: PUBLIC-approved data routed to Cloudflare CDN

**Consumed By**: Feature 5 (TiTiler APIs - US 5.1, 5.2, 5.3, 5.4)  
**Depends On**: Feature 1 (Job Orchestration - US 1.1), Feature 4 (Governance - US 4.2, 4.3)

### User Stories

#### US 2.1: Raster ETL Pipeline `[CLOSED]`
**As a** data publisher **(B2B - DDH team, climate data managers)**
**I want** to upload a GeoTIFF and have it converted to cloud-native format
**So that** it can be served as web map tiles and accessed directly

**Acceptance Criteria**:
- [x] Converts to OGC Cloud Optimized GeoTIFF (COG) standard
- [x] Handles large rasters (>1GB) via tiling pipeline
- [x] Enables HTTP range requests (no full download required)

---

#### US 2.2: Raster STAC Registration `[CLOSED]`
**As a** data consumer **(B2C - Internal staff for OUO, public for PUBLIC data)**
**I want** raster datasets in the STAC catalog
**So that** I can discover and access them programmatically

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog with COG asset links
- [x] Discoverable via standard STAC API queries

---

#### US 2.3: Raster Unpublish `[CLOSED]`
**As a** data manager **(B2B - Data stewards)**
**I want** to remove published raster datasets
**So that** storage is reclaimed and outdated data removed

**Acceptance Criteria**:
- [x] Deletes blob, STAC item, and metadata records

---

## FEATURE 3: Vector Data Pipeline `[CLOSED]`

**What**: Transform uploaded vector files into PostGIS tables ready for serving
**Why**: Enables data publishers to ingest vector data for API consumption
**Supported Formats**: CSV (lat/lon or WKT), KML, KMZ, Shapefile (zipped), GeoJSON, GeoPackage (.gpkg)
**Future**: GeoParquet support planned
**Note**: Consumption APIs (OGC Features, Vector Tiles) are in Feature 5

**User Journey**:
1. **Tier 2 (B2B - DDH team, DEC developers)**: Upload vector file via Platform API
2. **Tier 1 (Platform)**: Job orchestration validates, transforms, loads to PostGIS
3. **Tier 2 (B2B - Data steward)**: Review and approve dataset
4. **Tier 3 (B2C - Internal/external users)**: Query via OGC Features API, consume vector tiles
5. **Tier 3 (B2C - Public only)**: PUBLIC-approved data routed to Cloudflare CDN

**Consumed By**: Feature 5 (TiPG APIs - US 5.5, 5.6)  
**Depends On**: Feature 1 (Job Orchestration - US 1.1), Feature 4 (Governance - US 4.2, 4.3)

### User Stories

#### US 3.1: Vector ETL Pipeline `[CLOSED]`
**As a** data publisher **(B2B - DDH team, DEC developers)**
**I want** to upload vector files and have them processed automatically
**So that** they become available as queryable datasets

**Acceptance Criteria**:
- [x] Accepts CSV, KML, KMZ, Shapefile, GeoJSON, GeoPackage
- [x] Corrects geometry and topology errors for PostGIS data model
- [x] Transforms to EPSG:4326 and bulk loads to PostGIS

---

#### US 3.2: Vector STAC Registration `[CLOSED]`
**As a** data consumer **(B2C - Internal staff for OUO, public for PUBLIC data)**
**I want** vector datasets registered in the STAC catalog
**So that** I can discover them using standard tools

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog (same standard used by AWS, Microsoft Planetary Computer)

---

#### US 3.3: Vector Unpublish `[CLOSED]`
**As a** data manager **(B2B - Data stewards)**
**I want** to remove published vector datasets
**So that** outdated data doesn't remain accessible

**Acceptance Criteria**:
- [x] Drops PostGIS table, deletes STAC item, removes catalog entry

---

## FEATURE 4: Data Governance & Classification `[ACTIVE]`

**What**: Enforce data classification and provide auditable external delivery
**Why**: Prevents accidental exposure of OUO data; ensures compliance via audit trails
**Principle**: Data classification determines routingâ€”only PUBLIC-approved data can exit to external systems

**User Journey**:
1. **Tier 2 (B2B - Data publisher)**: Submits data with classification (OUO/PUBLIC)
2. **Tier 1 (Platform)**: Validates classification, creates approval record
3. **Tier 2 (B2B - Data steward)**: Approves/rejects dataset
4. **Tier 1 (Platform)**: Routes PUBLIC data to ADF for external delivery
5. **Tier 3 (B2C)**: OUO â†’ Internal only, PUBLIC â†’ Internal + External (Cloudflare)

**Consumed By**: All publishing features (F2, F3) and consumption features (F5)  
**Depends On**: Feature 1 (Job Orchestration)

**Service Layer Architecture**:
- **Internal Service Layer**: All approved data (OUO + PUBLIC) - WB staff only
- **External Service Layer**: PUBLIC-approved data only - Cloudflare CDN â†’ public
- **Access Control**: Entra ID authentication, not API-level restrictions

### User Stories

#### US 4.1: Classification Enforcement `[ACTIVE]`
**As a** data steward **(B2B)**
**I want** data classification enforced at submission
**So that** miscategorized data doesn't leak externally

**Acceptance Criteria**:
- [x] Platform API validates access_level parameter
- [x] Rejects invalid classification values
- [ ] Pipeline tasks fail-fast if classification missing (Phase 2 - optional)

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.1.1 | Create AccessLevel enum | Done |
| T4.1.2 | Add Pydantic validator to PlatformRequest | Done |
| T4.1.3 | Add fail-fast check in pipeline tasks | Optional/Future |

---

#### US 4.2: Approval Workflow `[CLOSED]`
**As a** data steward **(B2B)**
**I want** to approve datasets before they're published
**So that** I can review data quality and classification

**Acceptance Criteria**:
- [x] Approval record created when job completes
- [x] Approve/reject endpoints functional
- [x] STAC item updated on approval (published=true)
- [x] Revocation supported for approved items

---

#### US 4.3: Governed External Delivery `[ACTIVE]`
**As a** data steward **(B2B)**
**I want** PUBLIC data delivered externally via Azure Data Factory
**So that** external delivery is auditable and OUO data cannot be accidentally exposed

**Why ADF**: Azure Data Factory provides enterprise audit trails, RBAC, and activity logging required for compliance. Classification-based routing ensures only explicitly PUBLIC-approved data triggers external deliveryâ€”OUO data never enters the external pipeline.

**Acceptance Criteria**:
- [x] ADF repository code complete with classification gate
- [ ] Environment variables configured
- [ ] ADF pipeline created in Azure with audit logging enabled
- [ ] End-to-end test verifying PUBLIC routes externally, OUO blocked

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.3.1 | Create AzureDataFactoryRepository | Done |
| T4.3.2 | Wire to ApprovalService with classification check | Done |
| T4.3.3 | Submit eService for ADF instance | Not started |
| T4.3.4 | Configure environment variables | Blocked by T4.3.3 |
| T4.3.5 | Create ADF pipeline with audit logging | Blocked by T4.3.3 |

---

## FEATURE 5: Service Layer (TiTiler/TiPG) `[ACTIVE]`

**What**: OGC-compliant APIs for consuming geospatial data
**Why**: Any OGC-compatible client can access dataâ€”no proprietary tools required
**Deployment**: Separate Docker application (`rmhtitiler` repository)
**Components**: TiTiler (raster), TiPG (vector), pgSTAC (catalog)

**User Journey**:
1. **Tier 3 (B2C - Internal users)**: Access all approved data (OUO + PUBLIC) via internal service layer
2. **Tier 3 (B2C - External users)**: Access PUBLIC-only data via external service layer (Cloudflare)
3. **Standards-based access**: Any OGC-compatible client (QGIS, ArcGIS, Python, web maps)

**Consumed By**: B2B applications embedding geospatial (DDH, other WB apps) + Direct B2C access  
**Depends On**: Features 2, 3 (data pipelines), Feature 4 (governance routing)

**Deployment**: Separate Docker application (`rmhtitiler` repository)

### User Stories

#### Raster APIs (TiTiler)

#### US 5.1: COG Tile Serving `[CLOSED]`
**As a** web developer **(B2C - Building applications for internal/external users)**
**I want** server-side rendered map tiles from COGs
**So that** I can build interactive web maps

**Acceptance Criteria**:
- [x] XYZ tile endpoint (/cog/tiles/{z}/{x}/{y})
- [x] Supports rescaling and colormap parameters
- [x] Preview/thumbnail generation

---

#### US 5.2: COG Data Access `[CLOSED]`
**As a** data analyst **(B2C - Internal researchers, external analysts)**
**I want** to extract data from COGs without downloading files
**So that** I can analyze raster data efficiently via API

**Acceptance Criteria**:
- [x] Point queries: GET /cog/point/{lon}/{lat} returns pixel values for all bands
- [x] Image statistics: GET /cog/statistics returns band min/max/mean/std
- [x] Zonal statistics: POST /cog/statistics with GeoJSON polygon returns stats within area
- [x] HTTP range requests enable partial reads

---

#### US 5.3: Multidimensional Data `[CLOSED]`
**As a** climate analyst **(B2C - Internal researchers)**
**I want** tiles from Zarr/NetCDF data
**So that** I can visualize time-series climate data

**Acceptance Criteria**:
- [x] TiTiler-xarray integration working
- [x] Zarr tiles functional
- [x] Time dimension selectable

---

#### US 5.4: pgSTAC Mosaic Searches `[ACTIVE]`
**As a** data analyst **(B2C - Internal researchers, external analysts)**
**I want** dynamic mosaics from STAC queries
**So that** I can view multiple datasets as one layer

**Acceptance Criteria**:
- [x] Mosaic search registration endpoint
- [x] Tiles from search results
- [x] Works with temporal queries
- [ ] Dynamic CQL queries for filtered mosaics

---

#### Vector APIs (TiPG)

#### US 5.5: OGC Features API `[CLOSED]`
**As a** data scientist **(B2C - Internal/external researchers)**
**I want** to query vector data via OGC Features API
**So that** I can use any OGC-compatible tool (QGIS, ArcGIS, Python, R)

**Acceptance Criteria**:
- [x] Implements OGC API - Features standard
- [x] GeoJSON responses compatible with all major clients
- [x] Supports bbox and property filters

---

#### US 5.6: Vector Tiles `[CLOSED]`
**As a** web developer **(B2C - Building applications for internal/external users)**
**I want** vector tiles for web maps
**So that** I can render large datasets efficiently

Compatible with MapLibre, Leaflet, OpenLayers, and any MVT-consuming client.

**Acceptance Criteria**:
- [x] Implements OGC API - Tiles standard
- [x] Serves Mapbox Vector Tiles (MVT) format

---

#### Catalog (pgSTAC)

#### US 5.7: STAC API `[CLOSED]`
**As a** data consumer **(B2C - Internal staff, external researchers, public)**
**I want** to search the STAC catalog
**So that** I can discover available datasets

Compatible with STAC Browser, pystac, and other standard clients.

**Acceptance Criteria**:
- [x] Implements OGC STAC API specification

---

#### Service Operations

#### US 5.8: Service Infrastructure `[CLOSED]`
**As a** platform operator **(Platform Ops - Tier 1)**
**I want** the service layer deployed and monitored
**So that** consumers can reliably access data

**Acceptance Criteria**:
- [x] TiTiler, TiPG, pgSTAC deployed as Docker application
- [x] Health probes (/livez, /readyz)
- [x] Azure Managed Identity authentication
- [x] Azure Blob Storage SAS token for COG access
- [x] OpenTelemetry observability

---

## FEATURE 6: Admin & Developer Portal `[ACTIVE]`

**What**: Web interfaces for operators and integrators
**Why**: Enables self-service operations and accelerates B2B integration

**User Journey (Admin Portal - US 6.1)**:
1. **Tier 1 (Platform Ops - Robert, Dimitar)**: Monitor system health, manage jobs, approve datasets
2. **Platform only**: Not exposed to B2B clients until operational maturity demonstrated

**User Journey (Developer Portal - US 6.2)**:
1. **Tier 2 (B2B - DDH ITSDA, other integrators)**: Review API docs, test integration
2. **Self-service**: Accelerates B2B integration without support calls

**Consumed By**: Platform Ops (US 6.1), B2B integrators (US 6.2)  
**Depends On**: Feature 5 (provides APIs to document)

### User Stories

#### US 6.1: Admin Portal `[ACTIVE]`
**As a** platform administrator **(Platform Ops - Tier 1 - Robert, Dimitar, future ops team)**
**I want** web UI access to manage the system
**So that** I can monitor health, submit jobs, and manage data without CLI

**Acceptance Criteria**:
- [x] System health dashboard (Function App, Docker Worker, queues, database)
- [x] Job submission and monitoring interface
- [x] STAC browser with collection/item management
- [x] Vector data browser with map preview
- [x] Approval workflow interface
- [ ] DAG workflow visualization

---

#### US 6.2: Developer Integration Portal `[ACTIVE]`
**As a** B2B application developer **(B2B - DDH ITSDA team, other WB app integrators)**
**I want** comprehensive API documentation and integration guides
**So that** I can integrate with the platform without support calls

**Acceptance Criteria**:
- [x] Swagger UI for interactive API exploration
- [x] ReDoc for readable API reference
- [x] OpenAPI 3.0 specification downloadable
- [x] CURL examples for common operations
- [ ] Integration quick-start guide

---

## FEATURE 7: DDH Platform Integration `[ACTIVE]`

**What**: Enable DDH application to consume geospatial services
**Why**: DDH is the primary consumer of this platform

**User Journey**:
1. **Tier 1 (Platform Ops)**: Configure DDH Managed Identity access (US 7.2) âœ“
2. **Tier 2 (B2B - DDH ITSDA)**: Review API documentation (US 7.1) - In Progress
3. **Tier 1 (Platform Ops)**: Provision UAT/Prod environments (US 7.3) - Planned
4. **Tier 2 (B2B - DDH ITSDA)**: Integrate OGC APIs into DDH application
5. **Tier 3 (B2C - DDH users)**: Access geospatial features within DDH interface

**Consumed By**: DDH application (primary B2B client)  
**Depends On**: Features 2, 3, 4, 5 (entire platform stack)

### User Stories

#### US 7.1: API Contract Documentation `[ACTIVE]`
**As a** DDH developer **(B2B - DDH ITSDA team)**
**I want** formal API documentation
**So that** I can integrate with the geospatial platform

**Acceptance Criteria**:
- [x] OpenAPI 3.0 specification published
- [x] Swagger UI available at /api/interface/swagger
- [ ] Request/response examples reviewed with DDH team

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T7.1.1 | Generate OpenAPI spec | Done |
| T7.1.2 | Deploy Swagger UI | Done |
| T7.1.3 | Review with DDH team | In Progress |

---

#### US 7.2: Identity & Access Configuration `[CLOSED]`
**As a** platform administrator **(Platform Ops - Tier 1)**
**I want** DDH to authenticate via Managed Identity
**So that** no secrets are shared between teams

**Acceptance Criteria**:
- [x] DDH Managed Identity granted Bronze Storage write access
- [x] DDH Managed Identity granted Platform API access
- [x] DDH Managed Identity granted Service Layer read access

---

#### US 7.3: Environment Provisioning `[NEW]`
**As a** platform administrator **(Platform Ops - Tier 1)**
**I want** configuration replicated to QA/UAT/Prod
**So that** DDH can test in each environment

**Acceptance Criteria**:
- [ ] QA environment configuration documented
- [ ] UAT environment provisioned
- [ ] Production environment provisioned

---

### Enablers

#### EN 7.4: Integration Test Suite `[ENABLER: Exploration]` `[OPTIONAL]`
**Purpose**: Provide DDH team with ready-to-use integration tests

**Rationale**: Optional test suite provided as platform service. DDH can use for CI/CD validation or ignore and write their own tests.

**Acceptance Criteria**:
- [ ] Vector publish round-trip test
- [ ] Raster publish round-trip test
- [ ] OGC Features query test
- [ ] Job status polling test

**Enables**: DDH CI/CD integration (if they choose to use)

**Status**: Planned (gift to DDH, not required for integration)

---

# SUMMARY

| Feature | Status | Stories | Enablers | Spikes | Complete |
|---------|--------|---------|----------|--------|----------|
| F1: ETL Pipeline Orchestration | Active | 3 | 2 | 1 | 2/6 |
| F2: Raster Data Pipeline | Closed | 3 | 0 | 0 | 3/3 |
| F3: Vector Data Pipeline | Closed | 3 | 0 | 0 | 3/3 |
| F4: Data Governance & Classification | Active | 3 | 0 | 0 | 1/3 |
| F5: Service Layer (TiTiler/TiPG) | Active | 8 | 0 | 0 | 7/8 |
| F6: Admin & Developer Portal | Active | 2 | 0 | 0 | 0/2 |
| F7: DDH Platform Integration | Active | 3 | 1 | 0 | 1/4 |
| **TOTAL** | | **25** | **3** | **1** | **17/29 (59%)** |

**User Stories**: 25 (deliver direct value to named personas)  
**Enablers**: EN 1.2 (Metadata Architecture - CLOSED), EN 1.6 (DAG Migration - ACTIVE), EN 7.4 (Test Suite - OPTIONAL)  
**Spikes**: SPIKE 1.4 (FATHOM - Proof-of-concept complete)

**Value Stream Status**:
- âœ“ **VS1: Vector Publishing** - Complete end-to-end
- âœ“ **VS2: Raster Publishing** - Complete end-to-end
- âš  **VS3: B2B Integration** - In Progress (blocked by eService RITMs)
- ðŸ”„ **VS4: VirtualiZarr NetCDF** - Planned (critical deliverable for DEC)

---

*End of ADO Work Items*