# ADO Work Items - Geospatial API for DDH

**Created**: 30 JAN 2026
**Purpose**: Review/revise before ADO import
**Sprint Cadence**: 2-week sprints

---

## Reading Guide

```
Structure:
  Epic (this project)
  └── Feature (business capability)
        └── User Story (deliverable)
              └── Task (implementation work) - only for in-progress items
```

**Conventions**:
- `[CLOSED]` = Complete, import as Closed
- `[ACTIVE]` = In Progress, import as Active
- `[NEW]` = Planned, import as New
- Tags shown as `#tag`

---

# EPIC: Geospatial API for DDH

**Description**: Cloud-native geospatial data platform replacing ArcGIS Server. Enables DDH (Data Hub Dashboard) to publish and consume vector/raster datasets via modern APIs.

**Tags**: `#geo-platform`

---

## FEATURE 1: Vector Data Pipeline `[CLOSED]`

**What**: Transform uploaded shapefiles into queryable map layers
**Why**: Enables data publishers to share vector data without ArcGIS Server
**Tags**: `#vector` `#complete`

### User Stories

#### US 1.1: Vector ETL Pipeline `[CLOSED]`
**As a** data publisher
**I want** to upload a shapefile and have it processed automatically
**So that** it becomes available as a queryable dataset

**Acceptance Criteria**:
- [x] Accepts Shapefile, GeoJSON, GeoPackage formats
- [x] Transforms to EPSG:4326 if needed
- [x] Bulk loads to PostGIS via COPY protocol
- [x] Completes within 5 minutes for files <100MB

---

#### US 1.2: Vector STAC Registration `[CLOSED]`
**As a** data consumer
**I want** vector datasets registered in the STAC catalog
**So that** I can discover them alongside raster data

**Acceptance Criteria**:
- [x] STAC item created with `postgis://` asset link
- [x] Bounding box calculated from geometry
- [x] Feature count and geometry type in metadata

---

#### US 1.3: Vector Unpublish `[CLOSED]`
**As a** data manager
**I want** to remove published vector datasets
**So that** outdated data doesn't remain accessible

**Acceptance Criteria**:
- [x] Drops PostGIS table
- [x] Deletes STAC item
- [x] Removes from geo.table_catalog

---

#### US 1.4: OGC Features API `[CLOSED]`
**As a** GIS analyst
**I want** to query vector data via OGC Features API
**So that** I can use standard tools and workflows

**Acceptance Criteria**:
- [x] GET /collections lists all vector tables
- [x] GET /collections/{id}/items returns GeoJSON
- [x] Supports bbox and property filters

---

#### US 1.5: Vector Tiles `[CLOSED]`
**As a** web developer
**I want** vector tiles (MVT) for web maps
**So that** I can render large datasets efficiently

**Acceptance Criteria**:
- [x] MVT tiles at /collections/{id}/tiles/{z}/{x}/{y}
- [x] Works with MapLibre GL JS

---

## FEATURE 2: Raster Data Pipeline `[CLOSED]`

**What**: Transform uploaded GeoTIFFs into cloud-optimized, tile-able datasets
**Why**: Enables raster data hosting without ArcGIS Image Server
**Tags**: `#raster` `#complete`

### User Stories

#### US 2.1: Raster ETL Pipeline `[CLOSED]`
**As a** data publisher
**I want** to upload a GeoTIFF and have it converted to COG
**So that** it can be served as web map tiles

**Acceptance Criteria**:
- [x] Validates GeoTIFF format and CRS
- [x] Creates Cloud Optimized GeoTIFF (COG)
- [x] Uploads to Azure Blob Storage
- [x] Records metadata in app.cog_metadata

---

#### US 2.2: Large Raster Pipeline `[CLOSED]`
**As a** data publisher
**I want** to process rasters larger than 1GB
**So that** I can publish high-resolution imagery

**Acceptance Criteria**:
- [x] Tiles large rasters into manageable chunks
- [x] Creates tile COGs in parallel
- [x] Builds mosaic index for seamless viewing

---

#### US 2.3: Raster STAC Registration `[CLOSED]`
**As a** data consumer
**I want** raster datasets in the STAC catalog
**So that** I can discover and access them programmatically

**Acceptance Criteria**:
- [x] STAC item with COG asset link
- [x] Band information in metadata
- [x] Projection and resolution recorded

---

#### US 2.4: Raster Unpublish `[CLOSED]`
**As a** data manager
**I want** to remove published raster datasets
**So that** storage is reclaimed and outdated data removed

**Acceptance Criteria**:
- [x] Deletes blob from Azure Storage
- [x] Deletes STAC item
- [x] Removes from app.cog_metadata

---

#### US 2.5: COG Tile Serving `[CLOSED]`
**As a** web developer
**I want** dynamic tiles from COGs
**So that** I can display raster data in web maps

**Acceptance Criteria**:
- [x] XYZ tiles at /cog/tiles/{z}/{x}/{y}
- [x] Supports rescaling and colormap parameters
- [x] Works with Leaflet and MapLibre

---

#### US 2.6: Point Queries `[CLOSED]`
**As a** data analyst
**I want** to query raster values at specific coordinates
**So that** I can extract data for analysis

**Acceptance Criteria**:
- [x] GET /cog/point/{lon}/{lat} returns pixel values
- [x] Returns all bands at location
- [x] Works with any COG in storage

---

## FEATURE 3: DDH Platform Integration `[ACTIVE]`

**What**: Enable DDH application to consume geospatial services
**Why**: DDH is the primary B2B consumer of this platform
**Tags**: `#ddh` `#integration` `#external-dependency`

### User Stories

#### US 3.1: API Contract Documentation `[CLOSED]`
**As a** DDH developer
**I want** formal API documentation
**So that** I can integrate with the geospatial platform

**Acceptance Criteria**:
- [x] OpenAPI 3.0 specification published
- [x] Swagger UI available at /api/interface/swagger
- [x] Request/response examples documented

---

#### US 3.2: Identity & Access Configuration `[ACTIVE]`
**As a** platform administrator
**I want** DDH to authenticate via Managed Identity
**So that** no secrets are shared between teams

**Acceptance Criteria**:
- [ ] DDH Managed Identity granted Bronze Storage write access
- [ ] DDH Managed Identity granted Platform API access
- [ ] DDH Managed Identity granted Service Layer read access

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T3.2.1 | Document required RBAC roles | Done |
| T3.2.2 | Request DDH Managed Identity from ITSDA | Waiting |
| T3.2.3 | Grant blob write access to Bronze | Blocked by T3.2.2 |
| T3.2.4 | Grant API access | Blocked by T3.2.2 |
| T3.2.5 | Verify end-to-end authentication | Blocked |

---

#### US 3.3: Environment Provisioning `[NEW]`
**As a** platform administrator
**I want** configuration replicated to QA/UAT/Prod
**So that** DDH can test in each environment

**Acceptance Criteria**:
- [ ] QA environment configuration documented
- [ ] UAT environment provisioned
- [ ] Production environment provisioned

---

#### US 3.4: Integration Verification `[NEW]`
**As a** DDH developer
**I want** an end-to-end test suite
**So that** I can verify integration works correctly

**Acceptance Criteria**:
- [ ] Vector publish round-trip test
- [ ] Raster publish round-trip test
- [ ] OGC Features query test
- [ ] Job status polling test

---

## FEATURE 4: Security & Data Classification `[ACTIVE]`

**What**: Control data classification and enable secure external delivery
**Why**: Ensures PUBLIC data can be shared externally while OUO stays internal
**Tags**: `#security` `#compliance`

### User Stories

#### US 4.1: Classification Enforcement `[ACTIVE]`
**As a** data steward
**I want** data classification enforced at submission
**So that** miscategorized data doesn't leak externally

**Acceptance Criteria**:
- [x] Platform API validates access_level parameter
- [x] Rejects invalid classification values
- [ ] Pipeline tasks fail-fast if classification missing (Phase 2 - optional)

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.1.1 | Create AccessLevel enum | Done |
| T4.1.2 | Add Pydantic validator to PlatformRequest | Done |
| T4.1.3 | Add fail-fast check in pipeline tasks | Optional/Future |

---

#### US 4.2: Approval Workflow `[CLOSED]`
**As a** data steward
**I want** to approve datasets before they're published
**So that** I can review data quality and classification

**Acceptance Criteria**:
- [x] Approval record created when job completes
- [x] Approve/reject endpoints functional
- [x] STAC item updated on approval (published=true)
- [x] Revocation supported for approved items

---

#### US 4.3: ADF External Delivery `[ACTIVE]`
**As a** data steward
**I want** PUBLIC data automatically sent to external partners
**So that** approved public data reaches its audience

**Acceptance Criteria**:
- [x] ADF repository code complete
- [ ] Environment variables configured (ADF_SUBSCRIPTION_ID, ADF_FACTORY_NAME)
- [ ] ADF pipeline created in Azure
- [ ] End-to-end test with PUBLIC dataset

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T4.3.1 | Create AzureDataFactoryRepository | Done |
| T4.3.2 | Wire to ApprovalService | Done |
| T4.3.3 | Submit eService for ADF instance | Not started |
| T4.3.4 | Configure environment variables | Blocked by T4.3.3 |
| T4.3.5 | Create ADF pipeline | Blocked by T4.3.3 |
| T4.3.6 | Test end-to-end delivery | Blocked |

---

## FEATURE 5: Consumer APIs (TiTiler/TiPG) `[CLOSED]`

**What**: Standards-based APIs for consuming geospatial data
**Why**: Replaces ArcGIS Server for web map applications
**Tags**: `#service-layer` `#b2c` `#complete`
**Note**: Implemented in separate repository `rmhtitiler`

### User Stories

#### US 5.1: COG Tile Serving `[CLOSED]`
**As a** web developer
**I want** dynamic map tiles from COGs
**So that** I can build interactive web maps

**Acceptance Criteria**:
- [x] TiTiler deployed and operational
- [x] XYZ tile endpoint functional
- [x] Preview/thumbnail generation working

---

#### US 5.2: Vector Tiles & OGC Features `[CLOSED]`
**As a** web developer
**I want** vector tiles and GeoJSON from PostGIS
**So that** I can render vector data efficiently

**Acceptance Criteria**:
- [x] TiPG deployed and operational
- [x] MVT tiles at standard endpoint
- [x] OGC Features API compliant

---

#### US 5.3: Multidimensional Data `[CLOSED]`
**As a** climate analyst
**I want** tiles from Zarr/NetCDF data
**So that** I can visualize time-series climate data

**Acceptance Criteria**:
- [x] TiTiler-xarray integration working
- [x] Zarr tiles functional
- [x] Time dimension selectable

---

#### US 5.4: pgSTAC Mosaic Searches `[CLOSED]`
**As a** data analyst
**I want** dynamic mosaics from STAC queries
**So that** I can view multiple datasets as one layer

**Acceptance Criteria**:
- [x] Mosaic search registration endpoint
- [x] Tiles from search results
- [x] Works with temporal queries

---

#### US 5.5: STAC API `[CLOSED]`
**As a** data consumer
**I want** to search the STAC catalog
**So that** I can discover available datasets

**Acceptance Criteria**:
- [x] Collection listing endpoint
- [x] Item search with filters
- [x] Conformance with STAC spec

---

#### US 5.6: Service Operations `[CLOSED]`
**As a** platform operator
**I want** health checks and monitoring
**So that** I know the service layer is operational

**Acceptance Criteria**:
- [x] Health probes (/livez, /readyz)
- [x] Azure Managed Identity auth
- [x] OpenTelemetry observability

---

## FEATURE 6: ETL Pipeline Infrastructure `[CLOSED]`

**What**: Job orchestration engine powering all data processing
**Why**: Enables reliable, scalable ETL without manual intervention
**Tags**: `#infrastructure` `#v0.8` `#complete`

### User Stories

#### US 6.1: CoreMachine Orchestration `[CLOSED]`
**As a** platform operator
**I want** reliable job orchestration
**So that** ETL jobs complete without manual intervention

**Acceptance Criteria**:
- [x] Job→Stage→Task pattern implemented
- [x] "Last task turns out the lights" stage completion
- [x] Retry logic for transient failures
- [x] Job events table for audit trail

---

#### US 6.2: Docker Worker `[CLOSED]`
**As a** platform operator
**I want** heavy operations to run in Docker
**So that** GDAL/geopandas have full resources

**Acceptance Criteria**:
- [x] Docker worker deployed (rmhheavyapi)
- [x] Listens to container-tasks queue
- [x] OSGeo GDAL image with full drivers
- [x] Checkpoint/resume for long tasks

---

#### US 6.3: Queue Architecture `[CLOSED]`
**As a** platform architect
**I want** clean queue separation
**So that** different workloads don't interfere

**Acceptance Criteria**:
- [x] geospatial-jobs queue (orchestrator)
- [x] container-tasks queue (Docker worker)
- [x] functionapp-tasks queue (lightweight ops)

---

#### US 6.4: Metadata Architecture `[CLOSED]`
**As a** data steward
**I want** consistent metadata tracking
**So that** I can audit data provenance

**Acceptance Criteria**:
- [x] VectorMetadata model and repository
- [x] RasterMetadata model and repository
- [x] geo.table_catalog populated
- [x] app.cog_metadata populated

---

#### US 6.5: Job Lifecycle `[CLOSED]`
**As a** platform operator
**I want** complete job status tracking
**So that** I can monitor pipeline health

**Acceptance Criteria**:
- [x] Job status in app.jobs table
- [x] Task status in app.tasks table
- [x] Approval record auto-created on completion
- [x] Job resubmit capability

---

## FEATURE 7: H3 Analytics Pipeline `[ACTIVE]`

**What**: Transform hosted data into H3-aggregated analysis products
**Why**: Enables climate risk analysis at multiple spatial resolutions
**Tags**: `#analytics` `#h3`

### User Stories

#### US 7.1: H3 Grid Infrastructure `[CLOSED]`
**As a** data analyst
**I want** an H3 hexagonal grid in the database
**So that** I can aggregate data to standard cells

**Acceptance Criteria**:
- [x] h3.cells table with geometries
- [x] h3.cell_admin0 country mappings
- [x] Resolutions 2-7 populated

---

#### US 7.2: Grid Bootstrap System `[CLOSED]`
**As a** platform operator
**I want** to generate H3 grids automatically
**So that** new regions can be added easily

**Acceptance Criteria**:
- [x] bootstrap_h3_land_grid_pyramid job
- [x] Cascade generation (res 2 → res 7)
- [x] Country boundary intersection

---

#### US 7.3: Raster→H3 Aggregation `[CLOSED]`
**As a** climate analyst
**I want** raster values aggregated to H3 cells
**So that** I can analyze flood risk by hexagon

**Acceptance Criteria**:
- [x] Zonal statistics (mean, max, count, etc.)
- [x] Results in h3.zonal_stats table
- [x] Works with COGs in Azure Storage

---

#### US 7.4: Vector→H3 Aggregation `[NEW]`
**As a** analyst
**I want** point/polygon counts aggregated to H3
**So that** I can analyze building density by hexagon

**Acceptance Criteria**:
- [ ] Point-in-cell counting
- [ ] Polygon intersection counting
- [ ] Category grouping (e.g., by building type)

---

#### US 7.5: H3 Export to OGC Features `[CLOSED]`
**As a** web developer
**I want** H3 results as OGC Features
**So that** I can display them on a web map

**Acceptance Criteria**:
- [x] Denormalized wide-format export
- [x] Available in geo schema
- [x] Queryable via TiPG

---

#### US 7.6: GeoParquet Export `[NEW]`
**As a** data scientist
**I want** H3 results as GeoParquet
**So that** I can analyze them in DuckDB/Databricks

**Acceptance Criteria**:
- [ ] PostgreSQL → GeoParquet export
- [ ] Columnar format for OLAP
- [ ] Available in Bronze/Silver storage

---

#### US 7.7: Building Exposure Analysis `[NEW]`
**As a** climate risk analyst
**I want** building flood exposure calculations
**So that** I can report % buildings in flood zones

**Acceptance Criteria**:
- [ ] MS Building Footprints loaded
- [ ] FATHOM flood depth sampled at buildings
- [ ] Results aggregated to H3 cells

---

## FEATURE 8: Large Dataset Processing `[ACTIVE]`

**What**: Specialized pipelines for FATHOM/CMIP6-scale data
**Why**: Standard pipelines can't handle TB-scale climate datasets
**Tags**: `#large-data` `#fathom` `#climate`

### User Stories

#### US 8.1: FATHOM ETL Operations `[CLOSED]`
**As a** climate data manager
**I want** FATHOM flood tiles processed into COGs
**So that** they can be served as map tiles

**Acceptance Criteria**:
- [x] Band stacking (8 return periods → 1 COG)
- [x] Spatial merge (N tiles → regional COG)
- [x] Proven on Rwanda test region

---

#### US 8.2: FATHOM Data Hosting `[NEW]`
**As a** climate analyst
**I want** complete FATHOM coverage available
**So that** I can query flood risk globally

**Acceptance Criteria**:
- [ ] STAC collection with datacube extension
- [ ] Scenario-based organization
- [ ] All return periods accessible

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T8.2.1 | Define STAC collection schema | Not started |
| T8.2.2 | Process full FATHOM inventory | Not started |
| T8.2.3 | Create STAC items for all COGs | Not started |
| T8.2.4 | Verify tile serving | Not started |

---

#### US 8.3: VirtualiZarr Pipeline `[NEW]`
**As a** climate data manager
**I want** NetCDF files served without conversion
**So that** I avoid duplicating TB of data

**Acceptance Criteria**:
- [ ] Kerchunk JSON references generated
- [ ] TiTiler-xarray can read references
- [ ] No data duplication required

---

#### US 8.4: CMIP6 Data Hosting `[NEW]`
**As a** climate analyst
**I want** curated CMIP6 projections available
**So that** I can analyze future climate scenarios

**Acceptance Criteria**:
- [ ] Variables: tas, pr, tasmax, tasmin
- [ ] Scenarios: SSP2-4.5, SSP5-8.5
- [ ] Time range: 2020-2100

---

#### US 8.5: FATHOM Query API `[NEW]`
**As a** application developer
**I want** flood-specific query endpoints
**So that** I don't need to understand STAC

**Acceptance Criteria**:
- [ ] /api/fathom/point?flood_type=X&return_period=Y
- [ ] /api/fathom/profile (all return periods)
- [ ] Semantic parameter names

---

## FEATURE 9: Operator Admin Portal `[CLOSED]`

**What**: Self-service interface for platform operators
**Why**: Enables operations without developer assistance
**Tags**: `#admin` `#ui` `#complete`

### User Stories

#### US 9.1: HTMX Integration `[CLOSED]`
**As a** frontend developer
**I want** interactive UI without custom JavaScript
**So that** the interface is maintainable

**Acceptance Criteria**:
- [x] HTMX library integrated
- [x] Cascading dropdowns work
- [x] Form submissions work
- [x] Auto-polling for status updates

---

#### US 9.2: System Dashboard `[CLOSED]`
**As a** platform operator
**I want** a unified health view
**So that** I can see system status at a glance

**Acceptance Criteria**:
- [x] Function App + Docker Worker health
- [x] Queue status (message counts, DLQ)
- [x] Database statistics
- [x] Cross-system view

---

#### US 9.3: Pipeline Workflow Hub `[CLOSED]`
**As a** data publisher
**I want** to submit and monitor jobs via UI
**So that** I don't need to use curl

**Acceptance Criteria**:
- [x] Job submission forms
- [x] Recent jobs table
- [x] Task viewer with logs
- [x] Job timeline visualization

---

#### US 9.4: API Documentation Hub `[CLOSED]`
**As a** integrator
**I want** API docs in the admin portal
**So that** I can learn integration patterns

**Acceptance Criteria**:
- [x] Swagger UI embedded
- [x] OpenAPI JSON downloadable
- [x] CURL examples shown

---

#### US 9.5: STAC Browser `[CLOSED]`
**As a** data manager
**I want** to browse STAC collections/items
**So that** I can verify published data

**Acceptance Criteria**:
- [x] Collection listing
- [x] Item listing with thumbnails
- [x] Raster curator with map preview
- [x] Approve/reject buttons

---

#### US 9.6: OGC Features Browser `[CLOSED]`
**As a** data manager
**I want** to browse vector collections
**So that** I can verify published vector data

**Acceptance Criteria**:
- [x] Collection listing with feature counts
- [x] Vector curator with map preview
- [x] MapLibre + TiPG integration

---

#### US 9.7: Approvals Interface `[NEW]`
**As a** data steward
**I want** a dedicated approvals queue UI
**So that** I can efficiently review pending items

**Acceptance Criteria**:
- [ ] List pending approvals
- [ ] Filter by classification, job type
- [ ] Bulk approve/reject
- [ ] Review notes interface

##### Tasks
| Task | Description | Status |
|------|-------------|--------|
| T9.7.1 | Design approvals list page | Not started |
| T9.7.2 | Add filter dropdowns | Not started |
| T9.7.3 | Implement bulk actions | Not started |
| T9.7.4 | Add review notes modal | Not started |

---

# SUMMARY

| Feature | Status | User Stories | Complete |
|---------|--------|--------------|----------|
| Vector Data Pipeline | Closed | 5 | 5/5 |
| Raster Data Pipeline | Closed | 6 | 6/6 |
| DDH Platform Integration | Active | 4 | 1/4 |
| Security & Data Classification | Active | 3 | 1/3 |
| Consumer APIs (TiTiler/TiPG) | Closed | 6 | 6/6 |
| ETL Pipeline Infrastructure | Closed | 5 | 5/5 |
| H3 Analytics Pipeline | Active | 7 | 4/7 |
| Large Dataset Processing | Active | 5 | 1/5 |
| Operator Admin Portal | Closed | 7 | 6/7 |
| **TOTAL** | | **48** | **35/48 (73%)** |

---

*End of ADO Work Items*
