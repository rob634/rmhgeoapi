# V0.8 AzCopy Integration Plan

**Created**: 29 JAN 2026
**Status**: PLANNED
**Priority**: MEDIUM - Performance optimization (5-10x speedup on blob transfers)
**Epic**: E7 Pipeline Infrastructure

---

## Summary

Replace Python SDK streaming with AzCopy for blob ↔ Azure Files transfers in the Docker worker. This is a low-effort, high-impact optimization.

| Metric | Current (Python SDK) | With AzCopy |
|--------|---------------------|-------------|
| Transfer speed | Baseline | **5-10x faster** |
| Parallelism | Limited (GIL) | Native Go goroutines |
| Memory efficiency | 32MB chunks | Optimized buffering |
| Implementation effort | N/A | ~1 hour |

---

## Background

### Current Architecture

```
Bronze Blob → Python SDK (32MB chunks) → Azure Files Mount → GDAL → Python SDK → Silver Blob
              ↑ SLOW (GIL, chunked)                          ↑ SLOW (GIL, chunked)
```

The Docker worker currently uses `stream_blob_to_mount()` and `stream_mount_to_blob()` methods in `infrastructure/blob.py` which:
- Download/upload in 32MB chunks via Python Azure SDK
- Are limited by Python's GIL for true parallelism
- Work reliably but are not optimized for throughput

### With AzCopy

```
Bronze Blob → AzCopy (parallel, Go) → Azure Files Mount → GDAL → AzCopy → Silver Blob
              ↑ 5-10x FASTER                              ↑ 5-10x FASTER
```

AzCopy is a Go-based CLI tool optimized for Azure Storage transfers:
- Native goroutines for true parallelism
- Parallel block uploads (8MB blocks uploaded concurrently)
- Optimized memory buffering
- Server-side copy when possible (blob-to-blob only)

---

## Why AzCopy vs ADF?

| Approach | Pros | Cons |
|----------|------|------|
| **AzCopy in Docker** | Easy integration, 5-10x speedup, keeps current architecture | Still uses container compute |
| **Azure Data Factory** | Managed service, no container compute | Architectural change, more complex orchestration |

**Recommendation**: Start with AzCopy for quick win. Consider ADF for future if copy becomes the bottleneck.

---

## Authentication Options

### Option 1: SAS Tokens (Simplest)

**No RBAC changes needed** - works with existing storage credentials.

```python
# Generate SAS at runtime (already implemented in BlobRepository)
sas = blob_repo.get_blob_url_with_sas(container, blob_path, hours=1).split('?')[1]

# AzCopy with SAS
subprocess.run([
    "azcopy", "copy",
    f"https://{account}.blob.core.windows.net/{container}/{blob}?{sas}",
    f"/mounts/etl-temp/file.tif"
])
```

### Option 2: Managed Identity (More Secure)

Requires RBAC setup but no secrets in URLs.

**Environment variable** (set in docker.env):
```bash
AZCOPY_AUTO_LOGIN_TYPE=MSI
```

**Required RBAC roles**:

| Operation | Blob Storage Role | Azure Files Role |
|-----------|-------------------|------------------|
| Read (source) | `Storage Blob Data Reader` | `Storage File Data Privileged Reader` |
| Write (sink) | `Storage Blob Data Contributor` | `Storage File Data Privileged Contributor` |

**Important**: Azure Files requires the "Privileged" roles for REST/OAuth access (AzCopy uses REST, not SMB).

The current `Storage File SMB Share Contributor` role only works for SMB access. AzCopy requires:
- `readFileBackupSemantics/action`
- `writeFileBackupSemantics/action`

These are only in the "Privileged" variants.

---

## Implementation Plan

### Step 1: Dockerfile Change (1 line)

Add AzCopy installation after the existing apt-get block:

```dockerfile
# Add after line 41 (after rm -rf /var/lib/apt/lists/*)
RUN curl -sL https://aka.ms/downloadazcopy-v10-linux | tar xz --strip-components=1 -C /usr/local/bin
```

**Full context** (lines 37-42 become):
```dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3-pip \
    libpq-dev \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && curl -sL https://aka.ms/downloadazcopy-v10-linux | tar xz --strip-components=1 -C /usr/local/bin
```

### Step 2: RBAC Permission (eService Request Required)

**Note**: RBAC assignments require an eService request. Use the template below.

#### eService Request Template

**Request Type**: Azure RBAC Role Assignment

**Summary**: Add Storage File Data Privileged Contributor role to Docker worker managed identity for AzCopy REST API access

**Description**:

```
We need to add an Azure RBAC role assignment to enable our Docker worker (rmhheavyapi)
to use AzCopy for high-performance file transfers to Azure Files.

CURRENT STATE:
- Web App "rmhheavyapi" has system-assigned managed identity enabled
- Identity currently has "Storage File SMB Share Contributor" role
- This role only allows SMB protocol access

PROBLEM:
- AzCopy uses REST APIs (not SMB) for file transfers
- REST API access requires the "Privileged" variant of the role
- Without this role, AzCopy fails with authentication errors

REQUESTED CHANGE:
Add the following role assignment:

  Principal: rmhheavyapi (Web App managed identity)
  Role: Storage File Data Privileged Contributor
  Scope: Storage account containing the Azure Files share used by Docker worker

  (Alternatively, scope to the specific file share if more restrictive access preferred)

JUSTIFICATION:
- AzCopy provides 5-10x performance improvement over Python SDK
- Required for processing large geospatial files efficiently
- No additional secrets or credentials needed (uses existing managed identity)
- Role is scoped only to the storage account used by this application

TECHNICAL DETAILS:
- The "Privileged" role adds these data actions required for REST/OAuth:
  - Microsoft.Storage/storageAccounts/fileServices/readFileBackupSemantics/action
  - Microsoft.Storage/storageAccounts/fileServices/writeFileBackupSemantics/action
- Reference: https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/storage

AZURE CLI EQUIVALENT (for reference):
  PRINCIPAL_ID=$(az webapp identity show --name rmhheavyapi --resource-group rmhazure_rg --query principalId -o tsv)
  az role assignment create \
    --assignee $PRINCIPAL_ID \
    --role "Storage File Data Privileged Contributor" \
    --scope "/subscriptions/<sub-id>/resourceGroups/rmhazure_rg/providers/Microsoft.Storage/storageAccounts/<storage-account>"
```

**Priority**: Medium (performance optimization, not blocking)

**Contact**: [Your name/email]

---

#### Azure CLI Commands (for IT to execute)

```bash
# Get the managed identity principal ID
PRINCIPAL_ID=$(az webapp identity show --name rmhheavyapi --resource-group rmhazure_rg --query principalId -o tsv)

# Add Storage File Data Privileged Contributor (for Azure Files via REST)
# Replace <sub-id> and <storage-account> with actual values
az role assignment create \
  --assignee $PRINCIPAL_ID \
  --role "Storage File Data Privileged Contributor" \
  --scope "/subscriptions/<sub-id>/resourceGroups/rmhazure_rg/providers/Microsoft.Storage/storageAccounts/<storage-account>"

# Also ensure Blob Data Reader/Contributor exists for blob operations (if not already assigned)
az role assignment create \
  --assignee $PRINCIPAL_ID \
  --role "Storage Blob Data Reader" \
  --scope "/subscriptions/<sub-id>/resourceGroups/rmhazure_rg/providers/Microsoft.Storage/storageAccounts/rmhazuregeobronze"
```

### Step 3: Environment Variable

Add to `docker.env`:
```bash
# Enable AzCopy to use managed identity
AZCOPY_AUTO_LOGIN_TYPE=MSI
```

### Step 4: Python Wrapper Module

Create `infrastructure/azcopy.py`:

```python
# ============================================================================
# AZCOPY WRAPPER
# ============================================================================
# EPOCH: 4 - ACTIVE
# STATUS: Infrastructure - High-performance blob transfers
# PURPOSE: Wrapper for AzCopy CLI for 5-10x faster blob ↔ mount transfers
# CREATED: 29 JAN 2026
# EXPORTS: azcopy_blob_to_mount, azcopy_mount_to_blob, azcopy_available
# ============================================================================
"""
AzCopy Wrapper for High-Performance Blob Transfers.

Provides Python interface to AzCopy CLI for transferring data between
Azure Blob Storage and mounted filesystems (Azure Files).

Performance: 5-10x faster than Python Azure SDK due to:
- Native Go parallelism (goroutines vs GIL)
- Parallel block uploads
- Optimized memory buffering

Authentication:
- Managed Identity: Set AZCOPY_AUTO_LOGIN_TYPE=MSI
- SAS Token: Pass sas_token parameter

Usage:
    from infrastructure.azcopy import azcopy_blob_to_mount, azcopy_available

    if azcopy_available():
        result = azcopy_blob_to_mount(
            blob_url="https://account.blob.core.windows.net/container/blob.tif",
            mount_path="/mounts/etl-temp/input.tif",
            sas_token="sv=2021-06-08&..."  # Optional if using managed identity
        )
"""

import subprocess
import shutil
import time
from typing import Dict, Any, Optional
from pathlib import Path

from util_logger import LoggerFactory, ComponentType

logger = LoggerFactory.create_logger(ComponentType.INFRASTRUCTURE, "azcopy")


def azcopy_available() -> bool:
    """Check if AzCopy is installed and available."""
    return shutil.which("azcopy") is not None


def azcopy_blob_to_mount(
    blob_url: str,
    mount_path: str,
    sas_token: Optional[str] = None,
    log_level: str = "ERROR"
) -> Dict[str, Any]:
    """
    Copy blob to mounted filesystem using AzCopy.

    Uses managed identity if AZCOPY_AUTO_LOGIN_TYPE=MSI is set,
    otherwise requires SAS token.

    Args:
        blob_url: Full blob URL (https://account.blob.core.windows.net/container/path)
        mount_path: Destination path on mounted filesystem
        sas_token: Optional SAS token (without leading ?)
        log_level: AzCopy log level (ERROR for max performance)

    Returns:
        Dict with success, duration_seconds, stdout, stderr
    """
    if not azcopy_available():
        return {
            "success": False,
            "error": "AzCopy not installed",
            "fallback_recommended": True
        }

    source = f"{blob_url}?{sas_token}" if sas_token else blob_url

    # Ensure parent directory exists
    Path(mount_path).parent.mkdir(parents=True, exist_ok=True)

    logger.info(f"AzCopy: {blob_url} → {mount_path}")
    start_time = time.time()

    result = subprocess.run(
        ["azcopy", "copy", source, mount_path, "--log-level", log_level],
        capture_output=True,
        text=True
    )

    duration = time.time() - start_time
    success = result.returncode == 0

    if success:
        file_size = Path(mount_path).stat().st_size if Path(mount_path).exists() else 0
        throughput = (file_size / (1024 * 1024)) / duration if duration > 0 else 0
        logger.info(f"AzCopy complete: {file_size / (1024*1024):.1f}MB in {duration:.1f}s ({throughput:.1f} MB/s)")
    else:
        logger.error(f"AzCopy failed: {result.stderr}")

    return {
        "success": success,
        "duration_seconds": duration,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "returncode": result.returncode
    }


def azcopy_mount_to_blob(
    mount_path: str,
    blob_url: str,
    sas_token: Optional[str] = None,
    log_level: str = "ERROR"
) -> Dict[str, Any]:
    """
    Copy file from mounted filesystem to blob using AzCopy.

    Args:
        mount_path: Source path on mounted filesystem
        blob_url: Full blob URL (https://account.blob.core.windows.net/container/path)
        sas_token: Optional SAS token (without leading ?)
        log_level: AzCopy log level (ERROR for max performance)

    Returns:
        Dict with success, duration_seconds, bytes_transferred, stdout, stderr
    """
    if not azcopy_available():
        return {
            "success": False,
            "error": "AzCopy not installed",
            "fallback_recommended": True
        }

    if not Path(mount_path).exists():
        return {
            "success": False,
            "error": f"Source file does not exist: {mount_path}"
        }

    dest = f"{blob_url}?{sas_token}" if sas_token else blob_url
    file_size = Path(mount_path).stat().st_size

    logger.info(f"AzCopy: {mount_path} → {blob_url} ({file_size / (1024*1024):.1f}MB)")
    start_time = time.time()

    result = subprocess.run(
        ["azcopy", "copy", mount_path, dest, "--log-level", log_level],
        capture_output=True,
        text=True
    )

    duration = time.time() - start_time
    success = result.returncode == 0

    if success:
        throughput = (file_size / (1024 * 1024)) / duration if duration > 0 else 0
        logger.info(f"AzCopy upload complete: {file_size / (1024*1024):.1f}MB in {duration:.1f}s ({throughput:.1f} MB/s)")
    else:
        logger.error(f"AzCopy upload failed: {result.stderr}")

    return {
        "success": success,
        "duration_seconds": duration,
        "bytes_transferred": file_size if success else 0,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "returncode": result.returncode
    }


# Module exports
__all__ = ['azcopy_blob_to_mount', 'azcopy_mount_to_blob', 'azcopy_available']
```

### Step 5: BlobRepository Integration (Optional)

Add AzCopy-accelerated methods to `infrastructure/blob.py`:

```python
def stream_blob_to_mount_fast(
    self,
    container: str,
    blob_path: str,
    mount_path: str,
    chunk_size_mb: int = 32
) -> Dict[str, Any]:
    """
    AzCopy-accelerated version of stream_blob_to_mount.

    Falls back to Python SDK if AzCopy unavailable.
    """
    from infrastructure.azcopy import azcopy_blob_to_mount, azcopy_available

    if azcopy_available():
        blob_url = f"https://{self.account_name}.blob.core.windows.net/{container}/{blob_path}"
        sas = self._generate_sas_token(container, blob_path, hours=1)
        return azcopy_blob_to_mount(blob_url, mount_path, sas)
    else:
        # Fallback to existing Python SDK implementation
        return self.stream_blob_to_mount(container, blob_path, mount_path, chunk_size_mb)


def stream_mount_to_blob_fast(
    self,
    container: str,
    blob_path: str,
    mount_path: str,
    content_type: str = None,
    chunk_size_mb: int = 32
) -> Dict[str, Any]:
    """
    AzCopy-accelerated version of stream_mount_to_blob.

    Falls back to Python SDK if AzCopy unavailable.
    """
    from infrastructure.azcopy import azcopy_mount_to_blob, azcopy_available

    if azcopy_available():
        blob_url = f"https://{self.account_name}.blob.core.windows.net/{container}/{blob_path}"
        sas = self._generate_sas_token(container, blob_path, hours=1, write=True)
        return azcopy_mount_to_blob(mount_path, blob_url, sas)
    else:
        # Fallback to existing Python SDK implementation
        return self.stream_mount_to_blob(container, blob_path, mount_path, content_type, chunk_size_mb)
```

---

## Testing

### Quick Test (in running container)

```bash
# SSH into container
docker exec -it <container_id> bash

# Verify AzCopy installed
azcopy --version

# Test download (with SAS)
azcopy copy "https://rmhazuregeobronze.blob.core.windows.net/bronze-raster/test.tif?<sas>" /mounts/etl-temp/test.tif

# Test upload (with SAS)
azcopy copy /mounts/etl-temp/test.tif "https://rmhazuregeobronze.blob.core.windows.net/bronze-raster/test-upload.tif?<sas>"
```

### Integration Test

```python
# In Python (inside Docker container)
from infrastructure.azcopy import azcopy_blob_to_mount, azcopy_available

print(f"AzCopy available: {azcopy_available()}")

result = azcopy_blob_to_mount(
    blob_url="https://rmhazuregeobronze.blob.core.windows.net/bronze-raster/test.tif",
    mount_path="/mounts/etl-temp/test.tif",
    sas_token="<your-sas-token>"
)
print(result)
```

---

## Rollout Plan

| Phase | Description | Risk |
|-------|-------------|------|
| 1 | Add AzCopy to Dockerfile, deploy | None - additive only |
| 2 | Add RBAC role, test manually | Low - can revert |
| 3 | Create `infrastructure/azcopy.py` | None - not wired yet |
| 4 | Add `_fast` methods to BlobRepository | Low - opt-in only |
| 5 | Switch `raster_cog.py` to use `_fast` methods | Medium - full integration |

**Recommendation**: Implement Phases 1-4, validate with manual testing, then proceed to Phase 5.

---

## Files to Create/Modify

| File | Action | Lines |
|------|--------|-------|
| `Dockerfile` | Modify | +2 lines |
| `docker.env` | Modify | +1 line |
| `infrastructure/azcopy.py` | Create | ~150 lines |
| `infrastructure/blob.py` | Modify (optional) | +40 lines |
| `services/raster_cog.py` | Modify (Phase 5) | ~10 lines |

---

## References

- [Optimize AzCopy Performance](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-optimize)
- [AzCopy GitHub](https://github.com/Azure/azure-storage-azcopy)
- [Authorize AzCopy with Managed Identity](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-authorize-managed-identity)
- [Azure Storage Built-in Roles](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/storage)
- [Transfer Data to Azure Files with AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-files)

---

## Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 29 JAN 2026 | Use AzCopy over ADF | Quick win, keeps current architecture, minimal changes |
| 29 JAN 2026 | Support both SAS and Managed Identity | Flexibility for testing vs production |
| 29 JAN 2026 | Create `_fast` methods vs replacing | Safe rollout, fallback to Python SDK |
