# V0.8 DDH Column Migration Plan

**Created**: 30 JAN 2026
**Status**: PLANNING
**Purpose**: Remove explicit DDH columns, migrate to `platform_id` + `platform_refs` model
**Approach**: Complete breaking changes - NO fallback patterns

---

## Executive Summary

Remove the DDH-specific columns (`dataset_id`, `resource_id`, `version_id`) from `GeospatialAsset` and all related code. All identification will use the flexible `platform_id` + `platform_refs` (JSONB) model.

**Before:**
```python
class GeospatialAsset:
    platform_id: str = "ddh"
    platform_refs: Dict = {"dataset_id": "X", "resource_id": "Y", "version_id": "Z"}
    # REDUNDANT - these duplicate platform_refs:
    dataset_id: str = "X"
    resource_id: str = "Y"
    version_id: str = "Z"
```

**After:**
```python
class GeospatialAsset:
    platform_id: str = "ddh"
    platform_refs: Dict = {"dataset_id": "X", "resource_id": "Y", "version_id": "Z"}
    # NO explicit DDH columns - use platform_refs only
```

---

## Prerequisites

- [x] `app.platforms` table exists with DDH seed
- [x] `Platform` model in `core/models/platform_registry.py`
- [x] `PlatformRegistryRepository` in `infrastructure/platform_registry_repository.py`
- [x] `platform_id` column on `geospatial_assets`
- [x] `platform_refs` JSONB column with GIN index
- [x] `list_by_platform_refs()` method exists

---

## Migration Tiers

### TIER 1: Core Model & Repository

These changes form the foundation - all other tiers depend on this.

#### 1.1 GeospatialAsset Model (`core/models/asset.py`)

- [ ] **Remove explicit DDH fields** (lines 221-235)
  ```python
  # DELETE THESE:
  dataset_id: str = Field(...)
  resource_id: str = Field(...)
  version_id: str = Field(...)
  ```

- [ ] **Update unique constraint** (line 146)
  ```python
  # FROM:
  __sql_unique_constraints = [
      {"columns": ["dataset_id", "resource_id", "version_id"], "name": "uq_assets_identity"}
  ]
  # TO:
  __sql_unique_constraints = [
      {"columns": ["platform_id", "platform_refs"], "name": "uq_assets_platform_identity"}
  ]
  ```

- [ ] **Update identity index** (line 149)
  ```python
  # FROM:
  {"columns": ["dataset_id", "resource_id", "version_id"], "name": "idx_assets_identity"}
  # TO: Remove - GIN index on platform_refs handles this
  ```

- [ ] **Update `to_dict()` method** (lines 580-582)
  - Remove `dataset_id`, `resource_id`, `version_id` from output

- [ ] **Update `generate_asset_id()` method** (lines 452-504)
  - Remove legacy signature support
  - Only accept `(platform_id, platform_refs)` signature
  ```python
  @staticmethod
  def generate_asset_id(platform_id: str, platform_refs: Dict[str, Any]) -> str:
      """Generate deterministic asset ID from platform identity."""
      import hashlib
      import json
      sorted_refs = json.dumps(platform_refs, sort_keys=True, separators=(',', ':'))
      composite = f"{platform_id}|{sorted_refs}"
      return hashlib.sha256(composite.encode()).hexdigest()[:32]
  ```

- [ ] **Remove `generate_asset_id_ddh()` helper** (lines 507-533)
  - Delete entirely - callers must use `generate_asset_id("ddh", {...})`

#### 1.2 Asset Repository (`infrastructure/asset_repository.py`)

- [ ] **Remove `get_by_identity()` method** (lines 262-288)
  - Delete entirely
  - Callers must use `get_by_platform_refs()` or `get()` with asset_id

- [ ] **Update `upsert()` method** (lines 71-191)
  - Change signature:
    ```python
    # FROM:
    def upsert(self, asset_id, dataset_id, resource_id, version_id, ...)
    # TO:
    def upsert(self, asset_id, platform_id, platform_refs, ...)
    ```
  - Update SQL to not insert explicit DDH columns

- [ ] **Update `_row_to_asset()` method** (lines 1049-1051)
  - Remove `dataset_id`, `resource_id`, `version_id` mapping

#### 1.3 Asset Service (`services/asset_service.py`)

- [ ] **Update `AssetExistsError`** (lines 54-60)
  ```python
  # FROM:
  def __init__(self, asset_id, dataset_id, resource_id, version_id):
  # TO:
  def __init__(self, asset_id: str, platform_id: str, platform_refs: Dict[str, Any]):
      self.platform_id = platform_id
      self.platform_refs = platform_refs
      super().__init__(f"Asset already exists for {platform_id}: {platform_refs}")
  ```

- [ ] **Update `create_or_update_asset()`** (lines 134-198)
  ```python
  # FROM:
  def create_or_update_asset(self, dataset_id, resource_id, version_id, ...)
  # TO:
  def create_or_update_asset(self, platform_id: str, platform_refs: Dict[str, Any], ...)
  ```

- [ ] **Update `create_asset()`** (lines 228-287)
  - Same signature change as above

- [ ] **Remove `get_by_identity()`** (lines 291-296)
  - Delete method
  - Callers use `list_by_platform_refs()` or `get()` with asset_id

- [ ] **Update `approve()`** (lines 396-423)
  ```python
  # FROM:
  def approve(self, dataset_id, resource_id, version_id, ...)
  # TO:
  def approve(self, asset_id: str, ...)  # Use asset_id directly
  ```

- [ ] **Update `change_clearance()`** (lines 591-614)
  - Same pattern - use `asset_id` not identity triple

- [ ] **Remove `generate_asset_id()` wrapper** (lines 858-863)
  - Callers use `GeospatialAsset.generate_asset_id()` directly

---

### TIER 2: Platform API Layer

#### 2.1 Platform Submit (`triggers/platform/submit.py`)

- [ ] **Update submit handler** (lines 96-106, 200-208, 274)
  - Build `platform_refs` dict from request
  - Pass to `create_or_update_asset(platform_id, platform_refs, ...)`
  ```python
  platform_refs = {
      "dataset_id": platform_req.dataset_id,
      "resource_id": platform_req.resource_id,
      "version_id": platform_req.version_id
  }
  asset, operation = asset_service.create_or_update_asset(
      platform_id="ddh",
      platform_refs=platform_refs,
      ...
  )
  ```

- [ ] **Update logging** (lines 206-208)
  - Log `platform_refs` dict instead of individual fields

#### 2.2 Platform Status (`triggers/trigger_platform_status.py`)

- [ ] **Update example in docstring** (lines 79-81)

- [ ] **Update status response building** (lines 181-183)
  - Read from `asset.platform_refs` not explicit fields
  ```python
  "identifiers": asset.platform_refs
  ```

- [ ] **Update query handling** (line 241)
  - Use `list_by_platform_refs()` for lookups

#### 2.3 Platform Catalog (`triggers/trigger_platform_catalog.py`)

- [ ] **Update catalog query endpoint** (lines 102-137)
  - Accept `platform_refs` as query params or JSON body
  - Use `list_by_platform_refs()` for lookup

- [ ] **Update examples in docstrings** (lines 302-304, 390-399)

- [ ] **Update route params handling** (line 410)

---

### TIER 3: Internal Job System

#### 3.1 Job Submission (`triggers/submit_job.py`)

- [ ] **Update job parameter extraction** (lines 67-69, 87, 109-111)
  ```python
  # FROM:
  dataset_id = req_body.get("dataset_id")
  resource_id = req_body.get("resource_id")
  version_id = req_body.get("version_id")
  job_params = {'dataset_id': dataset_id, ...}

  # TO:
  platform_refs = {
      "dataset_id": req_body.get("dataset_id"),
      "resource_id": req_body.get("resource_id"),
      "version_id": req_body.get("version_id")
  }
  job_params = {'platform_id': 'ddh', 'platform_refs': platform_refs}
  ```

#### 3.2 Docker Service (`docker_service.py`)

- [ ] **Update form handling** (lines 1608-1610)
  - Build `platform_refs` dict from form fields

---

### TIER 4: Supporting Models

#### 4.1 Platform Request Model (`core/models/platform.py`)

- [ ] **Update `PlatformSubmitRequest`** (lines 317, 333, 424-426, 454)
  - Keep `dataset_id`, `resource_id`, `version_id` as INPUT fields (API contract)
  - Add helper method to get `platform_refs`:
  ```python
  def get_platform_refs(self) -> Dict[str, Any]:
      return {
          "dataset_id": self.dataset_id,
          "resource_id": self.resource_id,
          "version_id": self.version_id
      }
  ```

#### 4.2 STAC Metadata (`core/models/stac.py`)

- [ ] **Update STAC properties generation** (lines 207-212)
  ```python
  # FROM:
  if self.dataset_id:
      result["platform:dataset_id"] = self.dataset_id

  # TO:
  for key, value in self.platform_refs.items():
      if value:
          result[f"platform:{key}"] = value
  ```

#### 4.3 External Refs (`core/models/external_refs.py`)

- [ ] **Update `ExternalRefs` model** (lines 103-113, 259)
  - Refactor to use `platform_refs` pattern

#### 4.4 UI Adapter (`ui/adapters/epoch4.py`)

- [ ] **Update asset display** (lines 373-375)
  ```python
  # FROM:
  dataset_id=asset.dataset_id,
  resource_id=asset.resource_id,
  version_id=asset.version_id,

  # TO:
  platform_refs=asset.platform_refs,
  ```

---

### TIER 5: Schema/DDL Changes

#### 5.1 SQL Generator (`core/schema/sql_generator.py`)

- [ ] **Remove DDH column definitions from `geospatial_assets`**
  - Remove `dataset_id VARCHAR(255)`
  - Remove `resource_id VARCHAR(255)`
  - Remove `version_id VARCHAR(100)`

- [ ] **Remove DDH-specific indexes** (lines 1006, 1023, 1101)
  - `idx_assets_identity` - DELETE

- [ ] **Update unique constraint**
  - FROM: `UNIQUE(dataset_id, resource_id, version_id)`
  - TO: `UNIQUE(platform_id, platform_refs)` (or use GIN exclusion)

- [ ] **Keep platform_refs GIN index** (already exists)

#### 5.2 Workflow Schema (`core/schema/workflow.py`)

- [ ] **Update workflow parameter definitions** (lines 403-415)
  - Change from explicit DDH params to `platform_refs` dict

---

### TIER 6: Database Migration

#### 6.1 Migration Script

- [ ] **Create migration script** (`migrations/remove_ddh_columns.sql`)
  ```sql
  -- V0.8 DDH Column Removal Migration
  -- Run AFTER code deployment

  -- Step 1: Verify all data has platform_refs populated
  SELECT COUNT(*) as missing_refs
  FROM app.geospatial_assets
  WHERE platform_refs IS NULL OR platform_refs = '{}';
  -- MUST be 0 before proceeding

  -- Step 2: Drop the redundant columns
  ALTER TABLE app.geospatial_assets DROP COLUMN IF EXISTS dataset_id;
  ALTER TABLE app.geospatial_assets DROP COLUMN IF EXISTS resource_id;
  ALTER TABLE app.geospatial_assets DROP COLUMN IF EXISTS version_id;

  -- Step 3: Drop old indexes
  DROP INDEX IF EXISTS app.idx_assets_identity;
  DROP INDEX IF EXISTS app.uq_assets_identity;

  -- Step 4: Add new unique constraint (if not using GIN)
  -- Note: JSONB unique constraint requires hash or specific approach
  -- The GIN index + application logic handles uniqueness for now
  ```

---

## Execution Order

1. **TIER 1** - Core Model & Repository (foundation)
2. **TIER 4** - Supporting Models (prepare helpers)
3. **TIER 2** - Platform API Layer (external interface)
4. **TIER 3** - Internal Job System
5. **TIER 5** - Schema/DDL Changes
6. **Deploy & Test**
7. **TIER 6** - Database Migration (run after code is stable)

---

## Rollback Plan

If issues arise:
1. Revert code to previous commit
2. Do NOT run database migration until code is verified
3. Database columns remain until migration script runs

---

## Testing Checklist

- [ ] Submit new asset via Platform API
- [ ] Query asset by `platform_refs` partial match
- [ ] Approve asset by `asset_id`
- [ ] Change clearance by `asset_id`
- [ ] Verify STAC metadata contains `platform:*` properties
- [ ] Verify UI displays asset identifiers correctly
- [ ] Run existing integration tests

---

## Files Changed Summary

| Tier | Files | Est. Lines Changed |
|------|-------|-------------------|
| 1 | `asset.py`, `asset_repository.py`, `asset_service.py` | ~200 |
| 2 | `submit.py`, `trigger_platform_status.py`, `trigger_platform_catalog.py` | ~80 |
| 3 | `submit_job.py`, `docker_service.py` | ~30 |
| 4 | `platform.py`, `stac.py`, `external_refs.py`, `epoch4.py` | ~50 |
| 5 | `sql_generator.py`, `workflow.py` | ~40 |
| 6 | Migration script | ~20 |
| **Total** | **~15 files** | **~420 lines** |

---

## References

- [V0.8_ENTITIES.md](/V0.8_ENTITIES.md) - Entity architecture specification
- [V0.8_ENTITIES.md#14](/V0.8_ENTITIES.md#14-platform-registry) - Platform Registry design
- `core/models/platform_registry.py` - Platform model
- `infrastructure/platform_registry_repository.py` - Platform repository
