# V0.8 Entity Architecture

**Created**: 29 JAN 2026
**Status**: ✅ IMPLEMENTED (29 JAN 2026)
**Purpose**: Define first-class entities for the Platform API data model

---

## Overview

This document defines the core entity architecture for V0.8, introducing:

1. **`GeospatialAsset`** - First-class entity representing a dataset managed through the Platform API
2. **Request/Job Separation** - Decoupling work orders from execution records
3. **Four-State Model** - Revision, Approval, Clearance, and Processing as orthogonal state dimensions
4. **Revision History** - Append-only audit log for dataset changes
5. **Platform Registry** - Flexible B2B platform support with `platform_id` + `platform_refs` (Section 14)
6. **DAG Orchestration Fields** - Processing status, workflow tracking, SLA monitoring (Section 15)

This architecture enables future **DAG orchestration** by cleanly separating intent (requests) from execution (jobs), allowing complex multi-step workflows to be composed.

---

## Implementation Summary (29 JAN 2026)

### Files Created

| File | Lines | Purpose |
|------|-------|---------|
| `core/models/asset.py` | ~450 | `ApprovalState`, `ClearanceState`, `ProcessingStatus`, `GeospatialAsset`, `AssetRevision` models |
| `infrastructure/asset_repository.py` | ~950 | `GeospatialAssetRepository` with CRUD, upsert, advisory locks, DAG processing methods |
| `infrastructure/revision_repository.py` | ~221 | `AssetRevisionRepository` for append-only audit log |
| `services/asset_service.py` | ~850 | `AssetService` orchestrating asset lifecycle + processing status |
| `core/models/platform_registry.py` | ~180 | `Platform` model + `DDH_PLATFORM` constant |
| `infrastructure/platform_registry_repository.py` | ~300 | CRUD for platforms with validation |

### Files Modified

| File | Changes |
|------|---------|
| `core/models/__init__.py` | Export new models |
| `core/schema/sql_generator.py` | Add enums, tables, indexes, `upsert_geospatial_asset()` function |
| `infrastructure/__init__.py` | Export new repositories |
| `triggers/platform/submit.py` | Create `GeospatialAsset` before job |
| `triggers/trigger_platform_status.py` | Support `asset_id` lookup, return asset info |
| `triggers/trigger_approvals.py` | Add `platform_reject`, require `clearance_level`, update asset |
| `triggers/platform/unpublish.py` | Soft delete asset |
| `triggers/platform/platform_bp.py` | Register `platform_reject` route |
| `services/handler_vector_docker_complete.py` | Link job to asset |
| `services/handler_process_raster_complete.py` | Link job to asset |
| `core/machine_factory.py` | Update asset on job completion |

### Schema Components (created by `action=rebuild`)

| Type | Name | Purpose |
|------|------|---------|
| ENUM | `app.approval_state` | pending_review, approved, rejected |
| ENUM | `app.clearance_state` | uncleared, ouo, public |
| ENUM | `app.processing_status` | ✅ pending, processing, completed, failed (DAG) |
| TABLE | `app.geospatial_assets` | First-class entity for managed datasets |
| TABLE | `app.asset_revisions` | Append-only revision audit log |
| TABLE | `app.platforms` | ✅ Platform registry for B2B integrations |
| FUNCTION | `app.upsert_geospatial_asset()` | Atomic create/update/reactivate with advisory locks |
| INDEX | 13 on `geospatial_assets` | identity, approval, clearance, platform, processing status, workflow, etc. |
| INDEX | 2 on `asset_revisions` | asset_id, created_at |
| INDEX | GIN on `platform_refs` | ✅ Efficient JSONB containment queries |

### API Endpoints

| Endpoint | Method | Status |
|----------|--------|--------|
| `/api/platform/submit` | POST | ✅ Creates asset before job |
| `/api/platform/status/{id}` | GET | ✅ Accepts request_id, job_id, or asset_id |
| `/api/platforms` | GET | ✅ List supported B2B platforms |
| `/api/platform/approve` | POST | ✅ Requires `clearance_level`, updates asset |
| `/api/platform/reject` | POST | ✅ NEW - Rejects pending assets |
| `/api/platform/revoke` | POST | ✅ Soft-deletes asset on revocation |
| `/api/platform/unpublish` | POST | ✅ Soft-deletes asset |

---

## Table of Contents

1. [Conceptual Model](#1-conceptual-model)
2. [Entity Definitions](#2-entity-definitions)
3. [State Dimensions](#3-state-dimensions)
4. [Relationships](#4-relationships)
5. [Pydantic Models](#5-pydantic-models)
6. [Database Schema](#6-database-schema)
7. [Workflows](#7-workflows)
8. [API Compatibility](#8-api-compatibility)
9. [Implementation Plan](#9-implementation-plan)
10. [Migration Strategy](#10-migration-strategy)
11. [Future: DAG Orchestration](#11-future-dag-orchestration)
12. [Design Decisions](#12-design-decisions)
13. [TODOs](#13-todos)
14. [Platform Registry](#14-platform-registry)

---

## 1. Conceptual Model

### Core Entities

```
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                         │
│   REQUEST (Work Order)              JOB (Execution Record)              │
│   ════════════════════              ══════════════════════              │
│   "Please process this"             "Here's what I did"                 │
│                                                                         │
│   - Acknowledged immediately        - Created when work starts          │
│   - Can exist without job           - Always tied to a request          │
│   - Intent/specification            - Actual processing record          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Relationships

```
┌──────────────┐         ┌─────────────────────┐         ┌──────────────┐
│   REQUEST    │   N:1   │  GEOSPATIAL_ASSET   │   1:N   │     JOB      │
│              │────────►│                     │◄────────│              │
│  (work order)│         │  (the entity)       │         │ (execution)  │
└──────────────┘         └─────────────────────┘         └──────────────┘
                                   │
                                   │ has one
                                   ▼
                         ┌──────────────────┐
                         │  current_job_id  │
                         │  (latest active) │
                         └──────────────────┘
```

### Key Principles

| Principle | Description |
|-----------|-------------|
| **Request ≠ Job** | A request is a work order; a job is an execution attempt |
| **Immediate Acknowledgment** | Requests are confirmed before any job exists |
| **Single Entity** | One `GeospatialAsset` per unique (dataset_id, resource_id, version_id) |
| **Revision Tracking** | Overwrites increment revision, log history, reset states |
| **Clearance Required** | All datasets start `uncleared` until explicitly approved |

### Naming Rationale

The entity is named `GeospatialAsset` (not `PublishedDataset`) because:
- It exists **before** publication (created on request receipt)
- It represents a **managed asset** in our platform
- "Published" implies a terminal state, but the asset has a full lifecycle
- Domain-specific name emphasizes geospatial focus

---

## 2. Entity Definitions

### 2.1 GeospatialAsset

The first-class entity representing a dataset managed via the Platform API.

```python
class GeospatialAsset:
    """
    First-class entity for datasets managed via Platform API.
    Links external identifiers to internal services with full state tracking.
    """
    # Primary Key (deterministic)
    asset_id: str  # SHA256(dataset_id|resource_id|version_id)[:32]

    # External Identifiers (DDH Anti-Corruption Layer)
    dataset_id: str           # DDH dataset
    resource_id: str          # DDH resource
    version_id: str           # DDH version

    # Data Type
    data_type: Literal["vector", "raster"]

    # Service Outputs (what TiPG/TiTiler serve)
    table_name: Optional[str]       # Vector: geo.{name}
    blob_path: Optional[str]        # Raster: silver-cogs/{path}
    stac_item_id: str
    stac_collection_id: str

    # === THREE STATE DIMENSIONS ===

    # Revision State
    revision: int                   # Increments on overwrite (1, 2, 3...)
    current_job_id: Optional[str]   # Job that created current revision
    content_hash: Optional[str]     # SHA256 of source file

    # Approval State
    approval_state: ApprovalState   # pending_review | approved | rejected
    reviewer: Optional[str]
    reviewed_at: Optional[datetime]
    rejection_reason: Optional[str]

    # Clearance State
    clearance_state: ClearanceState # uncleared | ouo | public
    adf_run_id: Optional[str]       # Only for public (ADF export tracking)

    # Soft Delete (audit trail)
    deleted_at: Optional[datetime]
    deleted_by: Optional[str]

    # Audit
    created_at: datetime
    updated_at: datetime
```

### 2.2 PlatformRequest

A work order submitted via the Platform API.

```python
class PlatformRequest:
    """
    Work order for dataset processing.
    Can exist before any job is created.
    """
    # Primary Key
    request_id: str  # SHA256(dataset_id|resource_id|version_id|timestamp)[:32]

    # Link to entity
    asset_id: Optional[str]  # FK to GeospatialAsset

    # Request specification
    dataset_id: str
    resource_id: str
    version_id: str
    data_type: str
    container_name: str
    file_name: str
    overwrite: bool
    processing_options: Optional[dict]

    # Request state
    status: str  # accepted | processing | completed | failed

    # Link to job (NULL until worker picks up)
    job_id: Optional[str]

    # Retry tracking
    retry_count: int
    last_error: Optional[str]

    # Timestamps
    created_at: datetime
    updated_at: datetime
```

### 2.3 AssetRevision

Append-only audit log for revision history.

```python
class AssetRevision:
    """
    Historical record of a superseded revision.
    Created when overwrite replaces existing data.
    """
    revision_id: str  # UUID
    asset_id: str     # FK to GeospatialAsset

    # Snapshot of superseded revision
    revision: int
    job_id: str
    content_hash: Optional[str]

    # State at time of supersession
    approval_state_at_supersession: ApprovalState
    clearance_state_at_supersession: ClearanceState
    reviewer_at_supersession: Optional[str]

    # Timestamps
    created_at: datetime      # When this revision was created
    superseded_at: datetime   # When it was replaced
```

---

## 3. State Dimensions

### 3.1 Revision State

Tracks versions of the dataset content.

```
Submit (new)           → revision = 1, current_job_id = job1
Submit (overwrite)     → revision = 2, current_job_id = job2
                         (logs revision 1 to history)
```

| Field | Description |
|-------|-------------|
| `revision` | Monotonic counter (1, 2, 3...) |
| `current_job_id` | Job that created the current revision |
| `content_hash` | SHA256 of source file for change detection |

### 3.2 Approval State

Review workflow state.

```python
class ApprovalState(str, Enum):
    PENDING_REVIEW = "pending_review"  # Awaiting reviewer action
    APPROVED = "approved"              # Reviewer approved
    REJECTED = "rejected"              # Reviewer rejected
```

```
                    ┌─────────────────┐
     (on create)    │ pending_review  │ ←── (resets here on overwrite)
         ──────────►│                 │
                    └────────┬────────┘
                             │
              POST /approve  │  POST /reject
                             │
                    ┌────────┴────────┐
                    ▼                 ▼
              ┌──────────┐      ┌──────────┐
              │ approved │      │ rejected │
              └──────────┘      └──────────┘
```

**Note**: From `rejected`, user must submit with `overwrite=true` to reset to `pending_review`.

### 3.3 Clearance State

Security classification level.

```python
class ClearanceState(str, Enum):
    UNCLEARED = "uncleared"  # Not yet cleared (same access as OUO)
    OUO = "ouo"              # Official Use Only - internal only
    PUBLIC = "public"        # Cleared for external - triggers ADF
```

```
                    ┌───────────┐
     (on create)    │ uncleared │ ←── (resets here on overwrite)
         ──────────►│           │
                    └─────┬─────┘
                          │
           Only when approval_state = approved
           Requires explicit clearance_level parameter
                          │
               ┌──────────┴──────────┐
               ▼                     ▼
          ┌─────────┐           ┌─────────┐
          │   ouo   │           │ public  │
          │         │           │         │
          │ (no ADF)│           │ (ADF!)  │
          └─────────┘           └─────────┘
```

### 3.4 Access Behavior by Clearance

| Clearance | Internal Services | External Zone | STAC Searchable |
|-----------|-------------------|---------------|-----------------|
| `uncleared` | Accessible | N/A | Yes (current) |
| `ouo` | Accessible | N/A | Yes |
| `public` | Accessible | ADF exports | Yes |

**Note**: `uncleared` and `ouo` have identical access behavior. The distinction is for B2B workflow confirmation.

### 3.5 Clearance Change Workflow (29 JAN 2026)

**Clearance level is:**
- **Optional** at submit time (defaults to `UNCLEARED`)
- **Mandatory** at approve time

**Clearance change use cases:**

| Scenario | Correct Workflow | Why |
|----------|------------------|-----|
| Data is internal, later got permission to share publicly | **Clearance change** (`ouo` → `public`) | Rare, legitimate workflow |
| Accidentally set wrong clearance | **Unpublish and resubmit** | Data itself may need review |
| Data should never have been public | **Unpublish** (soft delete) | Clean slate, full audit trail |

**Clearance upgrade (ouo → public):**
- Allowed via re-approval with new `clearance_level`
- Sets `made_public_at` and `made_public_by` audit fields
- Triggers ADF export to external zone

**Clearance downgrade (public → ouo):**
- Allowed with **warning**: External zone data must be removed via separate ADF reversal process
- ADF reversal pipeline is future implementation
- Response includes: `"warning": "External zone cleanup required (manual process)"`

**State Machine with Clearance Changes:**

```
┌───────────────────────────────────────────────────────────────────────┐
│                         CLEARANCE TRANSITIONS                          │
├───────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  Submit (no clearance_level)                                          │
│       │                                                               │
│       ▼                                                               │
│  ┌──────────────┐                                                     │
│  │  UNCLEARED   │ ◄─── Default on create/overwrite                    │
│  └──────┬───────┘                                                     │
│         │                                                             │
│         │ POST /approve (clearance_level = ouo)                       │
│         │                                                             │
│         ▼                                                             │
│  ┌──────────────┐      POST /approve (clearance_level = public)       │
│  │     OUO      │ ──────────────────────────────────────────────────► │
│  └──────────────┘                                                     │
│         ▲                                                             │
│         │                                                             │
│         │ POST /approve (clearance_level = ouo) + WARNING             │
│         │                                                             │
│  ┌──────────────┐                                                     │
│  │    PUBLIC    │  (ADF export triggered on upgrade)                  │
│  └──────────────┘                                                     │
│                                                                       │
│  Note: Downgrade requires manual ADF reversal (future automation)     │
│                                                                       │
└───────────────────────────────────────────────────────────────────────┘
```

---

## 4. Relationships

### Entity Relationship Diagram

```
┌─────────────────────┐       ┌─────────────────────┐
│  platform_requests  │       │        jobs         │
├─────────────────────┤       ├─────────────────────┤
│ request_id (PK)     │       │ job_id (PK)         │
│ asset_id (FK)       │──┐    │ asset_id (FK)       │──┐
│ job_id (nullable)   │──│────│                     │  │
│ status              │  │    │ status              │  │
│ retry_count         │  │    │ ...                 │  │
│ ...                 │  │    └─────────────────────┘  │
└─────────────────────┘  │                             │
                         │    ┌─────────────────────┐  │
                         │    │  geospatial_assets  │  │
                         │    ├─────────────────────┤  │
                         └───►│ asset_id (PK)       │◄─┘
                              │ current_job_id (FK) │───► jobs.job_id
                              │ revision            │
                              │ approval_state      │
                              │ clearance_state     │
                              │ deleted_at          │
                              │ ...                 │
                              └──────────┬──────────┘
                                         │
                                         │ 1:N
                                         ▼
                              ┌─────────────────────────────┐
                              │      asset_revisions        │
                              ├─────────────────────────────┤
                              │ revision_id (PK)            │
                              │ asset_id (FK)               │
                              │ revision                    │
                              │ job_id                      │
                              │ approval_state_at_super...  │
                              │ clearance_state_at_super... │
                              │ ...                         │
                              └─────────────────────────────┘
```

### Cardinality Summary

| Relationship | Cardinality | Description |
|--------------|-------------|-------------|
| Request → GeospatialAsset | N:1 | Many requests can target same asset |
| Job → GeospatialAsset | N:1 | Many jobs can process same asset |
| GeospatialAsset → current_job | 1:1 | One current job at a time |
| GeospatialAsset → Revisions | 1:N | One entry per superseded revision |

---

## 5. Pydantic Models

Following the project's model-driven DDL pattern with `__sql_*` ClassVar attributes.

### 5.1 Enums

**File**: `core/models/asset.py`

```python
from enum import Enum

class ApprovalState(str, Enum):
    """
    Approval workflow state for geospatial assets.

    Transitions:
    - PENDING_REVIEW → APPROVED (approve with clearance_level)
    - PENDING_REVIEW → REJECTED (reject with reason)
    - REJECTED → PENDING_REVIEW (only via overwrite submit)
    """
    PENDING_REVIEW = "pending_review"
    APPROVED = "approved"
    REJECTED = "rejected"


class ClearanceState(str, Enum):
    """
    Security clearance level for geospatial assets.

    Access behavior:
    - UNCLEARED: Same as OUO (internal only), awaiting confirmation
    - OUO: Official Use Only, internal access confirmed
    - PUBLIC: Triggers ADF export to external zone
    """
    UNCLEARED = "uncleared"
    OUO = "ouo"
    PUBLIC = "public"
```

### 5.2 GeospatialAsset Model

**File**: `core/models/asset.py`

```python
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List, ClassVar, Literal
from pydantic import BaseModel, Field, ConfigDict
from uuid import UUID


class GeospatialAsset(BaseModel):
    """
    First-class entity for datasets managed via Platform API.

    Created when Platform API receives a submit request.
    Tracks the full lifecycle: creation → approval → clearance → deletion.

    Table: app.geospatial_assets
    Primary Key: asset_id (deterministic from dataset_id|resource_id|version_id)

    DDL generation uses __sql_* class attributes for model-driven schema.
    """
    model_config = ConfigDict(
        use_enum_values=True,
        extra='ignore',
        str_strip_whitespace=True,
        json_encoders={datetime: lambda v: v.isoformat() if v else None}
    )

    # =========================================================================
    # DDL GENERATION HINTS (ClassVar = not a model field)
    # =========================================================================
    __sql_table_name: ClassVar[str] = "geospatial_assets"
    __sql_schema: ClassVar[str] = "app"
    __sql_primary_key: ClassVar[List[str]] = ["asset_id"]
    __sql_foreign_keys: ClassVar[Dict[str, str]] = {
        "current_job_id": "app.jobs(job_id)"
    }
    __sql_unique_constraints: ClassVar[List[List[str]]] = [
        ["dataset_id", "resource_id", "version_id"]
    ]
    __sql_indexes: ClassVar[List[Dict[str, Any]]] = [
        {"columns": ["dataset_id", "resource_id", "version_id"], "name": "idx_assets_identity"},
        {"columns": ["stac_item_id"], "name": "idx_assets_stac_item"},
        {"columns": ["approval_state"], "name": "idx_assets_approval"},
        {"columns": ["clearance_state"], "name": "idx_assets_clearance"},
        {"columns": ["current_job_id"], "name": "idx_assets_current_job"},
        {"columns": ["created_at"], "name": "idx_assets_created", "descending": True},
        {
            "columns": ["approval_state"],
            "name": "idx_assets_pending",
            "partial_where": "approval_state = 'pending_review' AND deleted_at IS NULL"
        },
        {
            "columns": ["deleted_at"],
            "name": "idx_assets_active",
            "partial_where": "deleted_at IS NULL"
        },
    ]

    # =========================================================================
    # IDENTITY (Primary Key - Deterministic)
    # =========================================================================
    asset_id: str = Field(
        ...,
        max_length=64,
        description="Deterministic ID: SHA256(dataset_id|resource_id|version_id)[:32]"
    )

    # =========================================================================
    # EXTERNAL IDENTIFIERS (DDH Anti-Corruption Layer)
    # =========================================================================
    dataset_id: str = Field(
        ...,
        max_length=100,
        description="DDH dataset identifier"
    )
    resource_id: str = Field(
        ...,
        max_length=100,
        description="DDH resource identifier"
    )
    version_id: str = Field(
        ...,
        max_length=100,
        description="DDH version identifier"
    )

    # =========================================================================
    # DATA TYPE
    # =========================================================================
    data_type: Literal["vector", "raster"] = Field(
        ...,
        description="Type of geospatial data"
    )

    # =========================================================================
    # SERVICE OUTPUTS (What TiPG/TiTiler Serve)
    # =========================================================================
    table_name: Optional[str] = Field(
        default=None,
        max_length=63,
        description="PostGIS table name for vectors (geo.{name})"
    )
    blob_path: Optional[str] = Field(
        default=None,
        max_length=500,
        description="Azure Blob path for rasters (silver-cogs/{path})"
    )
    stac_item_id: str = Field(
        ...,
        max_length=200,
        description="STAC item identifier"
    )
    stac_collection_id: str = Field(
        ...,
        max_length=200,
        description="STAC collection identifier"
    )

    # =========================================================================
    # REVISION STATE
    # =========================================================================
    revision: int = Field(
        default=1,
        ge=1,
        description="Revision counter, increments on overwrite"
    )
    current_job_id: Optional[str] = Field(
        default=None,
        max_length=64,
        description="Job that created current revision (FK to jobs)"
    )
    content_hash: Optional[str] = Field(
        default=None,
        max_length=128,
        description="SHA256 hash of source file for change detection"
    )

    # =========================================================================
    # APPROVAL STATE
    # =========================================================================
    approval_state: ApprovalState = Field(
        default=ApprovalState.PENDING_REVIEW,
        description="Current approval workflow state"
    )
    reviewer: Optional[str] = Field(
        default=None,
        max_length=200,
        description="Email of reviewer who approved/rejected"
    )
    reviewed_at: Optional[datetime] = Field(
        default=None,
        description="When approval decision was made"
    )
    rejection_reason: Optional[str] = Field(
        default=None,
        description="Reason for rejection (required if rejected)"
    )

    # =========================================================================
    # CLEARANCE STATE
    # =========================================================================
    clearance_state: ClearanceState = Field(
        default=ClearanceState.UNCLEARED,
        description="Security clearance level"
    )
    adf_run_id: Optional[str] = Field(
        default=None,
        max_length=100,
        description="ADF pipeline run ID (only for PUBLIC clearance)"
    )

    # =========================================================================
    # CLEARANCE AUDIT TRAIL (29 JAN 2026)
    # =========================================================================
    # Explicit audit columns for clearance changes. These events are rare:
    # - Use case: "Data was internal, later got permission to share publicly"
    # - Wrong clearance: Use unpublish workflow, not clearance change
    # =========================================================================
    cleared_at: Optional[datetime] = Field(
        default=None,
        description="When clearance first changed from UNCLEARED"
    )
    cleared_by: Optional[str] = Field(
        default=None,
        max_length=200,
        description="Who first cleared the asset (reviewer email)"
    )
    made_public_at: Optional[datetime] = Field(
        default=None,
        description="When clearance changed to PUBLIC (null if never public)"
    )
    made_public_by: Optional[str] = Field(
        default=None,
        max_length=200,
        description="Who made the asset public"
    )

    # =========================================================================
    # SOFT DELETE (Audit Trail)
    # =========================================================================
    deleted_at: Optional[datetime] = Field(
        default=None,
        description="When asset was deleted (soft delete for audit trail)"
    )
    deleted_by: Optional[str] = Field(
        default=None,
        max_length=200,
        description="Who deleted the asset"
    )

    # =========================================================================
    # TIMESTAMPS
    # =========================================================================
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When asset was first created"
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When asset was last modified"
    )

    # =========================================================================
    # HELPER METHODS
    # =========================================================================
    @staticmethod
    def generate_asset_id(dataset_id: str, resource_id: str, version_id: str) -> str:
        """Generate deterministic asset ID from external identifiers."""
        import hashlib
        composite = f"{dataset_id}|{resource_id}|{version_id}"
        return hashlib.sha256(composite.encode()).hexdigest()[:32]

    def is_active(self) -> bool:
        """Check if asset is not deleted."""
        return self.deleted_at is None

    def can_approve(self) -> bool:
        """Check if asset can be approved."""
        return (
            self.approval_state == ApprovalState.PENDING_REVIEW
            and self.is_active()
        )

    def can_reject(self) -> bool:
        """Check if asset can be rejected."""
        return (
            self.approval_state == ApprovalState.PENDING_REVIEW
            and self.is_active()
        )
```

### 5.3 PlatformRequest Model (Updated)

**File**: `core/models/platform.py` (modify existing)

```python
class PlatformRequest(BaseModel):
    """
    Work order for dataset processing via Platform API.

    Renamed from ApiRequest to clarify purpose.
    Can exist before any job is created (async acknowledgment).

    Table: app.platform_requests
    Primary Key: request_id
    """
    model_config = ConfigDict(
        use_enum_values=True,
        extra='ignore',
        str_strip_whitespace=True
    )

    # =========================================================================
    # DDL GENERATION HINTS
    # =========================================================================
    __sql_table_name: ClassVar[str] = "platform_requests"
    __sql_schema: ClassVar[str] = "app"
    __sql_primary_key: ClassVar[List[str]] = ["request_id"]
    __sql_foreign_keys: ClassVar[Dict[str, str]] = {
        "asset_id": "app.geospatial_assets(asset_id)",
        "job_id": "app.jobs(job_id)"
    }
    __sql_indexes: ClassVar[List[Dict[str, Any]]] = [
        {"columns": ["asset_id"], "name": "idx_requests_asset"},
        {"columns": ["job_id"], "name": "idx_requests_job"},
        {"columns": ["status"], "name": "idx_requests_status"},
        {"columns": ["created_at"], "name": "idx_requests_created", "descending": True},
        {
            "columns": ["status"],
            "name": "idx_requests_pending",
            "partial_where": "status IN ('accepted', 'processing')"
        },
    ]

    # =========================================================================
    # IDENTITY
    # =========================================================================
    request_id: str = Field(
        ...,
        max_length=64,
        description="Unique request ID (includes timestamp for uniqueness)"
    )

    # =========================================================================
    # LINK TO ASSET
    # =========================================================================
    asset_id: Optional[str] = Field(
        default=None,
        max_length=64,
        description="FK to geospatial_assets (set on creation)"
    )

    # =========================================================================
    # REQUEST SPECIFICATION
    # =========================================================================
    dataset_id: str = Field(..., max_length=100)
    resource_id: str = Field(..., max_length=100)
    version_id: str = Field(..., max_length=100)
    data_type: str = Field(..., max_length=20)
    container_name: str = Field(..., max_length=200)
    file_name: str = Field(..., max_length=500)
    overwrite: bool = Field(default=False)
    processing_options: Optional[Dict[str, Any]] = Field(default=None)

    # =========================================================================
    # REQUEST STATE
    # =========================================================================
    status: str = Field(
        default="accepted",
        max_length=20,
        description="accepted | processing | completed | failed"
    )

    # =========================================================================
    # LINK TO JOB (NULL until worker picks up)
    # =========================================================================
    job_id: Optional[str] = Field(
        default=None,
        max_length=64,
        description="FK to jobs, set when worker creates job"
    )

    # =========================================================================
    # RETRY TRACKING
    # =========================================================================
    retry_count: int = Field(default=0, ge=0)
    last_error: Optional[str] = Field(default=None)

    # =========================================================================
    # TIMESTAMPS
    # =========================================================================
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
```

### 5.4 AssetRevision Model

**File**: `core/models/asset.py`

```python
class AssetRevision(BaseModel):
    """
    Append-only audit log for asset revision history.

    Created when an overwrite replaces existing asset data.
    Records the state at the moment of supersession.

    Table: app.asset_revisions
    Primary Key: revision_id (UUID)
    """
    model_config = ConfigDict(
        use_enum_values=True,
        extra='ignore'
    )

    # =========================================================================
    # DDL GENERATION HINTS
    # =========================================================================
    __sql_table_name: ClassVar[str] = "asset_revisions"
    __sql_schema: ClassVar[str] = "app"
    __sql_primary_key: ClassVar[List[str]] = ["revision_id"]
    __sql_foreign_keys: ClassVar[Dict[str, str]] = {
        "asset_id": "app.geospatial_assets(asset_id) ON DELETE CASCADE",
        "job_id": "app.jobs(job_id)"
    }
    __sql_unique_constraints: ClassVar[List[List[str]]] = [
        ["asset_id", "revision"]
    ]
    __sql_indexes: ClassVar[List[Dict[str, Any]]] = [
        {"columns": ["asset_id"], "name": "idx_revisions_asset"},
        {"columns": ["asset_id", "revision"], "name": "idx_revisions_asset_rev"},
        {"columns": ["superseded_at"], "name": "idx_revisions_superseded", "descending": True},
    ]

    # =========================================================================
    # IDENTITY
    # =========================================================================
    revision_id: UUID = Field(
        ...,
        description="Unique revision record ID"
    )
    asset_id: str = Field(
        ...,
        max_length=64,
        description="FK to geospatial_assets"
    )

    # =========================================================================
    # REVISION SNAPSHOT
    # =========================================================================
    revision: int = Field(
        ...,
        ge=1,
        description="Revision number that was superseded"
    )
    job_id: str = Field(
        ...,
        max_length=64,
        description="Job that created this revision"
    )
    content_hash: Optional[str] = Field(
        default=None,
        max_length=128,
        description="Content hash at this revision"
    )

    # =========================================================================
    # STATE AT SUPERSESSION
    # =========================================================================
    approval_state_at_supersession: ApprovalState = Field(
        ...,
        description="Approval state when this revision was replaced"
    )
    clearance_state_at_supersession: ClearanceState = Field(
        ...,
        description="Clearance state when this revision was replaced"
    )
    reviewer_at_supersession: Optional[str] = Field(
        default=None,
        max_length=200,
        description="Reviewer at time of supersession"
    )

    # =========================================================================
    # TIMESTAMPS
    # =========================================================================
    created_at: datetime = Field(
        ...,
        description="When this revision was originally created"
    )
    superseded_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When this revision was replaced"
    )
```

---

## 6. Database Schema

### 6.1 Enums

```sql
-- Approval workflow state
CREATE TYPE app.approval_state_enum AS ENUM (
    'pending_review',
    'approved',
    'rejected'
);

-- Security clearance level
CREATE TYPE app.clearance_state_enum AS ENUM (
    'uncleared',
    'ouo',
    'public'
);
```

### 6.2 Geospatial Assets Table

```sql
CREATE TABLE app.geospatial_assets (
    -- Primary Key (deterministic from identifiers)
    asset_id VARCHAR(64) PRIMARY KEY,

    -- External Identifiers (immutable)
    dataset_id VARCHAR(100) NOT NULL,
    resource_id VARCHAR(100) NOT NULL,
    version_id VARCHAR(100) NOT NULL,
    data_type VARCHAR(20) NOT NULL,  -- 'vector' or 'raster'

    -- Service Outputs
    table_name VARCHAR(63),           -- Vector: geo.{name}
    blob_path VARCHAR(500),           -- Raster: silver-cogs/{path}
    stac_item_id VARCHAR(200) NOT NULL,
    stac_collection_id VARCHAR(200) NOT NULL,

    -- Revision State
    revision INTEGER NOT NULL DEFAULT 1,
    current_job_id VARCHAR(64) REFERENCES app.jobs(job_id),
    content_hash VARCHAR(128),

    -- Approval State
    approval_state app.approval_state_enum NOT NULL DEFAULT 'pending_review',
    reviewer VARCHAR(200),
    reviewed_at TIMESTAMP,
    rejection_reason TEXT,

    -- Clearance State
    clearance_state app.clearance_state_enum NOT NULL DEFAULT 'uncleared',
    adf_run_id VARCHAR(100),

    -- Clearance Audit Trail (29 JAN 2026)
    cleared_at TIMESTAMP,              -- When first changed from UNCLEARED
    cleared_by VARCHAR(200),           -- Who first cleared
    made_public_at TIMESTAMP,          -- When changed to PUBLIC (null if never)
    made_public_by VARCHAR(200),       -- Who made it public

    -- Soft Delete
    deleted_at TIMESTAMP,
    deleted_by VARCHAR(200),

    -- Timestamps
    created_at TIMESTAMP NOT NULL DEFAULT now(),
    updated_at TIMESTAMP NOT NULL DEFAULT now(),

    -- Constraints
    UNIQUE(dataset_id, resource_id, version_id)
);

-- Indexes
CREATE INDEX idx_assets_identity ON app.geospatial_assets(dataset_id, resource_id, version_id);
CREATE INDEX idx_assets_stac_item ON app.geospatial_assets(stac_item_id);
CREATE INDEX idx_assets_approval ON app.geospatial_assets(approval_state);
CREATE INDEX idx_assets_clearance ON app.geospatial_assets(clearance_state);
CREATE INDEX idx_assets_current_job ON app.geospatial_assets(current_job_id);
CREATE INDEX idx_assets_created ON app.geospatial_assets(created_at DESC);
CREATE INDEX idx_assets_pending ON app.geospatial_assets(approval_state)
    WHERE approval_state = 'pending_review' AND deleted_at IS NULL;
CREATE INDEX idx_assets_active ON app.geospatial_assets(deleted_at)
    WHERE deleted_at IS NULL;
```

### 6.3 Platform Requests Table

```sql
CREATE TABLE app.platform_requests (
    -- Primary Key
    request_id VARCHAR(64) PRIMARY KEY,

    -- Link to asset
    asset_id VARCHAR(64) REFERENCES app.geospatial_assets(asset_id),

    -- Request specification
    dataset_id VARCHAR(100) NOT NULL,
    resource_id VARCHAR(100) NOT NULL,
    version_id VARCHAR(100) NOT NULL,
    data_type VARCHAR(20) NOT NULL,
    container_name VARCHAR(200) NOT NULL,
    file_name VARCHAR(500) NOT NULL,
    overwrite BOOLEAN NOT NULL DEFAULT false,
    processing_options JSONB,

    -- Request state
    status VARCHAR(20) NOT NULL DEFAULT 'accepted',

    -- Link to job (NULL until worker picks up)
    job_id VARCHAR(64) REFERENCES app.jobs(job_id),

    -- Retry tracking
    retry_count INTEGER NOT NULL DEFAULT 0,
    last_error TEXT,

    -- Timestamps
    created_at TIMESTAMP NOT NULL DEFAULT now(),
    updated_at TIMESTAMP NOT NULL DEFAULT now()
);

-- Indexes
CREATE INDEX idx_requests_asset ON app.platform_requests(asset_id);
CREATE INDEX idx_requests_job ON app.platform_requests(job_id);
CREATE INDEX idx_requests_status ON app.platform_requests(status);
CREATE INDEX idx_requests_created ON app.platform_requests(created_at DESC);
CREATE INDEX idx_requests_pending ON app.platform_requests(status)
    WHERE status IN ('accepted', 'processing');
```

### 6.4 Asset Revisions Table

```sql
CREATE TABLE app.asset_revisions (
    -- Primary Key
    revision_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Foreign Key (CASCADE delete with asset)
    asset_id VARCHAR(64) NOT NULL
        REFERENCES app.geospatial_assets(asset_id) ON DELETE CASCADE,

    -- Revision snapshot
    revision INTEGER NOT NULL,
    job_id VARCHAR(64) NOT NULL REFERENCES app.jobs(job_id),
    content_hash VARCHAR(128),

    -- State at time of supersession
    approval_state_at_supersession app.approval_state_enum NOT NULL,
    clearance_state_at_supersession app.clearance_state_enum NOT NULL,
    reviewer_at_supersession VARCHAR(200),

    -- Timestamps
    created_at TIMESTAMP NOT NULL,
    superseded_at TIMESTAMP NOT NULL DEFAULT now(),

    -- Constraints
    UNIQUE(asset_id, revision)
);

-- Indexes
CREATE INDEX idx_revisions_asset ON app.asset_revisions(asset_id);
CREATE INDEX idx_revisions_asset_rev ON app.asset_revisions(asset_id, revision);
CREATE INDEX idx_revisions_superseded ON app.asset_revisions(superseded_at DESC);
```

### 6.5 Jobs Table Modification

```sql
-- Add FK to geospatial_assets
ALTER TABLE app.jobs ADD COLUMN
    asset_id VARCHAR(64)
    REFERENCES app.geospatial_assets(asset_id);

-- Index for lookups
CREATE INDEX idx_jobs_asset ON app.jobs(asset_id);
```

### 6.6 Advisory Lock Function

```sql
CREATE OR REPLACE FUNCTION app.upsert_geospatial_asset(
    p_asset_id VARCHAR(64),
    p_dataset_id VARCHAR(100),
    p_resource_id VARCHAR(100),
    p_version_id VARCHAR(100),
    p_data_type VARCHAR(20),
    p_stac_item_id VARCHAR(200),
    p_stac_collection_id VARCHAR(200),
    p_table_name VARCHAR(63) DEFAULT NULL,
    p_blob_path VARCHAR(500) DEFAULT NULL,
    p_overwrite BOOLEAN DEFAULT FALSE
)
RETURNS TABLE (
    operation VARCHAR(20),  -- 'created' | 'updated' | 'exists'
    new_revision INTEGER,
    error_message TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_existing RECORD;
BEGIN
    -- Acquire advisory lock for this asset_id (serialize concurrent requests)
    PERFORM pg_advisory_xact_lock(hashtext(p_asset_id));

    -- Check for existing record (excluding soft-deleted)
    SELECT * INTO v_existing
    FROM app.geospatial_assets
    WHERE asset_id = p_asset_id
      AND deleted_at IS NULL;

    IF v_existing IS NULL THEN
        -- Check if soft-deleted version exists
        SELECT * INTO v_existing
        FROM app.geospatial_assets
        WHERE asset_id = p_asset_id
          AND deleted_at IS NOT NULL;

        IF v_existing IS NOT NULL THEN
            -- Reactivate soft-deleted asset
            UPDATE app.geospatial_assets SET
                deleted_at = NULL,
                deleted_by = NULL,
                revision = v_existing.revision + 1,
                current_job_id = NULL,
                approval_state = 'pending_review',
                clearance_state = 'uncleared',
                reviewer = NULL,
                reviewed_at = NULL,
                rejection_reason = NULL,
                adf_run_id = NULL,
                table_name = p_table_name,
                blob_path = p_blob_path,
                updated_at = NOW()
            WHERE asset_id = p_asset_id;

            RETURN QUERY SELECT 'reactivated'::VARCHAR(20), v_existing.revision + 1, NULL::TEXT;
        ELSE
            -- Create new asset
            INSERT INTO app.geospatial_assets (
                asset_id, dataset_id, resource_id, version_id,
                data_type, stac_item_id, stac_collection_id,
                table_name, blob_path,
                revision, approval_state, clearance_state
            ) VALUES (
                p_asset_id, p_dataset_id, p_resource_id, p_version_id,
                p_data_type, p_stac_item_id, p_stac_collection_id,
                p_table_name, p_blob_path,
                1, 'pending_review', 'uncleared'
            );
            RETURN QUERY SELECT 'created'::VARCHAR(20), 1, NULL::TEXT;
        END IF;

    ELSIF p_overwrite THEN
        -- Log current revision to history
        INSERT INTO app.asset_revisions (
            asset_id, revision, job_id, content_hash,
            approval_state_at_supersession, clearance_state_at_supersession,
            reviewer_at_supersession, created_at, superseded_at
        ) SELECT
            asset_id, revision, current_job_id, content_hash,
            approval_state, clearance_state,
            reviewer, created_at, NOW()
        FROM app.geospatial_assets
        WHERE asset_id = p_asset_id;

        -- Update to new revision
        UPDATE app.geospatial_assets SET
            revision = revision + 1,
            current_job_id = NULL,
            content_hash = NULL,
            approval_state = 'pending_review',
            clearance_state = 'uncleared',
            reviewer = NULL,
            reviewed_at = NULL,
            rejection_reason = NULL,
            adf_run_id = NULL,
            table_name = COALESCE(p_table_name, table_name),
            blob_path = COALESCE(p_blob_path, blob_path),
            updated_at = NOW()
        WHERE asset_id = p_asset_id;

        RETURN QUERY SELECT 'updated'::VARCHAR(20), v_existing.revision + 1, NULL::TEXT;

    ELSE
        -- Exists and no overwrite requested
        RETURN QUERY SELECT 'exists'::VARCHAR(20), v_existing.revision,
            'Asset exists. Use overwrite=true to replace.'::TEXT;
    END IF;
END;
$$;
```

---

## 7. Workflows

### 7.1 Submit Workflow (New Asset)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         POST /api/platform/submit                       │
│                         (new dataset_id/resource_id/version_id)         │
└─────────────────────────────────┬───────────────────────────────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  1. Validate request        │
                    │  2. Generate IDs:           │
                    │     - request_id (unique)   │
                    │     - asset_id (determin.)  │
                    └──────────────┬──────────────┘
                                   │
                                   ▼
                    ┌─────────────────────────────┐
                    │  Call upsert_geospatial_    │
                    │  asset() with advisory lock │
                    │                             │
                    │  → Creates asset if new     │
                    │  → Returns 'exists' if dup  │
                    └──────────────┬──────────────┘
                                   │
                                   ▼
                    ┌─────────────────────────────┐
                    │  INSERT platform_requests   │
                    │                             │
                    │  status = 'accepted'        │
                    │  job_id = NULL              │◄─── NO JOB YET
                    └──────────────┬──────────────┘
                                   │
                                   ▼
                    ┌─────────────────────────────┐
                    │  RETURN TO CLIENT           │
                    │                             │
                    │  { request_id: "req-xxx",   │
                    │    status: "accepted",      │
                    │    asset_id: "asset-xxx",   │
                    │    monitor_url: "..." }     │
                    └──────────────┬──────────────┘
                                   │
                                   │  (async via Service Bus)
                                   ▼
                    ┌─────────────────────────────┐
                    │  Worker picks up request    │
                    │                             │
                    │  1. Create JOB              │
                    │  2. Update request.job_id   │
                    │  3. Update asset.current_   │
                    │     job_id                  │
                    │  4. Process data            │
                    │  5. Create services         │
                    │  6. Register STAC           │
                    │  7. Update content_hash     │
                    └─────────────────────────────┘
```

### 7.2 Submit Workflow (Overwrite)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         POST /api/platform/submit                       │
│                         (existing IDs, overwrite=true)                  │
└─────────────────────────────────┬───────────────────────────────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  Call upsert_geospatial_    │
                    │  asset(overwrite=true)      │
                    │                             │
                    │  1. Advisory lock           │
                    │  2. Log revision to history │
                    │  3. Increment revision      │
                    │  4. Reset approval/clearance│
                    └──────────────┬──────────────┘
                                   │
                                   ▼
                              (continue as above)
```

### 7.3 Approval Workflow

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         POST /api/platform/approve                      │
│                         { asset_id | request_id, clearance_level, ... } │
└─────────────────────────────────┬───────────────────────────────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  1. Lookup geospatial_asset │
                    │  2. Validate:               │
                    │     - approval = pending    │
                    │     - not deleted           │
                    │     - clearance_level valid │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────┴──────────────┐
                    │                             │
                    ▼                             ▼
          ┌─────────────────┐           ┌─────────────────┐
          │ clearance = ouo │           │ clearance = pub │
          │                 │           │                 │
          │ - Update STAC   │           │ - Update STAC   │
          │ - No ADF        │           │ - Trigger ADF   │
          │                 │           │ - Store run_id  │
          └────────┬────────┘           └────────┬────────┘
                   │                             │
                   └──────────────┬──────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  UPDATE geospatial_assets   │
                    │                             │
                    │  approval = approved        │
                    │  clearance = ouo | public   │
                    │  reviewer = ...             │
                    │  reviewed_at = NOW          │
                    └─────────────────────────────┘
```

### 7.4 Unpublish Workflow (Soft Delete)

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         POST /api/platform/unpublish                    │
│                         { asset_id | request_id }                       │
└─────────────────────────────────┬───────────────────────────────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  1. Lookup geospatial_asset │
                    │  2. Check clearance state   │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────┴──────────────┐
                    │                             │
                    ▼                             ▼
          ┌─────────────────┐           ┌─────────────────┐
          │ clearance = ouo │           │ clearance = pub │
          │ or uncleared    │           │                 │
          │                 │           │ TODO: Trigger   │
          │ - Delete table/ │           │ ADF Reversal    │
          │   blob          │           │ Pipeline        │
          │ - Delete STAC   │           │                 │
          │ - Soft delete   │           │ Then delete     │
          │   asset         │           │ internal...     │
          └────────┬────────┘           └────────┬────────┘
                   │                             │
                   └──────────────┬──────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────────┐
                    │  SOFT DELETE                │
                    │                             │
                    │  UPDATE geospatial_assets   │
                    │  SET deleted_at = NOW(),    │
                    │      deleted_by = ...       │
                    │  WHERE asset_id = ...       │
                    │                             │
                    │  (Requests preserved for    │
                    │   audit trail)              │
                    └─────────────────────────────┘
```

### 7.5 Combined State Machine

```
┌────────────────────────────────────────────────────────────────────────────┐
│                              LIFECYCLE                                      │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│  POST /submit (new)                                                        │
│       │                                                                    │
│       ▼                                                                    │
│  ┌──────────────────────────────────┐                                      │
│  │ revision: 1                      │                                      │
│  │ approval: pending_review         │                                      │
│  │ clearance: uncleared             │  ◄─── Services LIVE (internal)       │
│  └──────────────────┬───────────────┘                                      │
│                     │                                                      │
│        ┌────────────┼────────────┬───────────────┐                         │
│        │            │            │               │                         │
│        ▼            ▼            ▼               ▼                         │
│   POST /approve  POST /approve  POST /reject  POST /unpublish              │
│   (level=ouo)    (level=public) (reason)                                   │
│        │            │            │               │                         │
│        ▼            ▼            ▼               ▼                         │
│  ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐                  │
│  │ approved  │ │ approved  │ │ rejected  │ │ SOFT DEL  │                  │
│  │ ouo       │ │ public    │ │ uncleared │ │           │                  │
│  │           │ │ +ADF      │ │           │ │ deleted_at│                  │
│  └─────┬─────┘ └─────┬─────┘ └─────┬─────┘ └───────────┘                  │
│        │             │             │                                       │
│        └──────┬──────┴─────────────┘                                       │
│               │                                                            │
│     POST /submit (overwrite=true)                                          │
│               │                                                            │
│               ▼                                                            │
│  ┌──────────────────────────────────┐                                      │
│  │ revision: N+1                    │                                      │
│  │ approval: pending_review (RESET) │                                      │
│  │ clearance: uncleared (RESET)     │                                      │
│  │                                  │                                      │
│  │ Internal services: UPDATED       │                                      │
│  │ External (if was public):        │                                      │
│  │   UNCHANGED until re-approve     │                                      │
│  └──────────────────────────────────┘                                      │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘
```

---

## 8. API Compatibility

### 8.1 Status Endpoint Behavior

The `/api/platform/status/{identifier}` endpoint accepts `request_id`, `job_id`, or `asset_id`:

```python
async def get_status(identifier: str) -> dict:
    """
    Accepts: request_id OR job_id OR asset_id
    Returns: Unified status response
    """
    # Try as request_id first
    request = await request_repo.get(identifier)
    if request:
        asset = await asset_repo.get(request.asset_id)
        job = await job_repo.get(request.job_id) if request.job_id else None
        return build_status_response(request, asset, job)

    # Try as asset_id
    asset = await asset_repo.get(identifier)
    if asset:
        return build_status_response(None, asset, None)

    # Try as job_id
    job = await job_repo.get(identifier)
    if job:
        asset = await asset_repo.get(job.asset_id)
        return build_status_response(None, asset, job)

    raise NotFoundError(f"No request, asset, or job found: {identifier}")
```

### 8.2 Response Changes

| Field | Before | After |
|-------|--------|-------|
| `request_id` | Always present | Present if queried by request |
| `job_id` | Always present | **May be NULL** (queued state) |
| `status` | From job | Derived: queued if no job, else job.status |
| `asset_id` | N/A | **NEW** - always present |
| `revision` | N/A | **NEW** - current revision number |
| `approval_state` | N/A | **NEW** |
| `clearance_state` | N/A | **NEW** |

### 8.3 Approve Endpoint Changes

```python
# BEFORE
POST /api/platform/approve
{
    "job_id": "...",
    "reviewer": "user@example.com"
}

# AFTER - clearance_level REQUIRED
POST /api/platform/approve
{
    "job_id": "...",           # or request_id or asset_id
    "reviewer": "user@example.com",
    "clearance_level": "ouo"   # REQUIRED: "ouo" or "public"
}
```

---

## 9. Implementation Plan

### Phase 1: Models & Schema (Foundation) ✅ COMPLETE

| Task | File | Status |
|------|------|--------|
| 1.1 | `core/models/asset.py` | ✅ Created with `ApprovalState`, `ClearanceState`, `GeospatialAsset`, `AssetRevision` |
| 1.2 | `core/models/platform.py` | ⏭️ Deferred - using existing `ApiRequest` pattern |
| 1.3 | `core/models/__init__.py` | ✅ Exports added |
| 1.4 | `core/schema/sql_generator.py` | ✅ Imports and enums added |
| 1.5 | `core/schema/sql_generator.py` | ✅ Tables added via `generate_table_from_model()` |
| 1.6 | `core/schema/sql_generator.py` | ✅ Indexes added via `generate_indexes_from_model()` |
| 1.7 | `core/schema/sql_generator.py` | ✅ `upsert_geospatial_asset()` function added |

### Phase 2: Repository Layer ✅ COMPLETE

| Task | File | Status |
|------|------|--------|
| 2.1 | `infrastructure/asset_repository.py` | ✅ Created `GeospatialAssetRepository` (~549 lines) |
| 2.2 | `infrastructure/request_repository.py` | ⏭️ Deferred - using existing `PlatformRepository` |
| 2.3 | `infrastructure/revision_repository.py` | ✅ Created `AssetRevisionRepository` (~221 lines) |
| 2.4 | `infrastructure/__init__.py` | ✅ Exports added |
| 2.5 | `infrastructure/repository_factory.py` | ⏭️ Deferred - direct instantiation pattern |

### Phase 3: Service Layer ✅ COMPLETE

| Task | File | Status |
|------|------|--------|
| 3.1 | `services/asset_service.py` | ✅ Created with create/update/approve/reject/delete (~456 lines) |
| 3.2 | `services/platform_service.py` | ⏭️ Deferred - AssetService used directly |
| 3.3 | `services/__init__.py` | ℹ️ Not exported (direct import pattern - acceptable) |

### Phase 4: API Layer ✅ COMPLETE

| Task | File | Status |
|------|------|--------|
| 4.1 | `triggers/platform/submit.py` | ✅ Creates GeospatialAsset before job |
| 4.2 | `triggers/trigger_platform_status.py` | ✅ Supports asset_id lookup, returns asset info |
| 4.3 | `triggers/trigger_approvals.py` | ✅ Requires `clearance_level`, updates asset |
| 4.3b | `triggers/trigger_approvals.py` | ✅ Added `platform_reject` endpoint |
| 4.4 | `triggers/platform/unpublish.py` | ✅ Soft deletes asset |
| 4.5 | `triggers/platform/platform_bp.py` | ✅ Routes registered for approve/reject/revoke |

### Phase 5: Worker Integration ✅ COMPLETE

| Task | File | Status |
|------|------|--------|
| 5.1 | `services/handler_vector_docker_complete.py` | ✅ Links job to asset |
| 5.2 | `services/handler_process_raster_complete.py` | ✅ Links job to asset |
| 5.3 | `core/machine_factory.py` | ✅ Updates asset on job completion |

### Phase 6: Migration ⏭️ SKIPPED

| Task | Status | Notes |
|------|--------|-------|
| 6.1-6.3 | ⏭️ Skipped | Clean slate deployment - no pre-V0.8 data to migrate |

### Phase 7: Cleanup 🔜 FUTURE

| Task | File | Status |
|------|------|--------|
| 7.1 | Various | 🔜 Future - Deprecate old `api_requests` patterns |
| 7.2 | Various | 🔜 Future - Deprecate old `dataset_approvals` table |
| 7.3 | Documentation | ✅ This document updated |

---

## 10. Migration Strategy

### 10.1 Phase 1: Schema Creation

```bash
# Deploy new schema
POST /api/dbadmin/maintenance?action=ensure&confirm=yes
```

This creates:
- New enums (`approval_state_enum`, `clearance_state_enum`)
- New tables (`geospatial_assets`, `platform_requests`, `asset_revisions`)
- New function (`upsert_geospatial_asset`)

### 10.2 Phase 2: Data Migration

```sql
-- Migrate existing api_requests to geospatial_assets + platform_requests
-- Run as one-time migration script

-- Step 1: Create assets from api_requests
INSERT INTO app.geospatial_assets (
    asset_id,
    dataset_id, resource_id, version_id,
    data_type,
    stac_item_id, stac_collection_id,
    table_name, blob_path,
    revision, current_job_id,
    approval_state, clearance_state,
    created_at, updated_at
)
SELECT
    encode(sha256((dataset_id || '|' || resource_id || '|' || version_id)::bytea), 'hex')::varchar(32),
    dataset_id, resource_id, version_id,
    data_type,
    -- Derive STAC IDs from existing job results
    COALESCE(stac_item_id, lower(dataset_id || '-' || resource_id || '-' || version_id)),
    COALESCE(stac_collection_id, lower(dataset_id)),
    NULL, NULL,  -- table_name, blob_path populated by handler
    1 as revision,
    job_id as current_job_id,
    CASE WHEN da.status = 'approved' THEN 'approved' ELSE 'pending_review' END,
    CASE WHEN da.classification = 'public' THEN 'public'
         WHEN da.classification = 'ouo' THEN 'ouo'
         ELSE 'uncleared' END,
    ar.created_at, ar.updated_at
FROM app.api_requests ar
LEFT JOIN app.dataset_approvals da ON ar.job_id = da.job_id
ON CONFLICT (dataset_id, resource_id, version_id) DO NOTHING;

-- Step 2: Create platform_requests from api_requests
INSERT INTO app.platform_requests (
    request_id,
    asset_id,
    dataset_id, resource_id, version_id,
    data_type, container_name, file_name,
    overwrite, processing_options,
    status, job_id,
    retry_count, last_error,
    created_at, updated_at
)
SELECT
    request_id,
    encode(sha256((dataset_id || '|' || resource_id || '|' || version_id)::bytea), 'hex')::varchar(32),
    dataset_id, resource_id, version_id,
    data_type, container_name, file_name,
    false, processing_options::jsonb,
    status, job_id,
    retry_count, error_message,
    created_at, updated_at
FROM app.api_requests;

-- Step 3: Add asset_id to jobs
UPDATE app.jobs j SET
    asset_id = encode(sha256((
        (j.parameters->>'dataset_id') || '|' ||
        (j.parameters->>'resource_id') || '|' ||
        (j.parameters->>'version_id')
    )::bytea), 'hex')::varchar(32)
WHERE j.parameters ? 'dataset_id'
  AND j.parameters ? 'resource_id'
  AND j.parameters ? 'version_id';
```

### 10.3 Phase 3: Validation

```sql
-- Verify migration
SELECT COUNT(*) FROM app.geospatial_assets;
SELECT COUNT(*) FROM app.platform_requests;
SELECT COUNT(*) FROM app.jobs WHERE asset_id IS NOT NULL;

-- Check for orphans
SELECT COUNT(*) FROM app.platform_requests WHERE asset_id IS NULL;
```

### 10.4 Phase 4: Deprecation

After validation:
1. Mark `api_requests` as deprecated (keep for audit)
2. Mark `dataset_approvals` as deprecated (keep for audit)
3. Update code to use new tables exclusively

---

## 11. Future: DAG Orchestration

This entity model enables future DAG (Directed Acyclic Graph) orchestration:

### Current: Linear Pipeline

```
Request → Job → Tasks (parallel within stages) → Complete
```

### Future: DAG Pipeline

```
Request → Orchestrator → DAG Definition
                              │
                    ┌─────────┼─────────┐
                    ▼         ▼         ▼
                  Job A     Job B     Job C
                    │         │         │
                    └─────────┼─────────┘
                              ▼
                            Job D
                              │
                              ▼
                          Complete
```

### How This Model Supports DAG

| Concept | Current | DAG Future |
|---------|---------|------------|
| Request | 1 request → 1 job | 1 request → N jobs (DAG nodes) |
| Job | Monolithic | Node in DAG |
| asset_id | Set by single job | Set by terminal job in DAG |
| current_job_id | Single job | Terminal job or orchestrator |

The `PlatformRequest` becomes the DAG specification:

```python
class PlatformRequest:
    # ... existing fields ...

    # Future: DAG support
    dag_definition: Optional[dict]  # Node/edge definitions
    root_job_id: Optional[str]      # First job in DAG
    terminal_job_ids: List[str]     # Final jobs in DAG
```

---

## 12. Design Decisions

### Resolved

| # | Decision | Resolution | Rationale |
|---|----------|------------|-----------|
| 1 | Cascade on delete | **Soft delete** with `deleted_at` | Full audit trail |
| 2 | Retry behavior | **Same request, new job** | Track `retry_count` on request |
| 3 | STAC lifecycle | **On job complete** (current) | Future: hide uncleared from search |
| 4 | Approve from rejected | **Must overwrite first** | Rejection means "data is bad" |
| 5 | When create asset | **On request receipt** | Asset exists before job runs |
| 6 | Entity naming | **GeospatialAsset** | Not "published" - full lifecycle |
| 7 | Content hash timing | **On job execution** | Current behavior maintained |
| 8 | Concurrent requests | **Advisory locks** | Same pattern as CoreMachine |
| 9 | Platform identification | **`platform_id` + `platform_refs`** | Flexible B2B support, not just DDH |
| 10 | Platform registry | **Yes, with registry table** | Validates platforms, documents required refs |
| 11 | DDH columns | **Keep during migration** | Backward compatible, deprecate later |
| 12 | Asset ID formula | **Include `platform_id` in hash** | Uniqueness across platforms |

---

## 13. TODOs

### High Priority ✅ COMPLETE

- [x] **Create `core/models/asset.py`** with all models
- [x] **Update `core/schema/sql_generator.py`** with new enums, tables, function
- [x] **Create `infrastructure/asset_repository.py`**
- [x] **Create `infrastructure/revision_repository.py`**
- [x] **Create `services/asset_service.py`**
- [x] **Update Platform API triggers** (submit, status, approve, reject, revoke, unpublish)
- [x] **Update worker handlers** (vector_docker_complete, process_raster_complete)
- [x] **Update machine_factory.py** (asset update on job completion)

### Medium Priority

- [ ] **ADF Reversal Pipeline** - Implement unpublish for PUBLIC assets in external zone
- [x] ~~**Data migration script**~~ - SKIPPED (clean slate deployment)
- [x] **Update approval service** - Require `clearance_level` parameter

### Clearance Change Enhancement (29 JAN 2026) ✅ COMPLETE

- [x] **Add clearance audit columns to model** - `cleared_at`, `cleared_by`, `made_public_at`, `made_public_by`
- [x] **Update sql_generator.py** - Add clearance audit columns to DDL (reset on overwrite/reactivate)
- [x] **Update AssetService.create_or_update_asset()** - Accept optional `clearance_level` + `submitted_by`
- [x] **Update submit trigger** - Accept optional `clearance_level` from request body (or `access_level`)
- [x] **Update AssetService.approve()** - Allow re-approval from APPROVED state to change clearance
- [x] **Add downgrade warning** - Response includes `warning` + `action_required` when public→ouo

### Low Priority (Future)

- [ ] **Artifact table revision** - Keep for timer triggers, consider merge
- [ ] **STAC searchability** - Hide uncleared from STAC search
- [ ] **DAG orchestration** - Multi-job request support
- [ ] **PlatformRequest model update** - Rename ApiRequest when ready for breaking change

### Platform Registry (29 JAN 2026) ✅ COMPLETE

- [x] **Create `core/models/platform_registry.py`** - Platform model with required_refs + DDH_PLATFORM constant
- [x] **Create `infrastructure/platform_registry_repository.py`** - CRUD for platforms (~300 lines)
- [x] **Update `core/models/asset.py`** - Add `platform_id`, `platform_refs` fields + GIN index
- [x] **Update `infrastructure/asset_repository.py`** - Add `list_by_platform_refs()` with JSONB containment
- [x] **Update `services/asset_service.py`** - Add `list_by_platform_refs()` wrapper
- [x] **Update `core/schema/sql_generator.py`** - Add platforms table, GIN index, seed DDH via `generate_seed_data()`
- [x] **Update `generate_asset_id()`** - New signature: `(platform_id, platform_refs)` with backward compat
- [x] **Add API endpoints** - `GET /api/platforms` and `GET /api/platforms/{platform_id}`

---

## 14. Platform Registry

**Added**: 29 JAN 2026
**Status**: ✅ IMPLEMENTED (29 JAN 2026)
**Purpose**: Enable flexible asset lookups by arbitrary B2B platform identifiers

### 14.1 Problem Statement

Currently, `GeospatialAsset` has DDH-specific columns hardcoded:
- `dataset_id`, `resource_id`, `version_id`

This creates two issues:
1. **Tight coupling** - Can't support other B2B platforms with different identifier schemes
2. **No partial lookups** - Can't query "all assets for dataset X" without knowing resource/version

### 14.2 Solution: Platform Registry

Introduce a registry table to define valid platforms and their identifier requirements, plus flexible JSONB storage for platform-specific lookups.

### 14.3 New Entity: Platform

```python
class Platform(BaseModel):
    """
    Registry entry for a B2B platform integration.

    Defines what identifiers a platform requires/supports.

    Table: app.platforms
    Primary Key: platform_id
    """
    model_config = ConfigDict(
        use_enum_values=True,
        extra='ignore',
        str_strip_whitespace=True
    )

    # =========================================================================
    # DDL GENERATION HINTS
    # =========================================================================
    __sql_table_name: ClassVar[str] = "platforms"
    __sql_schema: ClassVar[str] = "app"
    __sql_primary_key: ClassVar[List[str]] = ["platform_id"]
    __sql_indexes: ClassVar[List[Dict[str, Any]]] = [
        {"columns": ["is_active"], "name": "idx_platforms_active"},
    ]

    # =========================================================================
    # FIELDS
    # =========================================================================
    platform_id: str = Field(
        ...,
        max_length=50,
        pattern=r'^[a-z][a-z0-9_]*$',
        description="Unique platform identifier (lowercase, underscores allowed)"
    )
    display_name: str = Field(
        ...,
        max_length=100,
        description="Human-readable platform name"
    )
    description: Optional[str] = Field(
        default=None,
        description="Platform description"
    )
    required_refs: List[str] = Field(
        default_factory=list,
        description="Required keys in platform_refs (e.g., ['dataset_id', 'resource_id', 'version_id'])"
    )
    optional_refs: List[str] = Field(
        default_factory=list,
        description="Optional keys in platform_refs"
    )
    is_active: bool = Field(
        default=True,
        description="Whether platform accepts new submissions"
    )
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
    updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )
```

### 14.4 GeospatialAsset Changes

Add two new fields to `GeospatialAsset`:

```python
class GeospatialAsset(BaseModel):
    # ... existing fields ...

    # =========================================================================
    # PLATFORM IDENTIFICATION (NEW - 29 JAN 2026)
    # =========================================================================
    platform_id: str = Field(
        default="ddh",
        max_length=50,
        description="FK to platforms.platform_id - identifies B2B platform"
    )
    platform_refs: Dict[str, Any] = Field(
        default_factory=dict,
        description="Platform-specific identifiers as JSONB for flexible queries"
    )

    # =========================================================================
    # DDH-SPECIFIC COLUMNS (KEPT FOR BACKWARD COMPATIBILITY)
    # =========================================================================
    # These remain during migration period. Eventually deprecated.
    dataset_id: str   # Mirrored in platform_refs["dataset_id"]
    resource_id: str  # Mirrored in platform_refs["resource_id"]
    version_id: str   # Mirrored in platform_refs["version_id"]
```

**DDL Changes:**

```sql
-- Add columns to geospatial_assets
ALTER TABLE app.geospatial_assets ADD COLUMN
    platform_id VARCHAR(50) NOT NULL DEFAULT 'ddh'
    REFERENCES app.platforms(platform_id);

ALTER TABLE app.geospatial_assets ADD COLUMN
    platform_refs JSONB NOT NULL DEFAULT '{}';

-- GIN index for efficient JSONB containment queries
CREATE INDEX idx_assets_platform_refs ON app.geospatial_assets
    USING GIN (platform_refs);

-- Composite index for platform + refs queries
CREATE INDEX idx_assets_platform_id ON app.geospatial_assets(platform_id);
```

### 14.5 Updated Asset ID Generation

Include `platform_id` in the hash for uniqueness across platforms:

```python
@staticmethod
def generate_asset_id(platform_id: str, platform_refs: Dict[str, Any]) -> str:
    """
    Generate deterministic asset ID from platform identity.

    Args:
        platform_id: Platform identifier (e.g., "ddh")
        platform_refs: Platform-specific identifiers

    Returns:
        32-character hex string (truncated SHA256)

    Example:
        generate_asset_id("ddh", {
            "dataset_id": "IDN_lulc",
            "resource_id": "jakarta_2024",
            "version_id": "v1.0"
        })
        # Returns: "a7b3c9d2..." (deterministic)
    """
    import hashlib
    import json
    # Sort keys for deterministic ordering
    sorted_refs = json.dumps(platform_refs, sort_keys=True, separators=(',', ':'))
    composite = f"{platform_id}|{sorted_refs}"
    return hashlib.sha256(composite.encode()).hexdigest()[:32]
```

**Backward Compatibility Helper:**

```python
@staticmethod
def generate_asset_id_ddh(dataset_id: str, resource_id: str, version_id: str) -> str:
    """
    Generate asset ID for DDH platform (backward compatible).

    Equivalent to:
        generate_asset_id("ddh", {
            "dataset_id": dataset_id,
            "resource_id": resource_id,
            "version_id": version_id
        })
    """
    return GeospatialAsset.generate_asset_id("ddh", {
        "dataset_id": dataset_id,
        "resource_id": resource_id,
        "version_id": version_id
    })
```

### 14.6 Database Schema

```sql
-- Platform Registry Table
CREATE TABLE app.platforms (
    platform_id VARCHAR(50) PRIMARY KEY,
    display_name VARCHAR(100) NOT NULL,
    description TEXT,
    required_refs JSONB NOT NULL DEFAULT '[]',
    optional_refs JSONB NOT NULL DEFAULT '[]',
    is_active BOOLEAN NOT NULL DEFAULT true,
    created_at TIMESTAMP NOT NULL DEFAULT now(),
    updated_at TIMESTAMP NOT NULL DEFAULT now()
);

-- Index for active platforms
CREATE INDEX idx_platforms_active ON app.platforms(is_active) WHERE is_active = true;

-- Seed DDH platform (on schema ensure)
INSERT INTO app.platforms (platform_id, display_name, description, required_refs)
VALUES (
    'ddh',
    'Data Distribution Hub',
    'Primary B2B integration platform with dataset/resource/version hierarchy',
    '["dataset_id", "resource_id", "version_id"]'
) ON CONFLICT (platform_id) DO NOTHING;
```

### 14.7 Query Patterns

**Query all assets for a dataset (any resource/version):**
```sql
SELECT * FROM app.geospatial_assets
WHERE platform_id = 'ddh'
  AND platform_refs @> '{"dataset_id": "IDN_lulc"}'
  AND deleted_at IS NULL;
```

**Query all assets for a specific resource (any version):**
```sql
SELECT * FROM app.geospatial_assets
WHERE platform_id = 'ddh'
  AND platform_refs @> '{"dataset_id": "IDN_lulc", "resource_id": "jakarta_2024"}'
  AND deleted_at IS NULL;
```

**Future B2B platform with different identifier scheme:**
```sql
-- Platform "acme_geo" uses project/layer/version
SELECT * FROM app.geospatial_assets
WHERE platform_id = 'acme_geo'
  AND platform_refs @> '{"project": "flood_mapping"}'
  AND deleted_at IS NULL;
```

### 14.8 Repository Methods

**PlatformRegistryRepository:**

```python
class PlatformRegistryRepository(PostgreSQLRepository):
    """Repository for platform registry CRUD."""

    def get_by_id(self, platform_id: str) -> Optional[Platform]:
        """Get platform by ID."""

    def list_active(self) -> List[Platform]:
        """List all active platforms."""

    def create(self, platform: Platform) -> Platform:
        """Create new platform."""

    def update(self, platform_id: str, updates: Dict) -> Optional[Platform]:
        """Update platform."""

    def validate_refs(self, platform_id: str, refs: Dict) -> Tuple[bool, List[str]]:
        """Validate refs against platform's required_refs. Returns (valid, missing_keys)."""
```

**GeospatialAssetRepository additions:**

```python
def list_by_platform_refs(
    self,
    platform_id: str,
    refs: Dict[str, Any],
    limit: int = 100,
    offset: int = 0,
    include_deleted: bool = False
) -> List[GeospatialAsset]:
    """
    Query assets by platform reference key-value pairs.

    Uses JSONB containment (@>) for efficient querying.

    Args:
        platform_id: Platform identifier
        refs: Key-value pairs to match (partial match supported)
        limit: Maximum results
        offset: Pagination offset
        include_deleted: Include soft-deleted assets

    Returns:
        List of matching GeospatialAsset records
    """
    query = sql.SQL("""
        SELECT * FROM {schema}.{table}
        WHERE platform_id = %s
          AND platform_refs @> %s::jsonb
          {deleted_filter}
        ORDER BY created_at DESC
        LIMIT %s OFFSET %s
    """).format(
        schema=sql.Identifier(self.schema),
        table=sql.Identifier(self.table),
        deleted_filter=sql.SQL("") if include_deleted else sql.SQL("AND deleted_at IS NULL")
    )
    # ... execute query ...
```

### 14.9 Service Methods

**AssetService additions:**

```python
def list_by_platform_refs(
    self,
    platform_id: str,
    refs: Dict[str, Any],
    limit: int = 100
) -> List[GeospatialAsset]:
    """
    Query assets by platform reference key-value pairs.

    Examples:
        # All DDH assets for dataset IDN_lulc
        service.list_by_platform_refs("ddh", {"dataset_id": "IDN_lulc"})

        # All DDH assets for a specific resource (any version)
        service.list_by_platform_refs("ddh", {
            "dataset_id": "IDN_lulc",
            "resource_id": "jakarta_2024"
        })

        # Future: Other B2B platform
        service.list_by_platform_refs("acme_geo", {"project": "flood_mapping"})
    """
    # Validate platform exists
    platform = self._platform_repo.get_by_id(platform_id)
    if not platform:
        raise ValueError(f"Unknown platform: {platform_id}")

    return self._asset_repo.list_by_platform_refs(platform_id, refs, limit=limit)

def validate_platform_refs(
    self,
    platform_id: str,
    refs: Dict[str, Any]
) -> None:
    """
    Validate that refs contain all required keys for platform.

    Raises:
        ValueError: If platform unknown or required refs missing
    """
    platform = self._platform_repo.get_by_id(platform_id)
    if not platform:
        raise ValueError(f"Unknown platform: {platform_id}")

    missing = [key for key in platform.required_refs if key not in refs]
    if missing:
        raise ValueError(
            f"Missing required refs for platform '{platform_id}': {missing}"
        )
```

### 14.10 API Endpoint

**GET /api/platforms** - List supported platforms:

```python
@platform_bp.route('/api/platform/platforms', methods=['GET'])
def list_platforms(req: func.HttpRequest) -> func.HttpResponse:
    """
    List all supported B2B platforms.

    Returns:
        {
            "platforms": [
                {
                    "platform_id": "ddh",
                    "display_name": "Data Distribution Hub",
                    "required_refs": ["dataset_id", "resource_id", "version_id"],
                    "optional_refs": [],
                    "is_active": true
                }
            ]
        }
    """
```

### 14.11 Implementation Files

| File | Status | Changes |
|------|--------|---------|
| `core/models/platform_registry.py` | **NEW** | `Platform` model (~80 lines) |
| `infrastructure/platform_registry_repository.py` | **NEW** | CRUD repository (~150 lines) |
| `core/models/asset.py` | Modify | Add `platform_id`, `platform_refs`, update `generate_asset_id()` |
| `infrastructure/asset_repository.py` | Modify | Add `list_by_platform_refs()`, FK to platforms |
| `services/asset_service.py` | Modify | Add `list_by_platform_refs()`, `validate_platform_refs()` |
| `core/schema/sql_generator.py` | Modify | Add platforms table, GIN index, seed DDH |
| `core/models/__init__.py` | Modify | Export `Platform` |
| `infrastructure/__init__.py` | Modify | Export `PlatformRegistryRepository` |
| `triggers/platform/platform_bp.py` | Modify | Add `GET /api/platform/platforms` |

**Estimated total: ~300 lines new code, ~100 lines modifications**

### 14.12 Migration Path

1. **Phase 1: Add infrastructure** (no breaking changes)
   - Create `platforms` table with DDH seed
   - Add `platform_id` and `platform_refs` columns to `geospatial_assets`
   - Populate `platform_refs` from existing DDH columns
   - Update `generate_asset_id()` to support both old and new patterns

2. **Phase 2: Update creation flow**
   - New assets populate both explicit columns AND `platform_refs`
   - Validate against platform's `required_refs`

3. **Phase 3: Add query methods**
   - `list_by_platform_refs()` in repository and service
   - API endpoint for platform listing

4. **Phase 4: Migrate callers** (future, gradual)
   - Update code to use `platform_refs` instead of explicit columns
   - Eventually deprecate `dataset_id`, `resource_id`, `version_id` columns

---

## Appendix A: Identifier Generation

### asset_id (Updated for Platform Registry)

```python
def generate_asset_id(platform_id: str, platform_refs: Dict[str, Any]) -> str:
    """
    Deterministic ID from platform identity.
    Same inputs always produce same ID (idempotent).

    Platform ID is included in hash to ensure uniqueness across platforms.
    Refs are sorted for deterministic ordering.

    Args:
        platform_id: Platform identifier (e.g., "ddh", "acme_geo")
        platform_refs: Platform-specific identifiers as dict

    Returns:
        32-character hex string

    Examples:
        # DDH asset
        generate_asset_id("ddh", {
            "dataset_id": "IDN_lulc",
            "resource_id": "jakarta_2024",
            "version_id": "v1.0"
        })
        # -> "a7b3c9d2e1f4..."

        # Other B2B platform
        generate_asset_id("acme_geo", {
            "project": "flood_mapping",
            "layer": "depth_2024"
        })
        # -> "f2e1d4c3b5a6..."
    """
    import hashlib
    import json
    sorted_refs = json.dumps(platform_refs, sort_keys=True, separators=(',', ':'))
    composite = f"{platform_id}|{sorted_refs}"
    return hashlib.sha256(composite.encode()).hexdigest()[:32]


def generate_asset_id_ddh(dataset_id: str, resource_id: str, version_id: str) -> str:
    """
    Backward-compatible helper for DDH platform.

    Equivalent to generate_asset_id("ddh", {...}).
    """
    return generate_asset_id("ddh", {
        "dataset_id": dataset_id,
        "resource_id": resource_id,
        "version_id": version_id
    })
```

### Legacy asset_id (Pre-Platform Registry)

```python
# DEPRECATED - Use generate_asset_id() with platform_id instead
def generate_asset_id_legacy(dataset_id: str, resource_id: str, version_id: str) -> str:
    """
    Original formula without platform_id.
    Kept for reference only - DO NOT USE for new assets.
    """
    import hashlib
    composite = f"{dataset_id}|{resource_id}|{version_id}"
    return hashlib.sha256(composite.encode()).hexdigest()[:32]
```

### request_id

```python
def generate_request_id(dataset_id: str, resource_id: str, version_id: str) -> str:
    """
    Unique ID for each request (includes timestamp for uniqueness).
    """
    import hashlib
    from datetime import datetime
    composite = f"{dataset_id}|{resource_id}|{version_id}|{datetime.utcnow().isoformat()}"
    return hashlib.sha256(composite.encode()).hexdigest()[:32]
```

---

## Appendix B: Service Output Naming

### Vector (PostGIS Table)

```python
def generate_table_name(dataset_id: str, resource_id: str, version_id: str) -> str:
    """PostgreSQL-safe table name (max 63 chars)."""
    import re
    composite = f"{dataset_id}_{resource_id}_{version_id}"
    slugified = re.sub(r'[^a-z0-9_]', '', composite.lower().replace(' ', '_'))
    return slugified[:63]
```

### Raster (COG Blob Path)

```python
def generate_blob_path(dataset_id: str, resource_id: str, version_id: str) -> str:
    """Azure Blob path for COG storage."""
    import re
    composite = f"{dataset_id}/{resource_id}/{version_id}"
    return re.sub(r'[^a-z0-9/\-]', '', composite.lower().replace(' ', '-'))
```

### STAC Item ID

```python
def generate_stac_item_id(dataset_id: str, resource_id: str, version_id: str) -> str:
    """STAC-compliant item identifier."""
    import re
    composite = f"{dataset_id}_{resource_id}_{version_id}"
    return re.sub(r'[^a-z0-9\-]', '', composite.lower().replace('_', '-').replace(' ', '-'))
```

---

## 15. DAG Orchestration Enhancements

**Added**: 29 JAN 2026
**Status**: ✅ IMPLEMENTED (29 JAN 2026)
**Purpose**: Prepare GeospatialAsset for DAG-based workflow orchestration in rmhdagmaster

### 15.1 DAG Architecture Overview

The rmhdagmaster project implements a DAG (Directed Acyclic Graph) orchestrator that separates orchestration from execution. This is Epoch 5 of the rmhgeoapi platform evolution.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    rmhdagmaster DAG ARCHITECTURE                             │
└─────────────────────────────────────────────────────────────────────────────┘

  CURRENT (Epoch 4):
  ══════════════════
  Request → Job → Tasks (parallel within stages) → Complete
  Problem: "Last task turns out lights" - failures go undetected for 24+ hours

  FUTURE (Epoch 5 - DAG):
  ═══════════════════════
  Request → Orchestrator → DAG Workflow
                                │
                      ┌─────────┴─────────┐
                      ▼                   ▼
                   Node A              Node B
                      │                   │
                      └─────────┬─────────┘
                                ▼
                             Node C
                                │
                                ▼
                            Complete

  Benefits:
  - Failure detection within 60 seconds
  - Conditional routing based on runtime data
  - Fan-out/fan-in parallel processing
  - Single source of truth for workflow state
```

**Key Terminology**:

| Term | Definition |
|------|------------|
| **Job** | Complete workflow execution (one job per workflow run) |
| **Node** | A step in the DAG workflow (defined in YAML) |
| **Task** | What gets dispatched to a worker queue (one node = one task, except fan-out) |
| **Handler** | The actual work logic executed by workers |

### 15.2 How V0.8 Enables DAG

The V0.8 Request/Job separation is critical for DAG:

| Concept | V0.8 Design | DAG Benefit |
|---------|-------------|-------------|
| **Request ≠ Job** | Request acknowledged immediately, job created later | DAG orchestrator creates jobs, not platform |
| **GeospatialAsset** | First-class entity with lifecycle | One asset can have multiple job attempts |
| **request_id** | B2B tracking ID | Callbacks route to correct partner |
| **current_job_id** | Points to active job | Asset always knows its processing job |

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    V0.8 + DAG FLOW                                           │
└─────────────────────────────────────────────────────────────────────────────┘

  1. DDH sends request
       │
       ▼
  2. Platform FA creates:
     - PlatformRequest (request_id) ◄── Acknowledged immediately
     - GeospatialAsset (asset_id)   ◄── Created BEFORE job exists
       │
       ▼
  3. Platform FA sends to dag-jobs queue:
     { request_id, asset_id, workflow_id, inputs }
       │
       ▼
  4. DAG Orchestrator creates:
     - dag_jobs (job_id) ◄── Created when work STARTS
     - Links: asset.current_job_id = job_id
       │
       ▼
  5. DAG executes workflow nodes
       │
       ▼
  6. On completion: Updates asset (stac_item_id, blob_path, etc.)
```

### 15.3 Proposed GeospatialAsset Enhancements

These fields prepare GeospatialAsset for DAG without breaking Epoch 4 functionality.

#### 15.3.1 New Enum: ProcessingStatus

```python
class ProcessingStatus(str, Enum):
    """
    Processing lifecycle state (distinct from approval/clearance).

    This fourth state dimension tracks workflow execution status,
    separate from review workflows.

    Transitions:
    - PENDING → PROCESSING (job starts)
    - PROCESSING → COMPLETED (job succeeds)
    - PROCESSING → FAILED (job fails)
    - FAILED → PENDING (retry submitted)
    """
    PENDING = "pending"          # Request received, no job yet (or retry queued)
    PROCESSING = "processing"    # Job running (DAG workflow active)
    COMPLETED = "completed"      # Job finished successfully
    FAILED = "failed"            # Job failed (may retry)
```

#### 15.3.2 New Fields: Tier 1 (NEEDED for DAG)

```python
class GeospatialAsset(BaseModel):
    # ... existing fields ...

    # =========================================================================
    # DAG ORCHESTRATION FIELDS - TIER 1: NEEDED
    # =========================================================================

    workflow_id: Optional[str] = Field(
        default=None,
        max_length=64,
        description="Workflow that processed this asset (e.g., 'raster_processing')"
    )

    workflow_version: Optional[int] = Field(
        default=None,
        ge=1,
        description="Version of workflow used (for debugging/rollback)"
    )

    job_count: int = Field(
        default=0,
        ge=0,
        description="Number of job attempts for this asset (retries + revisions)"
    )

    last_request_id: Optional[str] = Field(
        default=None,
        max_length=64,
        description="Most recent request ID (for callback routing to B2B partner)"
    )
```

#### 15.3.3 New Fields: Tier 2 (HIGHLY HELPFUL for DAG)

```python
class GeospatialAsset(BaseModel):
    # ... existing fields ...

    # =========================================================================
    # DAG ORCHESTRATION FIELDS - TIER 2: HIGHLY HELPFUL
    # =========================================================================

    processing_status: ProcessingStatus = Field(
        default=ProcessingStatus.PENDING,
        description="Processing lifecycle state (distinct from approval)"
    )

    processing_started_at: Optional[datetime] = Field(
        default=None,
        description="When first job started (for SLA tracking)"
    )

    processing_completed_at: Optional[datetime] = Field(
        default=None,
        description="When processing finished successfully"
    )

    last_error: Optional[str] = Field(
        default=None,
        max_length=2000,
        description="Last error message if failed (convenience field)"
    )

    node_summary: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Quick status: {total, completed, failed, current_node} (JSONB)"
    )
```

#### 15.3.4 New Fields: Tier 3 (NICE TO HAVE)

```python
class GeospatialAsset(BaseModel):
    # ... existing fields ...

    # =========================================================================
    # DAG ORCHESTRATION FIELDS - TIER 3: NICE TO HAVE
    # =========================================================================

    priority: int = Field(
        default=5,
        ge=1,
        le=10,
        description="Processing priority (1=highest, 10=lowest)"
    )

    estimated_completion_at: Optional[datetime] = Field(
        default=None,
        description="ETA based on workflow progress"
    )

    source_file_hash: Optional[str] = Field(
        default=None,
        max_length=64,
        description="SHA256 of input file (change detection)"
    )

    output_file_hash: Optional[str] = Field(
        default=None,
        max_length=64,
        description="SHA256 of output file (integrity verification)"
    )
```

### 15.4 Four State Dimensions

With DAG enhancements, GeospatialAsset has four orthogonal state dimensions:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    FOUR STATE DIMENSIONS                                     │
└─────────────────────────────────────────────────────────────────────────────┘

1. PROCESSING STATE (NEW - DAG-specific)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   pending ──► processing ──► completed
                         ╲
                          ╲──► failed ──► pending (retry)

   Question: "Has the workflow finished?"


2. REVISION STATE (existing)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   revision: 1 → 2 → 3 (increments on overwrite)
   current_job_id: points to active job

   Question: "Which version of the data?"


3. APPROVAL STATE (existing)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   pending_review ──► approved
                  ╲
                   ╲──► rejected ──► (overwrite) ──► pending_review

   Question: "Has a human reviewed it?"


4. CLEARANCE STATE (existing)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   uncleared ──► ouo
            ╲
             ╲──► public (triggers ADF export)

   Question: "What's the security classification?"
```

**Why separate processing_status from approval_state?**

| Scenario | processing_status | approval_state |
|----------|-------------------|----------------|
| Job running | `processing` | `pending_review` |
| Job completed, awaiting review | `completed` | `pending_review` |
| Job completed, approved | `completed` | `approved` |
| Job failed | `failed` | `pending_review` |
| Retry submitted | `pending` | `pending_review` |

### 15.5 DDL Changes

```sql
-- New enum for processing status
CREATE TYPE app.processing_status AS ENUM (
    'pending',
    'processing',
    'completed',
    'failed'
);

-- Add DAG columns to geospatial_assets
ALTER TABLE app.geospatial_assets ADD COLUMN
    workflow_id VARCHAR(64);

ALTER TABLE app.geospatial_assets ADD COLUMN
    workflow_version INTEGER;

ALTER TABLE app.geospatial_assets ADD COLUMN
    job_count INTEGER NOT NULL DEFAULT 0;

ALTER TABLE app.geospatial_assets ADD COLUMN
    last_request_id VARCHAR(64);

ALTER TABLE app.geospatial_assets ADD COLUMN
    processing_status app.processing_status NOT NULL DEFAULT 'pending';

ALTER TABLE app.geospatial_assets ADD COLUMN
    processing_started_at TIMESTAMP;

ALTER TABLE app.geospatial_assets ADD COLUMN
    processing_completed_at TIMESTAMP;

ALTER TABLE app.geospatial_assets ADD COLUMN
    last_error TEXT;

ALTER TABLE app.geospatial_assets ADD COLUMN
    node_summary JSONB;

ALTER TABLE app.geospatial_assets ADD COLUMN
    priority INTEGER NOT NULL DEFAULT 5;

-- Indexes for DAG queries
CREATE INDEX idx_assets_processing_status
    ON app.geospatial_assets(processing_status);

CREATE INDEX idx_assets_workflow
    ON app.geospatial_assets(workflow_id);

CREATE INDEX idx_assets_processing_started
    ON app.geospatial_assets(processing_started_at DESC);

-- Partial index for stuck processing
CREATE INDEX idx_assets_stuck_processing
    ON app.geospatial_assets(processing_started_at)
    WHERE processing_status = 'processing';

-- Partial index for failed needing attention
CREATE INDEX idx_assets_failed
    ON app.geospatial_assets(updated_at DESC)
    WHERE processing_status = 'failed' AND deleted_at IS NULL;
```

### 15.6 Query Benefits

With DAG fields, common monitoring queries become simple:

```sql
-- Assets stuck in processing > 1 hour (SLA violation)
SELECT asset_id, workflow_id, processing_started_at
FROM app.geospatial_assets
WHERE processing_status = 'processing'
  AND processing_started_at < NOW() - INTERVAL '1 hour';

-- Assets that failed and need attention
SELECT asset_id, workflow_id, last_error, updated_at
FROM app.geospatial_assets
WHERE processing_status = 'failed'
  AND deleted_at IS NULL
ORDER BY updated_at DESC;

-- Processing SLA report by workflow
SELECT
    workflow_id,
    COUNT(*) as total,
    AVG(EXTRACT(EPOCH FROM (processing_completed_at - processing_started_at))) as avg_seconds,
    PERCENTILE_CONT(0.95) WITHIN GROUP (
        ORDER BY EXTRACT(EPOCH FROM (processing_completed_at - processing_started_at))
    ) as p95_seconds
FROM app.geospatial_assets
WHERE processing_status = 'completed'
  AND processing_completed_at > NOW() - INTERVAL '7 days'
GROUP BY workflow_id;

-- Assets with multiple retries (reliability issues)
SELECT asset_id, workflow_id, job_count, last_error
FROM app.geospatial_assets
WHERE job_count > 1
ORDER BY job_count DESC;

-- Current processing load by workflow
SELECT
    workflow_id,
    COUNT(*) as active_jobs
FROM app.geospatial_assets
WHERE processing_status = 'processing'
GROUP BY workflow_id;

-- Quick status check without joins
SELECT
    asset_id,
    processing_status,
    node_summary->>'current_node' as current_node,
    node_summary->>'completed' as nodes_completed,
    node_summary->>'total' as nodes_total
FROM app.geospatial_assets
WHERE current_job_id = 'job-abc123';
```

### 15.7 Epoch 4 Compatibility

**These are NON-BREAKING changes.** Epoch 4 can continue to function without using the new fields:

| Field | Default | Epoch 4 Behavior |
|-------|---------|------------------|
| `workflow_id` | NULL | Not set, ignored |
| `workflow_version` | NULL | Not set, ignored |
| `job_count` | 0 | Not updated (stays 0) |
| `last_request_id` | NULL | Not set, ignored |
| `processing_status` | 'pending' | Not updated (stays pending) |
| `processing_started_at` | NULL | Not set, ignored |
| `processing_completed_at` | NULL | Not set, ignored |
| `last_error` | NULL | Not set, ignored |
| `node_summary` | NULL | Not set, ignored |
| `priority` | 5 | Default priority |

**Epoch 4 jobs continue to work** - they just don't populate these fields. When DAG orchestrator takes over, it will:
1. Set `processing_status = 'processing'` when job starts
2. Update `processing_started_at`
3. Increment `job_count`
4. Set `workflow_id` and `workflow_version`
5. Update `node_summary` as workflow progresses
6. Set `processing_status = 'completed'` or `'failed'` when done
7. Update `processing_completed_at` or `last_error`

### 15.8 Updated SQL Index List

With DAG fields, the recommended indexes for `geospatial_assets`:

```python
__sql_indexes: ClassVar[List[Dict[str, Any]]] = [
    # Existing indexes
    {"columns": ["dataset_id", "resource_id", "version_id"], "name": "idx_assets_identity"},
    {"columns": ["stac_item_id"], "name": "idx_assets_stac_item"},
    {"columns": ["approval_state"], "name": "idx_assets_approval"},
    {"columns": ["clearance_state"], "name": "idx_assets_clearance"},
    {"columns": ["current_job_id"], "name": "idx_assets_current_job"},
    {"columns": ["created_at"], "name": "idx_assets_created", "descending": True},
    {"columns": ["approval_state"], "name": "idx_assets_pending",
     "partial_where": "approval_state = 'pending_review' AND deleted_at IS NULL"},
    {"columns": ["deleted_at"], "name": "idx_assets_active",
     "partial_where": "deleted_at IS NULL"},

    # NEW: DAG indexes
    {"columns": ["processing_status"], "name": "idx_assets_processing_status"},
    {"columns": ["workflow_id"], "name": "idx_assets_workflow"},
    {"columns": ["processing_started_at"], "name": "idx_assets_processing_started",
     "descending": True},
    {"columns": ["processing_started_at"], "name": "idx_assets_stuck_processing",
     "partial_where": "processing_status = 'processing'"},
    {"columns": ["updated_at"], "name": "idx_assets_failed",
     "partial_where": "processing_status = 'failed' AND deleted_at IS NULL",
     "descending": True},
    {"columns": ["priority", "created_at"], "name": "idx_assets_priority_queue",
     "partial_where": "processing_status = 'pending' AND deleted_at IS NULL"},
]
```

### 15.9 Implementation Tasks

| Task | Priority | Status | Notes |
|------|----------|--------|-------|
| Add `ProcessingStatus` enum | HIGH | ✅ DONE | `core/models/asset.py` - 29 JAN 2026 |
| Add Tier 1 fields to model | HIGH | ✅ DONE | workflow_id, workflow_version, job_count, last_request_id |
| Add Tier 2 fields to model | HIGH | ✅ DONE | processing_status, timestamps, last_error, node_summary |
| Add Tier 3 fields to model | LOW | ✅ DONE | priority, estimated_completion_at, hashes |
| Update sql_generator.py | HIGH | ✅ DONE | Added processing_status enum |
| Update AssetRepository | MEDIUM | ✅ DONE | 7 processing status methods added |
| Update AssetService | MEDIUM | ✅ DONE | 7 processing lifecycle methods added |
| Deploy DDL changes | HIGH | ⏳ TODO | Run `action=rebuild&confirm=yes` |

### 15.10 Future: rmhdagmaster Integration

When rmhdagmaster DAG orchestrator goes live, it will:

1. **On job creation**:
   ```python
   asset.processing_status = ProcessingStatus.PROCESSING
   asset.processing_started_at = datetime.utcnow()
   asset.job_count += 1
   asset.workflow_id = workflow_id
   asset.workflow_version = workflow_version
   asset.current_job_id = job_id
   asset.last_request_id = request_id
   ```

2. **On node completion** (periodic updates):
   ```python
   asset.node_summary = {
       "total": 5,
       "completed": 3,
       "failed": 0,
       "current_node": "process_cog"
   }
   ```

3. **On job success**:
   ```python
   asset.processing_status = ProcessingStatus.COMPLETED
   asset.processing_completed_at = datetime.utcnow()
   asset.stac_item_id = result.stac_item_id
   asset.blob_path = result.blob_path  # or table_name for vectors
   ```

4. **On job failure**:
   ```python
   asset.processing_status = ProcessingStatus.FAILED
   asset.last_error = error_message[:2000]
   ```

5. **On retry**:
   ```python
   asset.processing_status = ProcessingStatus.PENDING
   asset.last_error = None  # Clear for fresh start
   # job_count already incremented on new job creation
   ```

---

*End of V0.8 Entity Architecture Document*
