# V0.8 PLAN: Docker Worker Consolidation & Vector ETL

**Version**: 0.8.0
**Date**: 23 JAN 2026
**Status**: PLANNING
**Epic**: E7 - Platform Maturity

---

## Executive Summary

Consolidate task queues (`raster-tasks` + `vector-tasks` → `functionapp-tasks`), establish Docker worker as PRIMARY execution environment for all heavy operations, and implement Docker-based vector ETL pipeline.

### V0.8 Priorities

1. **Queue Consolidation**: Merge raster/vector queues into `functionapp-tasks`
2. **Docker Vector ETL**: Implement vector pipeline for Docker worker
3. **App Mode Simplification**: 5 clean modes for 3 deployment configurations
4. **Admin Override**: Force-functionapp option for debugging

---

## 1. DEPLOYMENT CONFIGURATIONS

### Configuration 1: Standalone (Development Only)

Single Function App handles everything. For local development and testing.

```
┌─────────────────────────────────────────────────────────────────┐
│                     STANDALONE (DEV)                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ Single Function App                                      │    │
│  │ APP_MODE=standalone                                      │    │
│  │                                                          │    │
│  │ Listens:                                                 │    │
│  │   - geospatial-jobs                                      │    │
│  │   - container-tasks (if DOCKER_WORKER_ENABLED=false)     │    │
│  │   - functionapp-tasks                                    │    │
│  │                                                          │    │
│  │ Serves:                                                  │    │
│  │   - ALL HTTP endpoints                                   │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

**Environment Variables**:
```bash
APP_MODE=standalone
DOCKER_WORKER_ENABLED=false  # Process container-tasks locally
```

---

### Configuration 2: This Environment (Alternate)

Platform + Orchestrator combined, separate FunctionApp Worker, Docker Worker.

```
┌─────────────────────────────────────────────────────────────────┐
│                   ALTERNATE (THIS ENVIRONMENT)                   │
│                      Corporate ASE (4 instances)                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Function App 1: Platform+Orchestrator │  Instance 4          │
│  │ APP_MODE=orchestrator                 │  (2 CPU, 8GB)        │
│  │                                       │                      │
│  │ Listens: geospatial-jobs              │                      │
│  │ Serves:  ALL HTTP endpoints           │                      │
│  │          (/platform/*, /jobs/*,       │                      │
│  │           /admin/*, /dbadmin/*)       │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Function App 2: Task Worker           │  Instances 1-2       │
│  │ APP_MODE=worker_functionapp           │  (2 CPU, 4GB each)   │
│  │                                       │                      │
│  │ Listens: functionapp-tasks            │                      │
│  │ Serves:  /admin/* only                │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Docker Worker (Container)             │  Instance 3          │
│  │ APP_MODE=worker_docker                │  (2 CPU, 8GB full)   │
│  │                                       │                      │
│  │ Listens: container-tasks              │                      │
│  │ Serves:  /health, /readyz only        │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

**Environment Variables**:

Function App 1 (Platform+Orchestrator):
```bash
APP_MODE=orchestrator
DOCKER_WORKER_ENABLED=true
DOCKER_WORKER_URL=https://rmhheavyapi-....azurewebsites.net
```

Function App 2 (Task Worker):
```bash
APP_MODE=worker_functionapp
```

Docker Worker:
```bash
APP_MODE=worker_docker
```

---

### Configuration 3: Production (Full Separation)

Maximum isolation. Platform has auth enabled, all apps independent.

```
┌─────────────────────────────────────────────────────────────────┐
│                      PRODUCTION (FULL SEPARATION)                │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Function App 1: Platform (Gateway)    │                      │
│  │ APP_MODE=platform                     │                      │
│  │                                       │                      │
│  │ Listens: NOTHING (send-only)          │                      │
│  │ Serves:  /api/platform/* only         │                      │
│  │ Auth:    ENABLED + MANAGED            │                      │
│  │                                       │                      │
│  │ External apps call this to request    │                      │
│  │ ETL operations                        │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Function App 2: Orchestrator          │                      │
│  │ APP_MODE=orchestrator                 │                      │
│  │                                       │                      │
│  │ Listens: geospatial-jobs              │                      │
│  │ Serves:  /api/jobs/*, /api/admin/*,   │                      │
│  │          /api/dbadmin/*               │                      │
│  │ Auth:    Internal network only        │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Function App 3: Task Worker           │                      │
│  │ APP_MODE=worker_functionapp           │                      │
│  │                                       │                      │
│  │ Listens: functionapp-tasks            │                      │
│  │ Serves:  /api/admin/* only            │                      │
│  │ Auth:    Internal network only        │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
│  ┌───────────────────────────────────────┐                      │
│  │ Docker Worker (Container)             │                      │
│  │ APP_MODE=worker_docker                │                      │
│  │                                       │                      │
│  │ Listens: container-tasks              │                      │
│  │ Serves:  /health, /readyz only        │                      │
│  └───────────────────────────────────────┘                      │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. QUEUE STRUCTURE

### Final Queue Names

| Queue | Primary Listener | Purpose |
|-------|------------------|---------|
| `geospatial-jobs` | Orchestrator | Job coordination, stage advancement |
| `container-tasks` | Docker Worker | ALL heavy operations (GDAL, geopandas, bulk SQL) |
| `functionapp-tasks` | FunctionApp Worker | Lightweight DB + backup heavy ops |

### Queue Migration

| Old Queue | New Queue | Action |
|-----------|-----------|--------|
| `geospatial-jobs` | `geospatial-jobs` | Keep unchanged |
| `long-running-tasks` | `container-tasks` | Rename |
| `raster-tasks` | `functionapp-tasks` | Merge |
| `vector-tasks` | `functionapp-tasks` | Merge |

### Message Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                        MESSAGE FLOW                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  External App                                                    │
│       │                                                          │
│       ▼                                                          │
│  ┌─────────────┐     ┌──────────────────┐                       │
│  │  Platform   │────▶│ geospatial-jobs  │                       │
│  │  (Gateway)  │     │     Queue        │                       │
│  └─────────────┘     └────────┬─────────┘                       │
│                               │                                  │
│                               ▼                                  │
│                      ┌────────────────┐                         │
│                      │  Orchestrator  │                         │
│                      │  (Job Router)  │                         │
│                      └───────┬────────┘                         │
│                              │                                   │
│              ┌───────────────┼───────────────┐                  │
│              │               │               │                   │
│              ▼               ▼               ▼                   │
│     ┌────────────────┐ ┌──────────────┐                         │
│     │ container-tasks│ │functionapp-  │                         │
│     │     Queue      │ │tasks Queue   │                         │
│     └───────┬────────┘ └──────┬───────┘                         │
│             │                 │                                  │
│             ▼                 ▼                                  │
│     ┌────────────────┐ ┌──────────────┐                         │
│     │ Docker Worker  │ │ FunctionApp  │                         │
│     │ (Heavy Work)   │ │   Worker     │                         │
│     └────────────────┘ └──────────────┘                         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## 3. APP MODES

### Mode Definitions

```python
class AppMode(str, Enum):
    """Application deployment modes."""

    STANDALONE = "standalone"              # All queues, all HTTP (dev only)
    PLATFORM = "platform"                  # HTTP gateway only, send to jobs
    ORCHESTRATOR = "orchestrator"          # Jobs queue + all HTTP (can combine with platform)
    WORKER_FUNCTIONAPP = "worker_functionapp"  # functionapp-tasks queue
    WORKER_DOCKER = "worker_docker"        # container-tasks queue
```

### Mode Behavior Matrix

| Mode | Jobs Queue | Container Queue | FunctionApp Queue | Platform HTTP | Admin HTTP |
|------|------------|-----------------|-------------------|---------------|------------|
| `standalone` | ✅ Listen | ✅ Listen* | ✅ Listen | ✅ | ✅ |
| `platform` | Send only | ❌ | ❌ | ✅ | ❌ |
| `orchestrator` | ✅ Listen | ❌ | ❌ | ✅** | ✅ |
| `worker_functionapp` | ❌ | ❌ | ✅ Listen | ❌ | ✅ |
| `worker_docker` | ❌ | ✅ Listen | ❌ | ❌ | Health only |

*standalone listens to container-tasks only if `DOCKER_WORKER_ENABLED=false`
**orchestrator includes platform endpoints for combined deployment

### HTTP Endpoint Exposure

| Endpoint Group | standalone | platform | orchestrator | worker_functionapp | worker_docker |
|----------------|------------|----------|--------------|-------------------|---------------|
| `/api/platform/*` | ✅ | ✅ | ✅ | ❌ | ❌ |
| `/api/jobs/*` | ✅ | ❌ | ✅ | ❌ | ❌ |
| `/api/admin/*` | ✅ | ❌ | ✅ | ✅ | ❌ |
| `/api/dbadmin/*` | ✅ | ❌ | ✅ | ✅ | ❌ |
| `/api/health` | ✅ | ✅ | ✅ | ✅ | ✅ |
| `/health`, `/readyz` | ✅ | ✅ | ✅ | ✅ | ✅ |

---

## 4. TASK ROUTING

### Routing Categories

#### DOCKER_TASKS → `container-tasks` Queue

All GDAL, geopandas, and heavy pgstac SQL operations:

```python
DOCKER_TASKS = frozenset([
    # =========================================================================
    # CONSOLIDATED RASTER HANDLERS (existing)
    # =========================================================================
    "raster_process_complete",        # F7.13: Validate → COG → STAC
    "raster_process_large_complete",  # F7.18: Tiling pipeline
    "h3_pyramid_complete",            # F7.20: H3 pyramid (post-V0.8)

    # =========================================================================
    # RASTER OPERATIONS (GDAL-dependent)
    # =========================================================================
    "raster_validate",
    "raster_create_cog",
    "raster_extract_stac_metadata",
    "raster_list_files",
    "raster_generate_tiling_scheme",
    "raster_extract_tiles",
    "raster_create_mosaicjson",
    "raster_create_stac_collection",

    # =========================================================================
    # FATHOM RASTER OPERATIONS (GDAL)
    # =========================================================================
    "fathom_band_stack",              # Stack 8 return periods
    "fathom_spatial_merge",           # Merge tiles band-by-band

    # =========================================================================
    # H3 RASTER AGGREGATION (rasterstats - memory intensive)
    # =========================================================================
    "h3_raster_zonal_stats",

    # =========================================================================
    # VECTOR ETL - DOCKER (V0.8 NEW - geopandas + pgstac SQL)
    # =========================================================================
    "vector_docker_prepare",          # NEW: Geopandas read/transform
    "vector_docker_upload",           # NEW: Bulk pgstac insert
    "vector_docker_stac",             # NEW: STAC item creation
])
```

#### FUNCTIONAPP_TASKS → `functionapp-tasks` Queue

Lightweight DB operations, inventory, STAC queries:

```python
FUNCTIONAPP_TASKS = frozenset([
    # =========================================================================
    # LEGACY VECTOR ETL (FunctionApp - backup/admin only after V0.8)
    # =========================================================================
    "process_vector_prepare",
    "process_vector_upload",
    "vector_create_stac",
    "vector_extract_stac_metadata",

    # =========================================================================
    # INVENTORY OPERATIONS (blob listing, lightweight)
    # =========================================================================
    "inventory_container_summary",
    "inventory_list_blobs",
    "inventory_analyze_blob",
    "inventory_aggregate_analysis",
    "inventory_classify_geospatial",
    "inventory_aggregate_geospatial",

    # =========================================================================
    # FATHOM INVENTORY (DB queries, not raster)
    # =========================================================================
    "fathom_generate_scan_prefixes",
    "fathom_scan_prefix",
    "fathom_assign_grid_cells",
    "fathom_inventory_summary",
    "fathom_tile_inventory",
    "fathom_grid_inventory",
    "fathom_stac_register",
    "fathom_stac_rebuild",

    # =========================================================================
    # H3 POSTGIS OPERATIONS (DB-bound, not memory intensive)
    # =========================================================================
    "h3_create_stac",
    "h3_native_streaming_postgis",
    "h3_generate_grid",
    "h3_cascade_descendants",
    "h3_finalize_pyramid",
    "h3_inventory_cells",
    "h3_aggregation_finalize",

    # =========================================================================
    # STAC OPERATIONS (pgSTAC queries, lightweight)
    # =========================================================================
    "stac_repair_inventory",
    "stac_repair_item",
    "stac_rebuild_validate",
    "stac_rebuild_item",

    # =========================================================================
    # UNPUBLISH OPERATIONS (STAC queries, blob deletes)
    # =========================================================================
    "unpublish_inventory_raster",
    "unpublish_inventory_vector",
    "unpublish_delete_blob",
    "unpublish_drop_table",
    "unpublish_delete_stac",

    # =========================================================================
    # CURATED DATASET UPDATES (HTTP, DB, lightweight)
    # =========================================================================
    "curated_check_source",
    "curated_fetch_data",
    "curated_etl_process",
    "curated_finalize",

    # =========================================================================
    # H3 EXPORT (DB-bound)
    # =========================================================================
    "h3_export_validate",
    "h3_export_build",
    "h3_export_register",

    # =========================================================================
    # INGEST COLLECTION (blob copy, pgSTAC)
    # =========================================================================
    "ingest_inventory",
    "ingest_copy_batch",
    "ingest_register_collection",
    "ingest_register_items",
    "ingest_finalize",

    # =========================================================================
    # ORPHAN BLOB OPERATIONS (blob listing, DB queries)
    # =========================================================================
    "orphan_blob_inventory",
    "silver_blob_validate",
    "silver_blob_register",

    # =========================================================================
    # TEST HANDLERS
    # =========================================================================
    "hello_world_greeting",
    "hello_world_reply",
])
```

### Routing Logic

```python
def _get_queue_for_task(self, task_type: str, force_functionapp: bool = False) -> str:
    """
    Route task to appropriate queue.

    Args:
        task_type: The handler task type
        force_functionapp: Admin override to use FunctionApp (debug only)

    Returns:
        Queue name for this task
    """
    # Admin override - force to FunctionApp for debugging
    # NOTE: Remove before production deployment
    if force_functionapp and task_type in DOCKER_TASKS:
        logger.warning(f"Admin override: routing {task_type} to functionapp-tasks")
        return config.queues.functionapp_tasks_queue

    # Normal routing
    if task_type in DOCKER_TASKS:
        return config.queues.container_tasks_queue  # "container-tasks"
    elif task_type in FUNCTIONAPP_TASKS:
        return config.queues.functionapp_tasks_queue  # "functionapp-tasks"
    else:
        raise ContractViolationError(
            f"Task type '{task_type}' not in DOCKER_TASKS or FUNCTIONAPP_TASKS. "
            f"Add to TaskRoutingDefaults in config/defaults.py"
        )
```

---

## 5. DOCKER VECTOR ETL (V0.8 Priority)

### New Job: `vector_docker_etl`

```python
class VectorDockerETLJob(JobBaseMixin, JobBase):
    """
    Docker-based vector ETL pipeline.

    Processes vector data using geopandas for transformations
    and bulk pgstac SQL for database operations.

    Stages:
        1. Prepare: Read source, validate, transform with geopandas
        2. Upload: Bulk insert to PostGIS using COPY
        3. STAC: Create/update STAC item
    """

    job_type = "vector_docker_etl"
    description = "Docker vector ETL with geopandas and bulk SQL"

    stages = [
        {
            "number": 1,
            "name": "prepare",
            "task_type": "vector_docker_prepare",
            "parallelism": "single"
        },
        {
            "number": 2,
            "name": "upload",
            "task_type": "vector_docker_upload",
            "parallelism": "single"
        },
        {
            "number": 3,
            "name": "stac",
            "task_type": "vector_docker_stac",
            "parallelism": "single"
        }
    ]

    parameters_schema = {
        'source_url': {'type': 'str', 'required': True},
        'collection_id': {'type': 'str', 'required': True},
        'table_name': {'type': 'str', 'required': False},
        'transform_options': {'type': 'dict', 'required': False},
    }
```

---

## 6. IMPLEMENTATION STEPS

### Phase 1: Queue Infrastructure
**Effort: 30 minutes | Risk: Low**

Create new Service Bus queues. Old queues remain active during migration.

#### Step 1.1: Create container-tasks queue
```bash
az servicebus queue create \
  --name container-tasks \
  --namespace-name $SB_NAMESPACE \
  --resource-group $RESOURCE_GROUP \
  --max-size 5120 \
  --default-message-time-to-live P14D \
  --lock-duration PT5M
```

#### Step 1.2: Create functionapp-tasks queue
```bash
az servicebus queue create \
  --name functionapp-tasks \
  --namespace-name $SB_NAMESPACE \
  --resource-group $RESOURCE_GROUP \
  --max-size 5120 \
  --default-message-time-to-live P14D \
  --lock-duration PT5M
```

#### Step 1.3: Verify queues created
```bash
az servicebus queue list --namespace-name $SB_NAMESPACE --query "[].name" -o table
```

**Deliverable**: New queues exist alongside old queues

---

### Phase 2: Configuration Updates
**Effort: 2-3 hours | Risk: Medium**

Update routing configuration and queue defaults.

#### Step 2.1: Update `config/defaults.py`

```python
# Changes to make:

# 1. Rename LONG_RUNNING_TASKS → DOCKER_TASKS
# 2. Add all raster tasks to DOCKER_TASKS
# 3. Rename RASTER_TASKS + VECTOR_TASKS → FUNCTIONAPP_TASKS
# 4. Update QueueDefaults class

class QueueDefaults:
    JOBS_QUEUE = "geospatial-jobs"
    CONTAINER_TASKS_QUEUE = "container-tasks"      # NEW (was long-running-tasks)
    FUNCTIONAPP_TASKS_QUEUE = "functionapp-tasks"  # NEW (merged raster + vector)

    # Deprecated - keep for migration period
    LONG_RUNNING_TASKS_QUEUE = "long-running-tasks"  # DEPRECATED
    RASTER_TASKS_QUEUE = "raster-tasks"              # DEPRECATED
    VECTOR_TASKS_QUEUE = "vector-tasks"              # DEPRECATED
```

#### Step 2.2: Update `config/queue_config.py`

```python
# Add new queue properties:

@property
def container_tasks_queue(self) -> str:
    """Queue for Docker container tasks (heavy operations)."""
    return os.environ.get(
        "SERVICE_BUS_CONTAINER_TASKS_QUEUE",
        QueueDefaults.CONTAINER_TASKS_QUEUE
    )

@property
def functionapp_tasks_queue(self) -> str:
    """Queue for FunctionApp tasks (lightweight operations)."""
    return os.environ.get(
        "SERVICE_BUS_FUNCTIONAPP_TASKS_QUEUE",
        QueueDefaults.FUNCTIONAPP_TASKS_QUEUE
    )
```

#### Step 2.3: Update `core/machine.py` routing

```python
# Update _get_queue_for_task method:

def _get_queue_for_task(self, task_type: str) -> str:
    """Route task to appropriate queue."""
    # Check for admin override
    force_functionapp = self._current_job_params.get('_force_functionapp', False)

    if force_functionapp and task_type in TaskRoutingDefaults.DOCKER_TASKS:
        logger.warning(f"Admin override: {task_type} → functionapp-tasks")
        return self.config.queues.functionapp_tasks_queue

    if task_type in TaskRoutingDefaults.DOCKER_TASKS:
        return self.config.queues.container_tasks_queue
    elif task_type in TaskRoutingDefaults.FUNCTIONAPP_TASKS:
        return self.config.queues.functionapp_tasks_queue
    else:
        raise ContractViolationError(
            f"Task type '{task_type}' not routed. "
            f"Add to DOCKER_TASKS or FUNCTIONAPP_TASKS in config/defaults.py"
        )
```

**Deliverable**: Routing logic uses new queue names

---

### Phase 3: App Mode Simplification
**Effort: 2-3 hours | Risk: Medium**

Simplify app modes from 8 to 5.

#### Step 3.1: Update `config/app_mode_config.py`

```python
# Remove these modes:
# - PLATFORM_ONLY (use ORCHESTRATOR instead)
# - PLATFORM_RASTER (no longer needed)
# - PLATFORM_VECTOR (no longer needed)
# - WORKER_RASTER (merged into WORKER_FUNCTIONAPP)
# - WORKER_VECTOR (merged into WORKER_FUNCTIONAPP)

class AppMode(str, Enum):
    STANDALONE = "standalone"
    PLATFORM = "platform"
    ORCHESTRATOR = "orchestrator"
    WORKER_FUNCTIONAPP = "worker_functionapp"
    WORKER_DOCKER = "worker_docker"
```

#### Step 3.2: Update queue listening properties

```python
@property
def listens_to_functionapp_tasks(self) -> bool:
    """Whether this mode processes functionapp-tasks queue."""
    return self.mode in [
        AppMode.STANDALONE,
        AppMode.WORKER_FUNCTIONAPP,
    ]

@property
def listens_to_container_tasks(self) -> bool:
    """Whether this mode processes container-tasks queue."""
    if self.mode == AppMode.WORKER_DOCKER:
        return True
    if self.mode == AppMode.STANDALONE and not self.docker_worker_enabled:
        return True
    return False
```

#### Step 3.3: Update HTTP endpoint properties

```python
@property
def has_platform_endpoints(self) -> bool:
    """Whether this mode exposes /api/platform/* endpoints."""
    return self.mode in [
        AppMode.STANDALONE,
        AppMode.PLATFORM,
        AppMode.ORCHESTRATOR,  # Combined deployment
    ]

@property
def has_jobs_endpoints(self) -> bool:
    """Whether this mode exposes /api/jobs/* endpoints."""
    return self.mode in [
        AppMode.STANDALONE,
        AppMode.ORCHESTRATOR,
    ]

@property
def has_admin_endpoints(self) -> bool:
    """Whether this mode exposes /api/admin/* endpoints."""
    return self.mode in [
        AppMode.STANDALONE,
        AppMode.ORCHESTRATOR,
        AppMode.WORKER_FUNCTIONAPP,
    ]
```

**Deliverable**: 5 clean app modes

---

### Phase 4: Azure Functions Triggers
**Effort: 1-2 hours | Risk: Medium**

Update Service Bus triggers to use new queues.

#### Step 4.1: Update `function_app.py`

```python
# Remove old triggers:
# - process_raster_task (raster-tasks queue)
# - process_vector_task (vector-tasks queue)

# Add new trigger:
if _app_mode.listens_to_functionapp_tasks:
    @app.service_bus_queue_trigger(
        arg_name="msg",
        queue_name="functionapp-tasks",
        connection="ServiceBusConnection"
    )
    def process_functionapp_task(msg: func.ServiceBusMessage):
        """Process tasks from functionapp-tasks queue."""
        handle_task_message(msg, core_machine, queue_name="functionapp-tasks")

# Note: container-tasks trigger only in standalone mode with DOCKER_WORKER_ENABLED=false
if _app_mode.listens_to_container_tasks:
    @app.service_bus_queue_trigger(
        arg_name="msg",
        queue_name="container-tasks",
        connection="ServiceBusConnection"
    )
    def process_container_task(msg: func.ServiceBusMessage):
        """Process tasks from container-tasks queue (dev only)."""
        handle_task_message(msg, core_machine, queue_name="container-tasks")
```

**Deliverable**: Functions listen to new queues

---

### Phase 5: Docker Worker Update
**Effort: 30 minutes | Risk: Low**

Update Docker worker to listen to `container-tasks` queue.

#### Step 5.1: Update `docker_service.py`

```python
# In BackgroundQueueWorker.__init__:
# Change queue name from "long-running-tasks" to "container-tasks"

class BackgroundQueueWorker:
    def __init__(self, ...):
        self.queue_name = config.queues.container_tasks_queue  # "container-tasks"
```

#### Step 5.2: Update `docker_main.py`

```python
# Same change if this file has hardcoded queue name
queue_name = config.queues.container_tasks_queue
```

**Deliverable**: Docker worker listens to `container-tasks`

---

### Phase 6: Docker Vector ETL Implementation
**Effort: 3-5 hours | Risk: Medium**

Implement the new vector ETL pipeline for Docker.

#### Step 6.1: Create `jobs/vector_docker_etl.py`

```python
# New job definition file
# See Section 5 for full implementation
```

#### Step 6.2: Create `handlers/vector_docker.py`

```python
# New handlers file with:
# - vector_docker_prepare
# - vector_docker_upload
# - vector_docker_stac
```

#### Step 6.3: Register job in `jobs/__init__.py`

```python
from jobs.vector_docker_etl import VectorDockerETLJob

ALL_JOBS = {
    # ... existing jobs ...
    "vector_docker_etl": VectorDockerETLJob,
}
```

#### Step 6.4: Register handlers in `services/__init__.py`

```python
from handlers.vector_docker import (
    vector_docker_prepare,
    vector_docker_upload,
    vector_docker_stac,
)

ALL_HANDLERS = {
    # ... existing handlers ...
    "vector_docker_prepare": vector_docker_prepare,
    "vector_docker_upload": vector_docker_upload,
    "vector_docker_stac": vector_docker_stac,
}
```

#### Step 6.5: Add to `config/defaults.py` DOCKER_TASKS

```python
DOCKER_TASKS = frozenset([
    # ... existing ...
    "vector_docker_prepare",
    "vector_docker_upload",
    "vector_docker_stac",
])
```

**Deliverable**: Docker vector ETL functional

---

### Phase 7: Admin Override Implementation
**Effort: 1-2 hours | Risk: Low**

Add force-functionapp parameter for debugging.

#### Step 7.1: Update admin job submission endpoint

```python
# In triggers/http/admin.py or equivalent:

def submit_job_admin(req: func.HttpRequest) -> func.HttpResponse:
    """Admin job submission with override options."""
    # Parse override parameter
    force_functionapp = req.params.get("force_functionapp", "").lower() == "true"

    if force_functionapp:
        logger.warning(f"Admin override: force_functionapp=true for job submission")

    # Add to job params
    job_params = req.get_json()
    job_params["_force_functionapp"] = force_functionapp

    # Submit job
    # ...
```

#### Step 7.2: Ensure NOT available on /platform endpoints

```python
# In triggers/http/platform.py:
# Do NOT add force_functionapp parameter
# This ensures external apps cannot use the override
```

**Deliverable**: Admin can override routing for debugging

---

### Phase 8: Testing & Validation
**Effort: 2-3 hours | Risk: Low**

Comprehensive testing before deployment.

#### Step 8.1: Local testing

```bash
# Run with standalone mode
APP_MODE=standalone DOCKER_WORKER_ENABLED=false python -m pytest tests/

# Test routing logic
python -c "from config.defaults import TaskRoutingDefaults; print(TaskRoutingDefaults.DOCKER_TASKS)"
```

#### Step 8.2: Integration tests

```bash
# Test hello_world routes to functionapp-tasks
curl -X POST .../api/jobs/submit/hello_world -d '{"message": "test"}'

# Test raster_etl routes to container-tasks
curl -X POST .../api/jobs/submit/raster_etl -d '{"source_url": "..."}'

# Test admin override
curl -X POST ".../api/admin/jobs/submit/raster_etl?force_functionapp=true" -d '{"source_url": "..."}'
```

**Deliverable**: All tests pass

---

### Phase 9: Deployment
**Effort: 1-2 hours | Risk: Medium**

Deploy to production environment.

#### Step 9.1: Deploy Function App updates

```bash
func azure functionapp publish rmhazuregeoapi --python --build remote
```

#### Step 9.2: Update Docker worker

```bash
# Build and push new container image
az acr build --registry rmhazureacr --image geospatial-worker:0.8.0 --file Dockerfile .

# Update container
az webapp config container set --name rmhheavyapi --resource-group rmhazure_rg \
  --docker-custom-image-name "rmhazureacr.azurecr.io/geospatial-worker:0.8.0"

# Restart
az webapp stop --name rmhheavyapi --resource-group rmhazure_rg && \
az webapp start --name rmhheavyapi --resource-group rmhazure_rg
```

#### Step 9.3: Post-deployment validation

```bash
# Health check
curl https://rmhazuregeoapi-....azurewebsites.net/api/health

# Sync schema
curl -X POST ".../api/dbadmin/maintenance?action=ensure&confirm=yes"

# Test job submission
curl -X POST .../api/jobs/submit/hello_world -d '{"message": "v0.8 test"}'
```

**Deliverable**: V0.8.0 deployed and validated

---

### Phase 10: Cleanup (After 1 Week Stabilization)
**Effort: 30 minutes | Risk: Low**

Remove old queues after confirming stability.

#### Step 10.1: Verify old queues are empty

```bash
az servicebus queue show --name raster-tasks --namespace-name $SB_NAMESPACE --query messageCount
az servicebus queue show --name vector-tasks --namespace-name $SB_NAMESPACE --query messageCount
az servicebus queue show --name long-running-tasks --namespace-name $SB_NAMESPACE --query messageCount
```

#### Step 10.2: Delete old queues

```bash
az servicebus queue delete --name raster-tasks --namespace-name $SB_NAMESPACE
az servicebus queue delete --name vector-tasks --namespace-name $SB_NAMESPACE
az servicebus queue delete --name long-running-tasks --namespace-name $SB_NAMESPACE
```

#### Step 10.3: Remove deprecated code

```python
# Remove from config/defaults.py:
# - LONG_RUNNING_TASKS_QUEUE
# - RASTER_TASKS_QUEUE
# - VECTOR_TASKS_QUEUE
# - LONG_RUNNING_TASKS list
# - RASTER_TASKS list
# - VECTOR_TASKS list
```

**Deliverable**: Clean codebase with no deprecated references

---

## 7. FILE CHANGE SUMMARY

### New Files

| File | Purpose |
|------|---------|
| `handlers/vector_docker.py` | Docker vector ETL handlers |
| `jobs/vector_docker_etl.py` | Docker vector ETL job definition |

### Modified Files

| File | Phase | Changes |
|------|-------|---------|
| `config/defaults.py` | 2 | DOCKER_TASKS, FUNCTIONAPP_TASKS, queue defaults |
| `config/queue_config.py` | 2 | New queue properties |
| `config/app_mode_config.py` | 3 | Simplified 5 modes |
| `core/machine.py` | 2 | Updated routing, force_functionapp |
| `function_app.py` | 4 | New triggers, removed old |
| `docker_service.py` | 5 | Queue name update |
| `docker_main.py` | 5 | Queue name update |
| `jobs/__init__.py` | 6 | Register vector_docker_etl |
| `services/__init__.py` | 6 | Register vector_docker handlers |
| `triggers/http/admin.py` | 7 | force_functionapp parameter |

### Documentation Updates

| File | Changes |
|------|---------|
| `CLAUDE.md` | Deployment configs, queue names |
| `docs_claude/DOCKER_INTEGRATION.md` | Mandatory status, architecture |
| `docs_claude/ARCHITECTURE_DIAGRAMS.md` | Updated diagrams |

---

## 8. ENVIRONMENT VARIABLES

### New Variables

```bash
# Queue configuration
SERVICE_BUS_CONTAINER_TASKS_QUEUE=container-tasks
SERVICE_BUS_FUNCTIONAPP_TASKS_QUEUE=functionapp-tasks

# App modes
APP_MODE=standalone           # Dev: all in one
APP_MODE=platform             # Prod: gateway only
APP_MODE=orchestrator         # Prod: jobs queue + HTTP
APP_MODE=worker_functionapp   # Prod: functionapp-tasks queue
APP_MODE=worker_docker        # All: container-tasks queue
```

### Deprecated Variables (Remove after Phase 10)

```bash
SERVICE_BUS_RASTER_TASKS_QUEUE
SERVICE_BUS_VECTOR_TASKS_QUEUE
SERVICE_BUS_LONG_RUNNING_TASKS_QUEUE
```

---

## 9. TESTING CHECKLIST

### Pre-Migration
- [ ] All existing jobs complete successfully
- [ ] Docker worker health check passes
- [ ] Current queue depths are zero

### Phase 1-2 (Queues + Routing)
- [ ] New queues created in Service Bus
- [ ] Routing config compiles without errors
- [ ] Unit tests pass for routing logic

### Phase 3-5 (App Modes + Triggers)
- [ ] App mode config validates for all 5 modes
- [ ] Function triggers register for correct queues
- [ ] Docker worker connects to `container-tasks`

### Phase 6 (Docker Vector ETL)
- [ ] `vector_docker_etl` job submits successfully
- [ ] Stage 1 (prepare) completes in Docker
- [ ] Stage 2 (upload) inserts to PostGIS
- [ ] Stage 3 (stac) creates STAC item
- [ ] End-to-end test with real vector file

### Phase 7 (Admin Override) ✅ COMPLETE (24 JAN 2026)
- [x] `?force_functionapp=true` routes to FunctionApp
- [x] Override only works on admin endpoints (not /platform)
- [x] Override logged with warning

### Integration Tests
- [ ] `hello_world` job → functionapp-tasks ✓
- [ ] `raster_etl` job → container-tasks ✓
- [ ] `vector_docker_etl` job → container-tasks ✓
- [ ] `fathom_etl` job → mixed routing ✓
- [ ] Stage completion signaling works
- [ ] Checkpoint/resume works in Docker

---

## 10. ROLLBACK PLAN

### If Issues Arise

1. **Queue Level**: Old queues exist during migration
   - Revert `TaskRoutingDefaults` to use old names
   - Redeploy previous version

2. **App Mode Level**:
   - Use `APP_MODE=standalone` as fallback

3. **Docker Vector ETL**:
   - New job, doesn't affect existing workflows
   - Can disable without impacting other jobs

### Recovery Commands

```bash
# Revert to previous deployment
func azure functionapp publish rmhazuregeoapi --python --build remote --slot previous

# Check queue depths
az servicebus queue show --name container-tasks --namespace-name $SB_NAMESPACE --query messageCount

# Force drain a queue (move to DLQ)
# Use Service Bus Explorer in Azure Portal
```

---

## 11. SUCCESS CRITERIA

1. **Queue consolidation complete**: 3 active queues (jobs, container, functionapp)
2. **Docker Vector ETL functional**: End-to-end pipeline works
3. **App modes simplified**: 5 clean modes
4. **Admin override works**: Can force FunctionApp for debugging
5. **No job failures**: All existing job types complete
6. **Recovery works**: Queued messages wait for Docker recovery

---

## 12. POST-V0.8 ROADMAP

### V0.8.1: H3 Docker Pipeline

Move H3 operations to Docker:
- `h3_native_streaming_postgis` → Docker (geopandas)
- `h3_generate_grid` → Docker
- `h3_cascade_descendants` → Docker

### V0.8.2: Remove Admin Override

Before production:
- Remove `force_functionapp` parameter
- Audit all admin endpoints
- Lock down FunctionApp worker to internal network

---

## 13. RASTER LOGIC AND FILE MOUNT IMPLEMENTATION

### V0.8 Doctrine

**Assumptions:**
- Docker worker is ALWAYS present with Azure Files mount
- No mount = **functional but degraded state** (not unhealthy, but not optimal)
  - System remains healthy and operational
  - Performance may be reduced (uses container memory instead of mount)
  - Logged as warning, not error
  - Production deployments SHOULD have mount, but system works without it
- ONE raster workflow: `process_raster_docker` running on Docker
- Tiling decision is INTERNAL to the job (not separate job types)

### Architecture

```
Raster Submission (B2B API)
       │
       ▼
  Docker Worker
  └── process_raster_docker
       │
       ▼
  ┌─────────────────────┐
  │ file_size > tiling  │
  │ threshold?          │
  └──────────┬──────────┘
             │
      ┌──────┴──────┐
      ▼             ▼
   > threshold   ≤ threshold
      │             │
      ▼             ▼
   N COG Tiles   Single COG
   (tiled)       (one file)
```

### Configuration Settings (Final State)

| Setting | Default | Purpose |
|---------|---------|---------|
| `RASTER_USE_ETL_MOUNT` | `true` | Expected state (false = degraded) |
| `RASTER_TILING_THRESHOLD_MB` | 2000 | When to switch to tiled output |
| `RASTER_TILE_TARGET_MB` | 200 | Target size per tile |

### Implementation TODO

#### Phase 13.1: Remove Obsolete Config Settings ✅ COMPLETE (24 JAN 2026)
**Effort: 1 hour | Risk: Low**

Removed deprecated routing settings no longer used in V0.8:

- [x] **config/defaults.py**: Removed from `RasterDefaults`:
  - `RASTER_ROUTE_LARGE_MB`
  - `RASTER_ROUTE_DOCKER_MB`
  - `RASTER_ROUTE_REJECT_MB`
  - `ETL_MOUNT_IN_MEMORY_SIZE_LIMIT_MB`

- [x] **config/raster_config.py**: Removed fields

- [x] **config/env_validation.py**: Removed validation rules

- [x] **config/__init__.py**: Updated `debug_config()` output

#### Phase 13.2: Add Tiling Threshold Config ✅ COMPLETE (24 JAN 2026)
**Effort: 30 minutes | Risk: Low**

- [x] **config/defaults.py**: Added `RASTER_TILING_THRESHOLD_MB = 2000`
- [x] **config/raster_config.py**: Added `raster_tiling_threshold_mb` field
- [x] **config/defaults.py**: Changed `USE_ETL_MOUNT = True` (V0.8 expectation)

#### Phase 13.3: Implement Tiling Logic in process_raster_docker ✅ COMPLETE (24 JAN 2026)
**Effort: 2-3 hours | Risk: Medium**

Implemented unified handler that handles both single COG and tiled output:

- [x] **jobs/process_raster_docker.py**:
  - Added `_file_size_mb` parameter passthrough from validator
  - Added tiling parameters (`tile_size`, `overlap`, `band_names`) to schema
  - Updated `finalize_job()` to handle both `single_cog` and `tiled` output modes

- [x] **services/handler_process_raster_complete.py**:
  - Added `_process_raster_tiled()` internal function (adapted from process_large_raster_complete)
  - Handler now checks `_file_size_mb` vs `raster_tiling_threshold_mb` at start
  - Routes to tiled workflow if file exceeds threshold
  - Returns `output_mode: "single_cog"` or `output_mode: "tiled"` in result

Architecture:
```
process_raster_complete (handler)
    │
    ├── file_size <= threshold → Single COG (3 phases)
    │   └── Validation → COG → STAC
    │
    └── file_size > threshold → Tiled (5 phases)
        └── Tiling → Extract → COGs → MosaicJSON → STAC
```

#### Phase 13.4: Deprecate process_large_raster_docker ✅ COMPLETE (24 JAN 2026)
**Effort: 30 minutes | Risk: Low**

- [x] **jobs/process_large_raster_docker.py**:
  - Updated header with DEPRECATED status
  - Added `validate_parameters()` override with deprecation warning
  - Job still works for backward compatibility

- [x] **services/handler_process_large_raster_complete.py**:
  - Updated header with DEPRECATED status
  - Added deprecation warning at handler start
  - Handler still works for backward compatibility

Migration path documented:
- OLD: `job_type="process_large_raster_docker"`
- NEW: `job_type="process_raster_docker"` (same parameters, tiling automatic)

#### Phase 13.5: Update Pre-flight Validation ✅ COMPLETE (24 JAN 2026)
**Effort: 30 minutes | Risk: Low**

- [x] **triggers/trigger_platform_status.py**: Updated `platform_validate()`:
  - All raster uses `process_raster_docker`
  - Returns `output_mode` (single_cog vs tiled) based on file size
  - Returns `estimated_tiles` for tiled mode
  - Warns if mount is disabled (degraded state)

#### Phase 13.6: Startup Validation ✅ COMPLETE (24 JAN 2026)
**Effort: 30 minutes | Risk: Low**

- [x] **docker_service.py**: Updated `validate_etl_mount()`:
  - Logs WARNING if mount not present (NOT failure - degraded state)
  - Sets `degraded: True` in result when mount disabled
  - V0.8 doctrine: mount is EXPECTED, disabled = degraded not unhealthy

#### Phase 13.7: Retrofit Function App process_raster_v2 ✅ COMPLETE (24 JAN 2026)
**Effort: 1 hour | Risk: Low**

Updated Function App raster jobs to remove references to obsolete settings.

- [x] **jobs/process_raster_v2.py**:
  - Removed `max_size_env: 'RASTER_ROUTE_REJECT_MB'` from validators
  - Added deprecation warning in header and `validate_parameters()`
  - Job still works for backward compatibility

- [x] **jobs/process_raster_collection_v2.py**:
  - Marked as ARCHIVED
  - Updated validator to use `RASTER_TILING_THRESHOLD_MB`
  - Added deprecation warning

- [x] **jobs/process_large_raster_v2.py**:
  - Marked as ARCHIVED
  - Added deprecation warning
  - Job still works for backward compatibility

- [x] **infrastructure/validators.py**:
  - Updated `blob_list_exists_with_max_size` to use `RASTER_TILING_THRESHOLD_MB`
  - Removed references to `RASTER_ROUTE_LARGE_MB`

- [x] **docs_claude/DOCKER_INTEGRATION.md**:
  - Updated documentation to reflect V0.8 settings

- [x] **Verified imports work**:
  - All config imports successful
  - All job imports successful
  - Old settings confirmed removed

### Testing Checklist (V0.8 Section 13)

**After deployment, verify:**

- [ ] Submit 500MB raster → Single COG output (`output_mode: "single_cog"`)
- [ ] Submit 3GB raster → Tiled output (`output_mode: "tiled"`, N tiles)
- [ ] Verify mount validation logs WARNING (not failure) when mount disabled
- [ ] Verify health endpoint shows "degraded" when mount missing (not "unhealthy")
- [ ] Verify system remains functional without mount (degraded performance)
- [ ] Verify pre-flight returns correct `output_mode` and `estimated_tiles`
- [ ] Verify `process_large_raster_docker` shows deprecation warning in logs
- [ ] Verify `finalize_job` correctly formats result for both modes
- [ ] Verify Function App starts without import errors (Phase 13.7)

### Files Changed

| File | Changes |
|------|---------|
| `config/defaults.py` | Remove 4 settings, add 1, change 1 default |
| `config/raster_config.py` | Remove 4 fields, add 1 |
| `config/env_validation.py` | Remove 3 rules |
| `config/__init__.py` | Update debug_config |
| `jobs/process_raster_docker.py` | Add tiling decision logic |
| `jobs/process_large_raster_docker.py` | Add deprecation |
| `triggers/trigger_platform_status.py` | Simplify validation |
| `docker_service.py` | Stricter mount validation |

---

## 14. STAC COLLECTION BEHAVIOR

### Overview

The `process_raster_docker` job creates/updates STAC items and collections. Understanding the collection behavior is essential for organizing raster catalogs.

### Collection Parameter

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `collection_id` | str | **REQUIRED** | Target STAC collection ID |
| `collection_must_exist` | bool | `false` | If `true`, fail if collection doesn't exist |

### Behavior Matrix

| Collection Exists? | `collection_must_exist` | Result |
|-------------------|-------------------------|--------|
| ✅ Yes | `false` | **Add item to existing collection** |
| ✅ Yes | `true` | **Add item to existing collection** |
| ❌ No | `false` | **Auto-create collection**, then add item |
| ❌ No | `true` | **❌ FAIL** with error |

### Use Cases

#### 1. Add Raster to Existing Collection

```bash
# Submit job with collection_id of an existing collection
curl -X POST .../api/jobs/submit/process_raster_docker \
  -H "Content-Type: application/json" \
  -d '{
    "blob_name": "new_raster.tif",
    "container_name": "bronze-data",
    "collection_id": "flood-analysis",
    "collection_must_exist": true
  }'
```

**Result**: Item added to `flood-analysis` collection. Fails if collection doesn't exist.

#### 2. Create New Collection (Auto-Create)

```bash
# Submit with new collection_id (auto-creates if missing)
curl -X POST .../api/jobs/submit/process_raster_docker \
  -H "Content-Type: application/json" \
  -d '{
    "blob_name": "first_raster.tif",
    "container_name": "bronze-data",
    "collection_id": "new-project-data"
  }'
```

**Result**: Collection `new-project-data` auto-created with minimal metadata, then item added.

#### 3. Strict Mode (Prevent Accidental Collections)

```bash
# Use collection_must_exist=true to prevent typos creating new collections
curl -X POST .../api/jobs/submit/process_raster_docker \
  -H "Content-Type: application/json" \
  -d '{
    "blob_name": "raster.tif",
    "container_name": "bronze-data",
    "collection_id": "floood-analysis",  # Typo!
    "collection_must_exist": true
  }'
```

**Result**: ❌ Fails with `"Collection 'floood-analysis' does not exist and collection_must_exist=True"`.

### Auto-Created Collection Template

When a collection is auto-created, it uses this minimal template:

```json
{
  "id": "{collection_id}",
  "type": "Collection",
  "stac_version": "1.0.0",
  "description": "Auto-created collection for standalone raster processing (created: {timestamp})",
  "extent": {
    "spatial": {"bbox": [[-180, -90, 180, 90]]},
    "temporal": {"interval": [[null, null]]}
  },
  "license": "proprietary"
}
```

**Note**: Auto-created collections have placeholder extents. The extent updates as items are added.

### Best Practices

1. **Pre-create collections** for organized projects:
   - Use `/api/stac/collections` endpoint or SQL to create collections with proper metadata
   - Then submit jobs with `collection_must_exist: true`

2. **Use `collection_must_exist: true`** in production:
   - Prevents accidental collection creation from typos
   - Enforces catalog organization

3. **Use auto-create only for**:
   - Quick prototyping/testing
   - Ad-hoc analysis where collection metadata isn't critical

### Implementation Location

Collection handling is in:
- `services/stac_catalog.py:355-398` - Checks existence, auto-creates if needed
- `services/raster_stac_collection.py` - Tiled output STAC collection creation

### Future Enhancement Ideas

| Enhancement | Description | Priority |
|-------------|-------------|----------|
| Collection CRUD API | `POST/PUT/DELETE /api/stac/collections` | Medium |
| Collection templates | Pre-defined collection schemas by type | Low |
| Collection validation | Validate collection metadata before job submission | Low |
| Bulk add to collection | Submit multiple rasters to same collection in one job | Medium |

---

## 15. VIEWER URL PATTERNS

### Single COG vs Collection/Tiled Output

The system uses different URL patterns for viewing depending on output mode:

| Output Mode | TiTiler Endpoint | URL Pattern |
|-------------|-----------------|-------------|
| **Single COG** | `/cog/viewer` | `{titiler_base}/cog/viewer?url={cog_url}` |
| **Tiled/Collection** | `/searches/{id}/viewer` | `{titiler_base}/searches/{search_id}/viewer` |

### Single COG URL Generation

For single COG output, TiTiler URLs are generated directly from the COG blob path:

```python
# In jobs/process_raster_docker.py finalize_job():
titiler_urls = config.generate_titiler_urls_unified(
    mode="cog",
    container=cog['cog_container'],
    blob_name=cog['cog_blob']
)
# Returns: {"viewer_url": "{titiler}/cog/viewer?url=..."}
```

**Implementation**: `config/app_config.py` → `generate_titiler_urls_unified()`

### Tiled/Collection URL Generation (pgSTAC Search)

For tiled output (collections), we register a pgSTAC search that treats the collection as a single scene:

```python
# In services/stac_collection.py:
from services.pgstac_search_registration import PgSTACSearchRegistration

search_registrar = PgSTACSearchRegistration()
search_id = search_registrar.register_collection_search(
    collection_id=collection_id,
    metadata={"name": f"{collection_id} mosaic"},
    bbox=collection_bbox
)

# Generate URLs from search_id
urls = search_registrar.get_search_urls(
    search_id=search_id,
    titiler_base_url=config.titiler_base_url,
    assets=["data"]
)
# Returns: {
#     "viewer": "{titiler}/searches/{search_id}/viewer",
#     "tilejson": "{titiler}/searches/{search_id}/WebMercatorQuad/tilejson.json",
#     "tiles": "{titiler}/searches/{search_id}/WebMercatorQuad/tiles/{z}/{x}/{y}"
# }
```

**Implementation**:
- `services/stac_collection.py` → `_create_stac_collection_impl()`
- `services/pgstac_search_registration.py` → `PgSTACSearchRegistration`

### Why pgSTAC Search for Collections?

1. **Dynamic mosaicking**: TiTiler-PgSTAC renders multiple COGs as a single layer
2. **OAuth compatibility**: Search-based URLs work with Managed Identity auth
3. **Automatic updates**: Adding items to collection updates the mosaic (no re-registration)
4. **Deterministic IDs**: `search_id = SHA256(collection_id)` (permanent, no expiry)

### ⚠️ CRITICAL BUG: Missing Import in Tiled Workflow

The tiled workflow in `handler_process_raster_complete.py` has a **broken import**:

```python
# Line 449 - BROKEN IMPORT
from .raster_stac_collection import create_stac_collection  # ❌ File doesn't exist!

# SHOULD BE:
from .stac_collection import create_stac_collection  # ✅ Correct path
```

**Impact**: Tiled workflow fails at Phase 5 (STAC Registration) with `ImportError`.

**Fix Required**:
- [x] Fix import path in `services/handler_process_raster_complete.py:449` ✅ FIXED (24 JAN 2026)
- [ ] Test tiled workflow end-to-end

### URL Examples

**Single COG**:
```
https://rmhtitiler-....azurecontainerapps.io/cog/viewer?url=https://storage.blob.core.windows.net/silver-cogs/flood_depth_cog.tif
```

**Tiled/Collection**:
```
https://rmhtitiler-....azurecontainerapps.io/searches/abc123def456/viewer
```

---

## 16. DOCKER VECTOR ETL IMPLEMENTATION

### Overview

Consolidate the 3-stage Function App vector ETL (`process_vector`) into a single Docker handler with checkpoint-based progress tracking. This eliminates pickle serialization overhead, enables persistent connection pooling, and supports large files via mounted storage.

### Current vs Docker Architecture

```
CURRENT (Function App - 3 Stages):
┌─────────────────────────────────────────────────────────────────┐
│ Stage 1: Prepare                                                │
│ ├── Download blob (2GB limit)                                   │
│ ├── Convert to GeoDataFrame                                     │
│ ├── Validate, reproject to EPSG:4326                           │
│ ├── CREATE TABLE + metadata                                     │
│ ├── Chunk → Pickle to Silver blob storage                      │
│ └── ⏱️ 10-minute timeout risk                                   │
└─────────────────────────────────────────────────────────────────┘
            ↓ Service Bus (latency)
┌─────────────────────────────────────────────────────────────────┐
│ Stage 2: Upload (Fan-out, N parallel tasks)                     │
│ ├── Download pickle from blob (per chunk)                       │
│ ├── DELETE batch_id → INSERT rows                               │
│ ├── New DB connection per task                                  │
│ └── ⏱️ 10-minute timeout per chunk                              │
└─────────────────────────────────────────────────────────────────┘
            ↓ Service Bus (latency)
┌─────────────────────────────────────────────────────────────────┐
│ Stage 3: STAC                                                   │
│ ├── Extract metadata from table                                 │
│ ├── Insert STAC item                                            │
│ └── Register artifact                                           │
└─────────────────────────────────────────────────────────────────┘

DOCKER (Single Handler with Checkpoints):
┌─────────────────────────────────────────────────────────────────┐
│ vector_docker_complete                                          │
│                                                                 │
│ ✓ Checkpoint 1: validated                                       │
│   └── Load file, validate geometry, detect CRS                 │
│                                                                 │
│ ✓ Checkpoint 2: table_created                                   │
│   └── CREATE TABLE + metadata + default style                  │
│                                                                 │
│ ✓ Checkpoint 3-N: chunk_uploaded (loop)                        │
│   └── Batch INSERT with pooled connection (no pickle!)         │
│                                                                 │
│ ✓ Checkpoint Final: complete                                    │
│   └── STAC item + artifact registry                            │
│                                                                 │
│ Benefits:                                                       │
│   • No timeout (long-running process)                          │
│   • No pickle serialization (direct memory → DB)               │
│   • Connection pool reuse (persistent)                         │
│   • Large file support (streaming from mount)                  │
│   • Fine-grained progress tracking                             │
└─────────────────────────────────────────────────────────────────┘
```

### Performance Comparison

| Metric | Function App | Docker | Improvement |
|--------|-------------|--------|-------------|
| 100K rows | ~3 min | ~45 sec | **4x faster** |
| 1M rows | ~25 min (timeout risk) | ~5 min | **5x faster** |
| 5M rows | ❌ Timeout | ~25 min | **Now possible** |
| 10GB file | ❌ Memory limit | ✅ Streaming | **Now possible** |
| Connection overhead | ~100-500ms/chunk | ~0ms (pooled) | **Eliminated** |
| Pickle I/O | ~30-60s/chunk | ~0ms | **Eliminated** |

### Implementation Phases

#### Phase 16.1: Job Definition ✅ COMPLETE (24 JAN 2026)
**Effort: 30 minutes | Risk: Low**

Created `jobs/vector_docker_etl.py`:

```python
class VectorDockerETLJob(JobBaseMixin, JobBase):
    """
    Docker-based vector ETL with checkpoint progress tracking.

    Single consolidated handler replaces 3-stage Function App workflow.
    Uses connection pooling and eliminates pickle serialization.
    """

    job_type = "vector_docker_etl"
    description = "Docker vector ETL with connection pooling"

    # SINGLE STAGE - all work in one handler with checkpoints
    stages = [
        {
            "number": 1,
            "name": "process_complete",
            "task_type": "vector_docker_complete",
            "parallelism": "single"
        }
    ]

    parameters_schema = {
        # === Source ===
        'source_url': {'type': 'str', 'required': True},
        'container_name': {'type': 'str', 'default': None},
        'blob_name': {'type': 'str', 'default': None},

        # === Target ===
        'table_name': {'type': 'str', 'required': True},
        'schema': {'type': 'str', 'default': 'geo'},
        'collection_id': {'type': 'str', 'required': True},
        'overwrite': {'type': 'bool', 'default': False},

        # === Geometry ===
        'geometry_column': {'type': 'str', 'default': None},
        'lat_column': {'type': 'str', 'default': None},
        'lon_column': {'type': 'str', 'default': None},
        'wkt_column': {'type': 'str', 'default': None},
        'source_crs': {'type': 'str', 'default': None},

        # === Column Mapping ===
        'column_mapping': {'type': 'dict', 'default': None},
        'temporal_column': {'type': 'str', 'default': None},

        # === Metadata ===
        'title': {'type': 'str', 'default': None},
        'description': {'type': 'str', 'default': None},
        'license': {'type': 'str', 'default': 'proprietary'},
        'keywords': {'type': 'list', 'default': None},
        'attribution': {'type': 'str', 'default': None},

        # === Style Parameters (V0.8+) ===
        'style': {'type': 'dict', 'default': None},  # See Phase 16.6

        # === Processing ===
        'chunk_size': {'type': 'int', 'default': 20000},
        'create_tile_view': {'type': 'bool', 'default': False},
    }
```

**Deliverable**: Job definition with comprehensive parameter schema

---

#### Phase 16.2: Handler Implementation ✅ COMPLETE (24 JAN 2026)
**Effort: 3-4 hours | Risk: Medium**

Created `services/handler_vector_docker_complete.py`:

```python
def vector_docker_complete(params: dict, context: TaskContext) -> dict:
    """
    Consolidated vector ETL with checkpoint-based progress.

    Checkpoints:
        validated      - Source validated, GeoDataFrame loaded
        table_created  - PostGIS table + metadata created
        style_created  - Default style registered (if style params)
        chunk_N        - Chunk N uploaded (N = 0, 1, 2...)
        stac_created   - STAC item registered
        complete       - Final result

    Resume Behavior:
        On restart, reads last checkpoint and resumes from that point.
        Chunk uploads are idempotent (DELETE batch_id before INSERT).
    """
    from services.vector.process_vector_tasks import (
        load_and_validate_geodataframe,
        create_postgis_table,
        insert_chunk_idempotent
    )
    from services.stac_vector_catalog import create_vector_stac
    from ogc_styles.repository import OGCStylesRepository

    db_pool = context.get_db_pool()  # Persistent connection pool
    last_checkpoint = context.get_last_checkpoint()

    # =========================================================================
    # PHASE 1: Validate & Load
    # =========================================================================
    if not last_checkpoint or last_checkpoint.phase < "validated":
        logger.info("📥 Phase 1: Loading and validating source file")

        gdf = load_and_validate_geodataframe(
            source_url=params['source_url'],
            container_name=params.get('container_name'),
            blob_name=params.get('blob_name'),
            geometry_column=params.get('geometry_column'),
            lat_column=params.get('lat_column'),
            lon_column=params.get('lon_column'),
            wkt_column=params.get('wkt_column'),
            source_crs=params.get('source_crs'),
            column_mapping=params.get('column_mapping'),
            mount_path=context.etl_mount_path  # Use mount if available
        )

        context.checkpoint("validated", {
            "features": len(gdf),
            "crs": str(gdf.crs),
            "geometry_type": gdf.geom_type.iloc[0] if len(gdf) > 0 else None,
            "columns": list(gdf.columns)
        })
        context.store("gdf", gdf)  # Keep in memory for next phases
    else:
        gdf = context.restore("gdf")  # Resume from checkpoint

    # =========================================================================
    # PHASE 2: Create Table
    # =========================================================================
    if not last_checkpoint or last_checkpoint.phase < "table_created":
        logger.info("🗄️ Phase 2: Creating PostGIS table")

        table_info = create_postgis_table(
            gdf=gdf,
            table_name=params['table_name'],
            schema=params.get('schema', 'geo'),
            overwrite=params.get('overwrite', False),
            title=params.get('title'),
            description=params.get('description'),
            license=params.get('license'),
            keywords=params.get('keywords'),
            attribution=params.get('attribution'),
            temporal_column=params.get('temporal_column'),
            db_pool=db_pool
        )

        context.checkpoint("table_created", {
            "table": f"{table_info['schema']}.{table_info['table_name']}",
            "geometry_type": table_info['geometry_type'],
            "srid": table_info['srid']
        })
        context.store("table_info", table_info)
    else:
        table_info = context.restore("table_info")

    # =========================================================================
    # PHASE 2.5: Create Default Style (if style params provided)
    # =========================================================================
    if not last_checkpoint or last_checkpoint.phase < "style_created":
        style_params = params.get('style')
        if style_params:
            logger.info("🎨 Phase 2.5: Creating style from parameters")
            style_result = _create_style_from_params(
                collection_id=params['table_name'],
                geometry_type=table_info['geometry_type'],
                style_params=style_params,
                db_pool=db_pool
            )
            context.checkpoint("style_created", style_result)
        else:
            # Auto-generate default style
            styles_repo = OGCStylesRepository()
            styles_repo.create_default_style_for_collection(
                collection_id=params['table_name'],
                geometry_type=table_info['geometry_type']
            )
            context.checkpoint("style_created", {"style_id": "default", "auto_generated": True})

    # =========================================================================
    # PHASE 3: Upload Chunks (Loop with checkpoints)
    # =========================================================================
    chunk_size = params.get('chunk_size', 20000)
    total_features = len(gdf)
    num_chunks = (total_features + chunk_size - 1) // chunk_size

    # Determine resume point
    start_chunk = 0
    if last_checkpoint and last_checkpoint.phase.startswith("chunk_"):
        start_chunk = int(last_checkpoint.phase.split("_")[1]) + 1

    logger.info(f"📤 Phase 3: Uploading {num_chunks} chunks (starting from {start_chunk})")

    total_rows_inserted = 0
    for i in range(start_chunk, num_chunks):
        chunk_start = i * chunk_size
        chunk_end = min((i + 1) * chunk_size, total_features)
        chunk = gdf.iloc[chunk_start:chunk_end]

        # Idempotent insert (DELETE batch_id, then INSERT)
        batch_id = f"{context.job_id[:8]}-chunk-{i}"
        rows = insert_chunk_idempotent(
            chunk=chunk,
            table_name=table_info['table_name'],
            schema=table_info['schema'],
            batch_id=batch_id,
            db_pool=db_pool
        )
        total_rows_inserted += rows

        progress_pct = int((i + 1) / num_chunks * 100)
        context.checkpoint(f"chunk_{i}", {
            "chunk": i,
            "rows": rows,
            "total_rows": total_rows_inserted,
            "progress_pct": progress_pct
        })

        # Log progress at milestones
        if progress_pct in [25, 50, 75, 100] or i == num_chunks - 1:
            logger.info(f"   Progress: {progress_pct}% ({total_rows_inserted:,} rows)")

    # =========================================================================
    # PHASE 4: STAC Registration
    # =========================================================================
    if not last_checkpoint or last_checkpoint.phase < "stac_created":
        logger.info("📋 Phase 4: Creating STAC item")

        stac_result = create_vector_stac(
            table_name=table_info['table_name'],
            schema=table_info['schema'],
            collection_id=params['collection_id'],
            source_file=params.get('source_url') or params.get('blob_name'),
            platform_job_id=params.get('_platform_job_id'),
            db_pool=db_pool
        )
        context.checkpoint("stac_created", stac_result)
    else:
        stac_result = last_checkpoint.data if last_checkpoint.phase == "stac_created" else {}

    # =========================================================================
    # COMPLETE
    # =========================================================================
    result = {
        "success": True,
        "table_name": table_info['table_name'],
        "schema": table_info['schema'],
        "total_rows": total_rows_inserted,
        "geometry_type": table_info['geometry_type'],
        "srid": table_info['srid'],
        "stac_item_id": stac_result.get('item_id'),
        "collection_id": params['collection_id'],
        "chunks_uploaded": num_chunks,
        "style_id": "default"
    }

    context.checkpoint("complete", result)
    logger.info(f"✅ Vector ETL complete: {result['table_name']} ({total_rows_inserted:,} rows)")

    return result
```

**Deliverable**: Consolidated handler with checkpoint-based progress

---

#### Phase 16.3: Task Routing ✅ COMPLETE (24 JAN 2026)
**Effort: 15 minutes | Risk: Low**

Updated `config/defaults.py`:

```python
class TaskRoutingDefaults:
    DOCKER_TASKS = frozenset([
        # ... existing ...

        # Vector Docker ETL (V0.8)
        "vector_docker_complete",
    ])
```

**Deliverable**: Handler routed to Docker worker

---

#### Phase 16.4: Job Registration ✅ COMPLETE (24 JAN 2026)
**Effort: 15 minutes | Risk: Low**

Updated `jobs/__init__.py`:

```python
from jobs.vector_docker_etl import VectorDockerETLJob

ALL_JOBS = {
    # ... existing ...
    "vector_docker_etl": VectorDockerETLJob,
}
```

Update `services/__init__.py`:

```python
from services.handler_vector_docker_complete import vector_docker_complete

ALL_HANDLERS = {
    # ... existing ...
    "vector_docker_complete": vector_docker_complete,
}
```

**Deliverable**: Job and handler registered

---

#### Phase 16.5: Platform/Submit Endpoint Routing ✅ COMPLETE (24 JAN 2026)
**Effort: 1 hour | Risk: Low**

Updated `triggers/trigger_platform.py` to route to Docker by default, with override option.

##### Parameter Addition

Add `docker` parameter to platform vector submit endpoint:

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `docker` | bool | `true` | Route to Docker worker (false = Function App) |

##### Implementation

Update `triggers/trigger_platform_submit.py` (or equivalent):

```python
def platform_submit_vector(req: func.HttpRequest) -> func.HttpResponse:
    """
    Platform vector submission endpoint.

    V0.8: Routes to Docker by default for performance.
    Set docker=false to use Function App worker (admin/debug).
    """
    params = req.get_json()

    # V0.8: Docker routing parameter (default: true)
    use_docker = params.pop('docker', True)

    if use_docker:
        # Route to Docker worker
        job_type = "vector_docker_etl"
        logger.info(f"Routing vector ETL to Docker worker")
    else:
        # Route to Function App (retained for future size-based routing)
        job_type = "process_vector"
        logger.info(f"Routing vector ETL to Function App (docker=false)")

    # Submit job
    return submit_job(job_type, params)
```

##### Routing Logic (Future Enhancement)

The Function App workflow is retained for future automatic routing based on:
- File size (small files → Function App, large files → Docker)
- Feature count estimates
- Geometry complexity
- Admin override for debugging

```python
# Future routing logic (not implemented in V0.8)
def _determine_vector_routing(params: dict, file_size_mb: float) -> str:
    """
    Determine optimal routing for vector ETL.

    Returns:
        Job type: "vector_docker_etl" or "process_vector"
    """
    # Explicit override
    if params.get('docker') is False:
        return "process_vector"
    if params.get('docker') is True:
        return "vector_docker_etl"

    # Auto-routing based on file size (future)
    # Small files: Function App (faster cold start)
    # Large files: Docker (no timeout, connection pool)
    DOCKER_THRESHOLD_MB = 50  # TBD

    if file_size_mb > DOCKER_THRESHOLD_MB:
        return "vector_docker_etl"
    else:
        return "process_vector"
```

**Deliverable**: Platform endpoint routes to Docker by default with override option

---

#### Phase 16.6: Style Parameters (Future Enhancement)
**Effort: 2-3 hours | Risk: Low**
**Priority: After basic functionality confirmed**

Add support for passing style parameters during ETL that get stored in OGC Styles system.

##### Style Parameter Schema

```python
# In job parameters_schema:
'style': {
    'type': 'dict',
    'default': None,
    'schema': {
        # === Basic Colors ===
        'fill_color': {'type': 'str', 'default': '#3388ff'},
        'fill_opacity': {'type': 'float', 'default': 0.6},
        'stroke_color': {'type': 'str', 'default': '#2266cc'},
        'stroke_width': {'type': 'float', 'default': 1.5},
        'stroke_opacity': {'type': 'float', 'default': 1.0},

        # === Point-specific ===
        'marker_size': {'type': 'float', 'default': 8},

        # === Data-driven Styling (TBD) ===
        'classify_by': {'type': 'str', 'default': None},       # Column name
        'classification': {'type': 'str', 'default': 'equal'}, # equal, quantile, natural
        'num_classes': {'type': 'int', 'default': 5},
        'color_ramp': {'type': 'str', 'default': 'viridis'},   # viridis, plasma, etc.
        'class_breaks': {'type': 'list', 'default': None},     # Manual breaks
        'class_colors': {'type': 'list', 'default': None},     # Manual colors

        # === Categorical Styling (TBD) ===
        'category_column': {'type': 'str', 'default': None},
        'category_colors': {'type': 'dict', 'default': None},  # {value: color}
    }
}
```

##### Example API Calls

**Basic Style:**
```json
{
    "source_url": "https://storage.blob.core.windows.net/bronze/parks.geojson",
    "table_name": "national_parks",
    "collection_id": "parks-collection",
    "style": {
        "fill_color": "#228B22",
        "fill_opacity": 0.7,
        "stroke_color": "#006400",
        "stroke_width": 2
    }
}
```

**Data-Driven Style (TBD):**
```json
{
    "source_url": "https://storage.blob.core.windows.net/bronze/population.geojson",
    "table_name": "census_tracts",
    "collection_id": "census-collection",
    "style": {
        "classify_by": "population_density",
        "classification": "quantile",
        "num_classes": 5,
        "color_ramp": "YlOrRd",
        "stroke_color": "#666666",
        "stroke_width": 0.5
    }
}
```

**Categorical Style (TBD):**
```json
{
    "source_url": "https://storage.blob.core.windows.net/bronze/landuse.geojson",
    "table_name": "land_use",
    "collection_id": "landuse-collection",
    "style": {
        "category_column": "land_type",
        "category_colors": {
            "residential": "#FFA500",
            "commercial": "#0000FF",
            "industrial": "#808080",
            "agricultural": "#90EE90",
            "forest": "#006400"
        }
    }
}
```

##### Style Creation Helper

```python
def _create_style_from_params(
    collection_id: str,
    geometry_type: str,
    style_params: dict,
    db_pool
) -> dict:
    """
    Create OGC Style from ETL parameters.

    Converts simple style params to CartoSym-JSON format
    and stores in geo.feature_collection_styles.

    Args:
        collection_id: Table name (used as collection_id)
        geometry_type: PostGIS geometry type
        style_params: Style parameters from job submission
        db_pool: Database connection pool

    Returns:
        dict with style_id and creation status
    """
    from ogc_styles.repository import OGCStylesRepository

    # Normalize geometry type
    geom_type_map = {
        "POLYGON": "Polygon", "MULTIPOLYGON": "Polygon",
        "LINESTRING": "Line", "MULTILINESTRING": "Line",
        "POINT": "Point", "MULTIPOINT": "Point"
    }
    sym_type = geom_type_map.get(geometry_type.upper(), "Polygon")

    # Extract style params with defaults
    fill_color = style_params.get('fill_color', '#3388ff')
    fill_opacity = style_params.get('fill_opacity', 0.6)
    stroke_color = style_params.get('stroke_color', '#2266cc')
    stroke_width = style_params.get('stroke_width', 1.5)
    stroke_opacity = style_params.get('stroke_opacity', 1.0)
    marker_size = style_params.get('marker_size', 8)

    # Build CartoSym-JSON symbolizer based on geometry type
    if sym_type == "Polygon":
        symbolizer = {
            "type": "Polygon",
            "fill": {"color": fill_color, "opacity": fill_opacity},
            "stroke": {"color": stroke_color, "width": stroke_width, "opacity": stroke_opacity}
        }
    elif sym_type == "Line":
        symbolizer = {
            "type": "Line",
            "stroke": {"color": stroke_color, "width": stroke_width, "opacity": stroke_opacity}
        }
    else:  # Point
        symbolizer = {
            "type": "Point",
            "marker": {
                "size": marker_size,
                "fill": {"color": fill_color, "opacity": fill_opacity},
                "stroke": {"color": stroke_color, "width": stroke_width}
            }
        }

    # Check for data-driven styling (TBD - future enhancement)
    classify_by = style_params.get('classify_by')
    category_column = style_params.get('category_column')

    if classify_by or category_column:
        # TBD: Generate data-driven CartoSym-JSON with selectors
        # This requires querying the data to get value ranges/categories
        logger.warning("Data-driven styling TBD - using basic style")
        styling_rules = [{"name": "default", "symbolizer": symbolizer}]
    else:
        styling_rules = [{"name": "default", "symbolizer": symbolizer}]

    # Build CartoSym-JSON document
    style_spec = {
        "name": f"{collection_id}-default",
        "title": f"Default style for {collection_id}",
        "description": "Style created from ETL parameters",
        "metadata": {
            "source": "vector_docker_etl",
            "params": style_params
        },
        "stylingRules": styling_rules
    }

    # Store in database
    styles_repo = OGCStylesRepository()
    styles_repo.create_style(
        collection_id=collection_id,
        style_id="default",
        style_spec=style_spec,
        title=style_spec["title"],
        description=style_spec["description"],
        is_default=True
    )

    return {
        "style_id": "default",
        "auto_generated": False,
        "params_used": style_params
    }
```

##### Future Data-Driven Styling (TBD)

When `classify_by` or `category_column` is provided, the system should:

1. **Query data statistics** during Phase 2 (after table creation):
   ```sql
   SELECT MIN(column), MAX(column),
          percentile_cont(ARRAY[0.2, 0.4, 0.6, 0.8]) WITHIN GROUP (ORDER BY column)
   FROM table
   ```

2. **Generate class breaks** based on classification method:
   - `equal`: Equal intervals between min/max
   - `quantile`: Percentile-based breaks
   - `natural`: Jenks natural breaks (requires additional computation)

3. **Generate CartoSym-JSON with selectors**:
   ```json
   {
     "stylingRules": [
       {
         "name": "class_1",
         "selector": {"op": "<=", "args": [{"property": "value"}, 100]},
         "symbolizer": {"type": "Polygon", "fill": {"color": "#ffffcc"}}
       },
       {
         "name": "class_2",
         "selector": {"op": "<=", "args": [{"property": "value"}, 500]},
         "symbolizer": {"type": "Polygon", "fill": {"color": "#fd8d3c"}}
       }
     ]
   }
   ```

4. **Store in OGC Styles** for consumption by viewers

**Implementation Priority**: After basic vector ETL functionality is confirmed working.

---

### Testing Checklist

#### Phase 16.1-16.4 (Basic Functionality)
- [ ] Submit small GeoJSON (1K features) → table created, STAC registered
- [ ] Submit medium Shapefile (100K features) → checkpoints visible in logs
- [ ] Submit large GeoPackage (1M features) → completes without timeout
- [ ] Verify connection pool reuse (no "connection created" log spam)
- [ ] Verify checkpoint resume (kill mid-job, restart, continues from last chunk)
- [ ] Verify default style auto-generated in `geo.feature_collection_styles`

#### Phase 16.5 (Deprecation)
- [ ] Submit to `process_vector` → deprecation warning in logs
- [ ] Verify Function App job still works (backward compatibility)

#### Phase 16.6 (Style Parameters)
- [ ] Submit with basic style params → custom colors applied
- [ ] Submit with marker_size → point style correct
- [ ] Submit without style params → default style auto-generated
- [ ] Verify style visible in OGC Styles API: `GET /collections/{id}/styles`
- [ ] Verify style renders correctly in vector tile viewer

### Files to Create/Modify

| File | Action | Phase |
|------|--------|-------|
| `jobs/vector_docker_etl.py` | CREATE | 16.1 |
| `services/handler_vector_docker_complete.py` | CREATE | 16.2 |
| `config/defaults.py` | MODIFY (add to DOCKER_TASKS) | 16.3 |
| `jobs/__init__.py` | MODIFY (register job) | 16.4 |
| `services/__init__.py` | MODIFY (register handler) | 16.4 |
| `jobs/process_vector.py` | MODIFY (add deprecation) | 16.5 |
| `services/handler_vector_docker_complete.py` | MODIFY (add style helper) | 16.6 |

---

## 17. FUNCTION_APP.PY CLEANUP (APP_CLEANUP Phases 5-7)

### Overview

Continue reducing `function_app.py` from ~2,990 lines to ~2,170 lines by extracting remaining inline code to blueprints. This is Phase 5-7 of the APP_CLEANUP epic (Phases 1-4 completed 23 JAN 2026).

**Reference Document**: `APP_CLEANUP.md` for full details.

### Phase 17.1: Platform Blueprint ✅ COMPLETE (23 JAN 2026)
**Status**: Already implemented in `triggers/platform/platform_bp.py`
**Lines extracted**: ~400

The Platform layer is the **anti-corruption layer** for external apps (DDH). Moving it to a dedicated blueprint enables:
1. Independent deployment as separate Function App
2. Clean separation from internal admin endpoints
3. Conditional auth configuration (gateway modes only)

#### Actions

1. **Delete deprecated endpoints** (3 total):
   - `GET /platform/jobs/{job_id}/status` → use `/platform/status/{job_id}`
   - `POST /platform/unpublish/vector` → use `/platform/unpublish`
   - `POST /platform/unpublish/raster` → use `/platform/unpublish`

2. **Create blueprint structure**:
   ```
   triggers/platform/
   ├── __init__.py
   ├── platform_bp.py           # Blueprint with all endpoints
   ├── submit_handler.py        # /platform/submit logic
   ├── status_handler.py        # /platform/status/* logic
   ├── approval_handler.py      # /platform/approve, /revoke, /approvals/*
   ├── catalog_handler.py       # /platform/catalog/* logic
   └── unpublish_handler.py     # /platform/unpublish logic
   ```

3. **Move 17 endpoints** to blueprint:
   | Route | Method | Purpose |
   |-------|--------|---------|
   | `/platform/submit` | POST | Submit request from DDH |
   | `/platform/status/{id}` | GET | Get request/job status |
   | `/platform/status` | GET | List all requests |
   | `/platform/health` | GET | System readiness |
   | `/platform/failures` | GET | Recent failures |
   | `/platform/lineage/{id}` | GET | Data lineage trace |
   | `/platform/validate` | POST | Pre-flight validation |
   | `/platform/unpublish` | POST | Consolidated unpublish |
   | `/platform/approve` | POST | Approve dataset |
   | `/platform/revoke` | POST | Revoke approval |
   | `/platform/approvals` | GET | List approvals |
   | `/platform/approvals/{id}` | GET | Get approval |
   | `/platform/approvals/status` | GET | Approval status summary |
   | `/platform/catalog/lookup` | GET | STAC lookup by DDH IDs |
   | `/platform/catalog/item/{c}/{i}` | GET | Get STAC item |
   | `/platform/catalog/assets/{c}/{i}` | GET | Get assets with TiTiler URLs |
   | `/platform/catalog/dataset/{id}` | GET | List items for dataset |

4. **Register blueprint conditionally**:
   ```python
   # Only register for modes with platform endpoints
   if _app_mode.has_platform_endpoints:
       from triggers.platform import bp as platform_bp
       app.register_functions(platform_bp)
   ```

#### Acceptance Criteria
- [x] Blueprint created with 17 endpoints
- [x] 3 deprecated endpoints not registered (handlers exist for backward compat)
- [x] ~400 lines in blueprint (triggers/platform/platform_bp.py)
- [x] All platform/* routes work as before
- [x] Conditional registration based on APP_MODE

---

### Phase 17.2: Endpoint Cleanup ✅ COMPLETE (24 JAN 2026)
**Lines removed**: ~220

#### 17.2A: Delete Duplicate STAC Endpoints

The `/api/collections/*` routes duplicate `/api/stac/collections/*`:

| Delete | Keep |
|--------|------|
| `GET /collections` | `GET /stac/collections` |
| `GET /collections/{id}` | `GET /stac/collections/{id}` |
| `GET /collections/{id}/items` | `GET /stac/collections/{id}/items` |
| `GET /search` | (move to stac/search or delete) |

**Lines to delete**: ~170 (approx. lines 1641-1807)

#### 17.2B: Consolidate Storage Endpoints

Merge `containers/*` into `storage/*`:

| Current | New |
|---------|-----|
| `GET /containers/{name}/blobs` | `GET /storage/{container}/blobs` |
| `GET /containers/{name}/blob` | `GET /storage/{container}/blob` |
| `GET /storage/containers` | Keep |
| `POST /storage/upload` | Keep |

**Lines affected**: ~50

#### Acceptance Criteria ✅
- [x] 4 duplicate collections/* endpoints deleted
- [x] containers/* merged into storage/*
- [x] ~220 lines removed
- [x] `/stac/collections/*` still functional

---

### Phase 17.3: Unified STAC Blueprint ✅ COMPLETE (24 JAN 2026)
**Status**: All 19 stac/* endpoints consolidated into `triggers/stac/stac_bp.py`
**Lines moved/deleted**: ~600 (from function_app.py + stac_api/ + admin_stac.py)

#### Design Decision

**ALL stac/ endpoints are admin-only** - there is no B2C (business-to-consumer) STAC API. All access is through our Platform layer or internal admin tools.

#### Actions

Move all 13 inline `stac/*` endpoints to existing `admin_stac_bp` blueprint:

| Route | Current | Action |
|-------|---------|--------|
| `stac/` (landing) | function_app.py | → `admin_stac_bp` |
| `stac/conformance` | function_app.py | → `admin_stac_bp` |
| `stac/collections` | function_app.py | → `admin_stac_bp` |
| `stac/collections/{id}` | function_app.py | → `admin_stac_bp` |
| `stac/collections/{id}/items` | function_app.py | → `admin_stac_bp` |
| `stac/collections/{id}/items/{item}` | function_app.py | → `admin_stac_bp` |
| `stac/schema/info` | function_app.py | → `admin_stac_bp` |
| `stac/collections/summary` | function_app.py | → `admin_stac_bp` |
| `stac/collections/{id}/stats` | function_app.py | → `admin_stac_bp` |
| `stac/items/{item_id}` | function_app.py | → `admin_stac_bp` |
| `stac/health` | function_app.py | → `admin_stac_bp` |
| `stac/vector` | function_app.py | → `admin_stac_bp` |
| `stac/extract` | function_app.py | → `admin_stac_bp` |
| `stac/repair/*` | admin_stac_bp | Keep |
| `stac/nuke` | admin_stac_bp | Keep |

#### Acceptance Criteria
- [ ] All 13 inline stac/* endpoints moved to admin_stac_bp
- [ ] Existing admin_stac_bp endpoints preserved
- [ ] ~200 lines removed from function_app.py
- [ ] Conditional registration for admin modes only

---

### Summary: Line Reduction

| Phase | Status | Lines | Target |
|-------|--------|-------|--------|
| 1-4 | ✅ COMPLETE | ~925 removed | startup/, service_bus/, timers/, machine_factory |
| **17.1** | ✅ COMPLETE | ~400 extracted | Platform Blueprint (already existed) |
| **17.2** | ✅ COMPLETE | ~220 removed | Endpoint Cleanup (24 JAN 2026) |
| **17.3** | ✅ COMPLETE | ~600 moved | Unified STAC Blueprint (24 JAN 2026) |

**Current function_app.py**: ~2,990 lines
**After Phases 17.1-17.3**: ~2,170 lines

---

### Testing Checklist

#### Phase 17.1 (Platform Blueprint)
- [ ] `POST /platform/submit` creates request
- [ ] `GET /platform/status/{id}` returns status
- [ ] `POST /platform/unpublish` works (consolidated)
- [ ] Deprecated endpoints return 404
- [ ] Blueprint only registered for platform/orchestrator/standalone modes

#### Phase 17.2 (Endpoint Cleanup) ✅
- [x] `/collections/*` returns 404
- [x] `/stac/collections/*` still works
- [x] `/storage/{container}/blobs` works (new route)
- [x] `/containers/*` returns 404

#### Phase 17.3 (Unified STAC Blueprint) ✅
- [x] All 19 stac/* endpoints consolidated into triggers/stac/stac_bp.py
- [x] Old stac_api/ module deleted
- [x] Old admin_stac.py deleted
- [x] Old stac_repair.py merged into stac_bp.py
- [x] Blueprint registered conditionally for admin modes

---

*Document created: 23 JAN 2026*
*Last updated: 24 JAN 2026*
*Author: Claude + Robert Harrison*
