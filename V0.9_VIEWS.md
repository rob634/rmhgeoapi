# V0.9 SPLIT VIEWS â€” Detailed Implementation Plan

**Created**: 24 FEB 2026
**Feature**: Per-value PostgreSQL views for vector tables
**Branch**: `dev-architecture-fixes` (or new feature branch)

---

## What This Feature Does

When a user uploads a vector file with a categorical column (e.g., `admin_level` with values 0, 1, 2), they can specify `split_column: "admin_level"` in processing_options. After the table is uploaded, the system:

1. Validates the column exists and is categorical (text/integer/boolean â€” NOT float/geometry/json)
2. Queries `SELECT DISTINCT` to find all values (max 20, rejects if exceeded)
3. Creates one `CREATE VIEW` per distinct value (e.g., `WHERE admin_level = 0`)
4. Registers each view in `geo.table_catalog` with title, bbox, feature_count
5. TiPG auto-discovers the views as separate OGC Feature collections

The base table stays visible alongside the views. Views use the base table's spatial index (zero storage overhead). `DROP TABLE CASCADE` on the base table auto-drops all views.

---

## Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Value discovery | Auto-discover via `SELECT DISTINCT` | Simpler API, user doesn't need to know values upfront |
| Cardinality limit | Hard cap at 20 | Prevents accidental 1000-view creation |
| Type validation | Categorical only (text, int, bool) | Floats create meaningless views |
| Base table visibility | Exposed alongside views | No TiPG config changes needed |
| Catalog entries | Yes, each view gets a row | First-class citizen in web UI |
| Future: explicit values | `split_values` field reserved, raises error | Model is future-proof but not implemented |
| SQL safety | `sql.Identifier()` + `sql.Literal()` everywhere | Zero SQL injection risk |
| Atomicity | All views in one transaction | Partial failure â†’ full rollback |

---

## Tasks (Ordered by Dependency)

Each task is independent and can be implemented by a separate Claude session. Tasks 1-3 have no dependencies. Tasks 4-5 depend on Task 1. Task 6 depends on Tasks 2 and 5. Task 7 depends on Task 2.

```
Task 1: processing_options model â”€â”€â”
Task 2: view_splitter.py (NEW)  â”€â”€â”€â”¤
Task 3: __init__.py exports     â”€â”€â”€â”¤â”€â”€ no dependencies between these
                                   â”‚
Task 4: platform_translation â”€â”€â”€â”€â”€â”€â”¤â”€â”€ depends on Task 1
Task 5: job schema + task params â”€â”€â”¤â”€â”€ depends on Task 1
                                   â”‚
Task 6: handler integration â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€ depends on Tasks 2 + 5
Task 7: DROP TABLE CASCADE â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€ depends on Task 2
```

---

## TASK 1: Add `split_column` to VectorProcessingOptions

**File**: `core/models/processing_options.py`
**Depends on**: Nothing

### 1a. Add `List` to imports (line 34)

Find this line:
```python
from typing import Optional
```

Replace with:
```python
from typing import List, Optional
```

### 1b. Add fields after line 208

Find the end of `VectorProcessingOptions` (after the `layer_name` field, line 205-208). The current code ends at:

```python
    layer_name: Optional[str] = Field(
        default=None,
        description="GeoPackage layer name to extract (defaults to first layer)"
    )


class RasterProcessingOptions(BaseProcessingOptions):
```

Insert between the `layer_name` field and the blank line before `RasterProcessingOptions`:

```python
    # Split views â€” per-value PostgreSQL views (24 FEB 2026)
    split_column: Optional[str] = Field(
        default=None,
        description=(
            "Column name to split into per-value PostgreSQL views. "
            "Must be categorical (text, integer, boolean). Max 20 distinct values."
        )
    )
    # Future: explicit value list (reserved, raises error if used)
    split_values: Optional[List[str]] = Field(
        default=None,
        description="Reserved for future use. Explicit list of split values."
    )

    @model_validator(mode='after')
    def _validate_split_options(self):
        """Block split_values until explicit-list feature is implemented."""
        if self.split_values is not None:
            raise ValueError(
                "split_values is reserved for future use. "
                "Currently only split_column with auto-discovery is supported."
            )
        return self
```

### After this change, `VectorProcessingOptions` should look like:

```python
class VectorProcessingOptions(BaseProcessingOptions):
    """..."""
    table_name: Optional[str] = Field(default=None, description="Custom PostGIS table name")
    lat_column: Optional[str] = Field(default=None, description="CSV latitude column name")
    lon_column: Optional[str] = Field(default=None, description="CSV longitude column name")
    wkt_column: Optional[str] = Field(default=None, description="CSV WKT geometry column name")
    layer_name: Optional[str] = Field(
        default=None,
        description="GeoPackage layer name to extract (defaults to first layer)"
    )
    # Split views â€” per-value PostgreSQL views (24 FEB 2026)
    split_column: Optional[str] = Field(
        default=None,
        description=(
            "Column name to split into per-value PostgreSQL views. "
            "Must be categorical (text, integer, boolean). Max 20 distinct values."
        )
    )
    # Future: explicit value list (reserved, raises error if used)
    split_values: Optional[List[str]] = Field(
        default=None,
        description="Reserved for future use. Explicit list of split values."
    )

    @model_validator(mode='after')
    def _validate_split_options(self):
        """Block split_values until explicit-list feature is implemented."""
        if self.split_values is not None:
            raise ValueError(
                "split_values is reserved for future use. "
                "Currently only split_column with auto-discovery is supported."
            )
        return self
```

### Verify Task 1

```bash
conda run -n azgeo python -c "
from core.models.processing_options import VectorProcessingOptions

# split_column works
opts = VectorProcessingOptions(split_column='admin_level')
assert opts.split_column == 'admin_level'
assert opts.split_values is None
print('split_column field: OK')

# split_values is blocked
try:
    VectorProcessingOptions(split_values=['a', 'b'])
    assert False, 'Should have raised ValueError'
except ValueError as e:
    assert 'reserved for future use' in str(e)
    print('split_values blocked: OK')

# No split_column is fine (default None)
opts2 = VectorProcessingOptions()
assert opts2.split_column is None
print('default None: OK')

print('TASK 1 COMPLETE')
"
```

---

## TASK 2: Create `services/vector/view_splitter.py` (NEW FILE)

**File**: `services/vector/view_splitter.py` â€” create this file from scratch
**Depends on**: Nothing (uses existing `column_sanitizer.py`)

Create the file with the **exact content below**. This is the core module â€” all split view logic lives here.

```python
# ============================================================================
# VECTOR VIEW SPLITTER
# ============================================================================
# STATUS: Service utility - Per-value PostgreSQL views for PostGIS tables
# PURPOSE: Split a table into filtered views based on a categorical column
# CREATED: 24 FEB 2026
# EXPORTS: validate_split_column, discover_split_values, create_split_views,
#          register_split_views, cleanup_split_view_metadata
# DEPENDENCIES: psycopg, services.vector.column_sanitizer
# ============================================================================
"""
Vector View Splitter.

After a vector table is uploaded to PostGIS, this module can split it into
multiple PostgreSQL VIEWs â€” one per distinct value in a categorical column.
Each view auto-registers in PostGIS geometry_columns and is auto-discovered
by TiPG as a separate OGC Feature collection.

Example:
    Table: geo.admin_boundaries_ord1 (columns: id, geom, admin_level, name)
    split_column: "admin_level"
    Distinct values: [0, 1, 2]

    Creates:
        VIEW geo.admin_boundaries_ord1_admin_level_0
             AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 0
        VIEW geo.admin_boundaries_ord1_admin_level_1
             AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 1
        VIEW geo.admin_boundaries_ord1_admin_level_2
             AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 2

Design:
    - Views use the base table's GIST spatial index (zero overhead)
    - CREATE OR REPLACE VIEW for idempotency
    - All views created in one transaction (atomic)
    - Catalog entries use INSERT ... ON CONFLICT DO UPDATE (idempotent)
    - DROP TABLE CASCADE on base table auto-drops all views

Exports:
    validate_split_column: Check column exists and has categorical type
    discover_split_values: Query DISTINCT values, enforce cardinality limit
    create_split_views: CREATE VIEW for each value
    register_split_views: Create geo.table_catalog entries for each view
    cleanup_split_view_metadata: Delete stale catalog entries before re-creation
"""

import json
import re
import logging
from typing import Any, Dict, List, Optional

from psycopg import sql

from util_logger import LoggerFactory, ComponentType
from services.vector.column_sanitizer import sanitize_column_name

logger = LoggerFactory.create_logger(ComponentType.SERVICE, "view_splitter")

# ============================================================================
# CONSTANTS
# ============================================================================

# Hard cap on distinct values. Rejects if exceeded.
# 20 views is generous for admin boundaries (admin0-4 = 5).
# Prevents accidental 1000-view creation on high-cardinality columns.
MAX_SPLIT_CARDINALITY = 20

# PostgreSQL NAMEDATALEN limit (63 bytes for identifiers)
PG_MAX_IDENTIFIER_LENGTH = 63

# PostgreSQL data types that are safe to split on (categorical).
# Checked against information_schema.columns.data_type and .udt_name.
ALLOWED_SPLIT_TYPES = frozenset({
    # Text types
    'text', 'character varying', 'varchar', 'character', 'char', 'bpchar',
    # Integer types
    'integer', 'int4', 'smallint', 'int2', 'bigint', 'int8',
    # Boolean
    'boolean', 'bool',
})

# Types explicitly rejected (informative error message).
REJECTED_SPLIT_TYPES_DISPLAY = (
    "float/double, numeric/decimal, geometry, geography, "
    "jsonb, json, bytea, uuid, timestamp, date, time"
)


# ============================================================================
# INTERNAL HELPERS
# ============================================================================

def _sanitize_view_name_segment(value: Any) -> str:
    """
    Convert a split column value to a safe PostgreSQL identifier segment.

    Rules:
        1. str(value).lower()
        2. Replace non-alphanumeric with underscore
        3. Collapse consecutive underscores, strip leading/trailing
        4. Fallback to 'val' if empty

    Examples:
        >>> _sanitize_view_name_segment(0)
        '0'
        >>> _sanitize_view_name_segment("New York")
        'new_york'
        >>> _sanitize_view_name_segment(True)
        'true'
        >>> _sanitize_view_name_segment("@#$")
        'val'
    """
    text = str(value).lower()
    text = re.sub(r'[^a-z0-9_]', '_', text)
    text = re.sub(r'_+', '_', text).strip('_')
    return text if text else 'val'


def _compute_view_name(table_name: str, split_column: str, value: Any) -> str:
    """
    Compute the view name: {table}_{column}_{value}, truncated to 63 chars.

    Truncation strategy (preserves readability):
        1. If full name fits in 63 chars, use it
        2. Otherwise, shorten table_name prefix (keep column+value)
        3. If suffix alone > 55 chars, shorten value too
        4. Raise ValueError if impossible (pathological)

    Args:
        table_name: Base table name (e.g., 'admin_boundaries_ord1')
        split_column: Column name (e.g., 'admin_level')
        value: Column value (e.g., 0)

    Returns:
        View name fitting within 63 characters
    """
    sanitized_value = _sanitize_view_name_segment(value)
    suffix = f"_{split_column}_{sanitized_value}"
    ideal = f"{table_name}{suffix}"

    if len(ideal) <= PG_MAX_IDENTIFIER_LENGTH:
        return ideal

    # Truncate table_name, preserve suffix
    max_table_len = PG_MAX_IDENTIFIER_LENGTH - len(suffix)
    if max_table_len >= 8:
        truncated = table_name[:max_table_len].rstrip('_')
        return f"{truncated}{suffix}"

    # Suffix too long â€” truncate value segment too
    max_val_len = PG_MAX_IDENTIFIER_LENGTH - len(table_name) - len(split_column) - 2
    if max_val_len < 3:
        raise ValueError(
            f"Cannot generate view name within 63-char limit for "
            f"table='{table_name}', column='{split_column}', value='{value}'"
        )
    sanitized_value = sanitized_value[:max_val_len]
    return f"{table_name}_{split_column}_{sanitized_value}"


# ============================================================================
# PUBLIC API
# ============================================================================

def validate_split_column(
    conn,
    table_name: str,
    schema: str,
    split_column: str
) -> Dict[str, Any]:
    """
    Validate that split_column exists in the table and has a categorical type.

    The split_column should already be sanitized (lowercase, cleaned) by the
    caller using sanitize_column_name() â€” this matches post-upload column names.

    Args:
        conn: Open psycopg connection (dict_row cursor factory)
        table_name: Base table name (already created in PostGIS)
        schema: Schema name (e.g., 'geo')
        split_column: Column name to validate (post-sanitization)

    Returns:
        {'column_name': str, 'data_type': str}

    Raises:
        ValueError: Column not found, or type not categorical
    """
    with conn.cursor() as cur:
        cur.execute("""
            SELECT column_name, data_type, udt_name
            FROM information_schema.columns
            WHERE table_schema = %s AND table_name = %s
            ORDER BY ordinal_position
        """, (schema, table_name))
        columns = {row['column_name']: row for row in cur.fetchall()}

    # Reserved columns the user should never split on
    reserved = {'id', 'geom', 'geometry', 'etl_batch_id'}

    if split_column not in columns:
        available = sorted(c for c in columns if c not in reserved)
        raise ValueError(
            f"Split column '{split_column}' not found in {schema}.{table_name}. "
            f"Available columns (post-sanitization): {available}. "
            f"Note: column names are sanitized during upload â€” "
            f"'Type' becomes 'f_type', 'Feature Name' becomes 'feature_name'."
        )

    if split_column in reserved:
        raise ValueError(
            f"Cannot split on reserved column '{split_column}'. "
            f"Choose a data column, not a system column."
        )

    col_info = columns[split_column]
    pg_type = col_info['data_type']
    udt_name = col_info['udt_name']

    if pg_type not in ALLOWED_SPLIT_TYPES and udt_name not in ALLOWED_SPLIT_TYPES:
        raise ValueError(
            f"Split column '{split_column}' has type '{pg_type}' which is not categorical. "
            f"Allowed types: text, varchar, integer, smallint, bigint, boolean. "
            f"Not allowed: {REJECTED_SPLIT_TYPES_DISPLAY}."
        )

    logger.info(f"Split column '{split_column}' validated: type={pg_type}")
    return {'column_name': split_column, 'data_type': pg_type}


def discover_split_values(
    conn,
    table_name: str,
    schema: str,
    split_column: str,
    max_cardinality: int = MAX_SPLIT_CARDINALITY
) -> List[Any]:
    """
    Query DISTINCT non-NULL values from the split column.

    Two-step: COUNT(DISTINCT) first (cheap), then SELECT DISTINCT (bounded).

    Args:
        conn: Open psycopg connection
        table_name: Base table name
        schema: Schema name
        split_column: Validated column name
        max_cardinality: Maximum distinct values (default: 20)

    Returns:
        Sorted list of distinct non-NULL values

    Raises:
        ValueError: All NULL, or cardinality exceeds limit
    """
    with conn.cursor() as cur:
        # Cheap cardinality check
        cur.execute(
            sql.SQL(
                "SELECT COUNT(DISTINCT {col}) AS cnt "
                "FROM {schema}.{table} WHERE {col} IS NOT NULL"
            ).format(
                col=sql.Identifier(split_column),
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name),
            )
        )
        count = cur.fetchone()['cnt']

        if count == 0:
            raise ValueError(
                f"Split column '{split_column}' contains only NULL values "
                f"in {schema}.{table_name}. Cannot create views."
            )

        if count > max_cardinality:
            raise ValueError(
                f"Split column '{split_column}' has {count} distinct values, "
                f"exceeding the maximum of {max_cardinality}. "
                f"Use a column with fewer categories."
            )

        # Fetch actual values (bounded by max_cardinality)
        cur.execute(
            sql.SQL(
                "SELECT DISTINCT {col} FROM {schema}.{table} "
                "WHERE {col} IS NOT NULL ORDER BY {col}"
            ).format(
                col=sql.Identifier(split_column),
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name),
            )
        )
        values = [row[split_column] for row in cur.fetchall()]

    logger.info(f"Discovered {len(values)} distinct values for '{split_column}': {values}")
    return values


def create_split_views(
    conn,
    table_name: str,
    schema: str,
    split_column: str,
    values: List[Any],
) -> List[Dict[str, Any]]:
    """
    Create one PostgreSQL VIEW per distinct value.

    Uses CREATE OR REPLACE VIEW for idempotency.
    All views created in caller's transaction (atomic â€” commit externally).

    SQL pattern (safe â€” no string interpolation):
        CREATE OR REPLACE VIEW {schema}.{view} AS
        SELECT * FROM {schema}.{table}
        WHERE {col} = {val}

    Args:
        conn: Open psycopg connection (caller manages commit)
        table_name: Base table name
        schema: Schema name
        split_column: Column to filter on
        values: List of distinct values

    Returns:
        List of dicts: [{'view_name': str, 'value': Any, 'qualified_name': str}]
    """
    views_created = []

    with conn.cursor() as cur:
        for value in values:
            view_name = _compute_view_name(table_name, split_column, value)

            logger.info(f"Creating view {schema}.{view_name} WHERE {split_column} = {value!r}")

            cur.execute(
                sql.SQL(
                    "CREATE OR REPLACE VIEW {schema}.{view} AS "
                    "SELECT * FROM {schema}.{table} "
                    "WHERE {col} = {val}"
                ).format(
                    schema=sql.Identifier(schema),
                    view=sql.Identifier(view_name),
                    table=sql.Identifier(table_name),
                    col=sql.Identifier(split_column),
                    val=sql.Literal(value),
                )
            )

            views_created.append({
                'view_name': view_name,
                'value': value,
                'qualified_name': f"{schema}.{view_name}",
            })

    logger.info(f"Created {len(views_created)} split views for {schema}.{table_name}")
    return views_created


def register_split_views(
    conn,
    views: List[Dict[str, Any]],
    base_table_name: str,
    schema: str,
    split_column: str,
    base_title: Optional[str] = None,
    geometry_type: Optional[str] = None,
    srid: int = 4326,
) -> int:
    """
    Register each split view in geo.table_catalog.

    Each view gets:
        - title: "{base_title} â€” {column} {value}"
        - feature_count: COUNT(*) on the view
        - bbox: ST_Extent(geom) on the view
        - table_type: 'split_view'
        - custom_properties: {split_view, base_table, split_column, split_value}

    Uses INSERT ... ON CONFLICT DO UPDATE for idempotency.

    Args:
        conn: Open psycopg connection (caller manages commit)
        views: List from create_split_views()
        base_table_name: Original base table name
        schema: Schema name
        split_column: Column used for splitting
        base_title: Base table title (for deriving view titles)
        geometry_type: Geometry type of base table
        srid: SRID (default: 4326)

    Returns:
        Number of catalog entries created
    """
    registered = 0

    with conn.cursor() as cur:
        for view_info in views:
            view_name = view_info['view_name']
            value = view_info['value']

            # Feature count
            cur.execute(
                sql.SQL("SELECT COUNT(*) AS cnt FROM {schema}.{view}").format(
                    schema=sql.Identifier(schema),
                    view=sql.Identifier(view_name),
                )
            )
            feature_count = cur.fetchone()['cnt']

            # Bounding box
            cur.execute(
                sql.SQL(
                    "SELECT "
                    "ST_XMin(ext) AS minx, ST_YMin(ext) AS miny, "
                    "ST_XMax(ext) AS maxx, ST_YMax(ext) AS maxy "
                    "FROM (SELECT ST_Extent({geom}) AS ext "
                    "FROM {schema}.{view}) sub"
                ).format(
                    geom=sql.Identifier('geom'),
                    schema=sql.Identifier(schema),
                    view=sql.Identifier(view_name),
                )
            )
            bbox = cur.fetchone()

            # Derive title
            value_label = str(value)
            view_title = (
                f"{base_title} â€” {split_column} {value_label}"
                if base_title
                else f"{base_table_name} â€” {split_column} {value_label}"
            )

            custom_props = json.dumps({
                'split_view': True,
                'base_table': base_table_name,
                'split_column': split_column,
                'split_value': value,
            })

            cur.execute("""
                INSERT INTO geo.table_catalog (
                    table_name, schema_name, title,
                    geometry_type, srid, feature_count,
                    bbox_minx, bbox_miny, bbox_maxx, bbox_maxy,
                    table_type, custom_properties,
                    created_at, updated_at
                ) VALUES (
                    %s, %s, %s,
                    %s, %s, %s,
                    %s, %s, %s, %s,
                    'split_view', %s,
                    NOW(), NOW()
                )
                ON CONFLICT (table_name) DO UPDATE SET
                    title = EXCLUDED.title,
                    geometry_type = EXCLUDED.geometry_type,
                    feature_count = EXCLUDED.feature_count,
                    bbox_minx = EXCLUDED.bbox_minx,
                    bbox_miny = EXCLUDED.bbox_miny,
                    bbox_maxx = EXCLUDED.bbox_maxx,
                    bbox_maxy = EXCLUDED.bbox_maxy,
                    table_type = EXCLUDED.table_type,
                    custom_properties = EXCLUDED.custom_properties,
                    updated_at = NOW()
            """, (
                view_name, schema, view_title,
                geometry_type, srid, feature_count,
                bbox['minx'], bbox['miny'], bbox['maxx'], bbox['maxy'],
                custom_props,
            ))

            registered += 1
            logger.info(
                f"Registered catalog: {view_name} "
                f"({feature_count} features, "
                f"bbox=[{bbox['minx']:.4f},{bbox['miny']:.4f},"
                f"{bbox['maxx']:.4f},{bbox['maxy']:.4f}])"
            )

    logger.info(f"Registered {registered} split view catalog entries for {schema}.{base_table_name}")
    return registered


def cleanup_split_view_metadata(conn, base_table_name: str) -> int:
    """
    Delete geo.table_catalog entries for split views of a base table.

    Called:
        - Before re-creating views (overwrite scenario)
        - When base table is dropped without split_column on new upload

    The actual PostgreSQL views are dropped by DROP TABLE CASCADE on the
    base table. This function only cleans up the metadata.

    Args:
        conn: Open psycopg connection
        base_table_name: Base table name

    Returns:
        Number of catalog entries deleted
    """
    with conn.cursor() as cur:
        cur.execute("""
            DELETE FROM geo.table_catalog
            WHERE table_type = 'split_view'
            AND custom_properties->>'base_table' = %s
            RETURNING table_name
        """, (base_table_name,))
        deleted = cur.fetchall()

    count = len(deleted)
    if count > 0:
        names = [r['table_name'] for r in deleted]
        logger.info(f"Cleaned up {count} stale split view catalog entries: {names}")
    return count


# ============================================================================
# MODULE EXPORTS
# ============================================================================

__all__ = [
    'validate_split_column',
    'discover_split_values',
    'create_split_views',
    'register_split_views',
    'cleanup_split_view_metadata',
    'MAX_SPLIT_CARDINALITY',
    'ALLOWED_SPLIT_TYPES',
]
```

### Verify Task 2

```bash
conda run -n azgeo python -c "
from services.vector.view_splitter import (
    _sanitize_view_name_segment,
    _compute_view_name,
    ALLOWED_SPLIT_TYPES,
    MAX_SPLIT_CARDINALITY,
)

# Name segment sanitization
assert _sanitize_view_name_segment(0) == '0'
assert _sanitize_view_name_segment('New York') == 'new_york'
assert _sanitize_view_name_segment(True) == 'true'
assert _sanitize_view_name_segment(False) == 'false'
assert _sanitize_view_name_segment('@#\$') == 'val'
assert _sanitize_view_name_segment('Level-0') == 'level_0'
print('_sanitize_view_name_segment: OK')

# View name computation
assert _compute_view_name('tbl', 'col', 1) == 'tbl_col_1'
assert _compute_view_name('admin_boundaries_ord1', 'admin_level', 0) == 'admin_boundaries_ord1_admin_level_0'
assert len(_compute_view_name('a' * 60, 'col', 1)) <= 63
print('_compute_view_name: OK')

# Constants
assert MAX_SPLIT_CARDINALITY == 20
assert 'text' in ALLOWED_SPLIT_TYPES
assert 'integer' in ALLOWED_SPLIT_TYPES
assert 'boolean' in ALLOWED_SPLIT_TYPES
assert 'double precision' not in ALLOWED_SPLIT_TYPES
assert 'geometry' not in ALLOWED_SPLIT_TYPES
print('Constants: OK')

print('TASK 2 COMPLETE')
"
```

---

## TASK 3: Register exports in `services/vector/__init__.py`

**File**: `services/vector/__init__.py`
**Depends on**: Task 2 (file must exist)

### 3a. Add import after the existing `from .core import (...)` block

Find this line (currently line 39):
```python
)
```
(the closing paren of the `from .core import (...)` block)

Add after it:
```python
# Split views (24 FEB 2026)
from .view_splitter import (
    create_split_views,
    register_split_views,
    cleanup_split_view_metadata,
)
```

### 3b. Add to `__all__` list

Find the end of the `__all__` list. Currently ends with:
```python
    'log_gdf_memory',
]
```

Change to:
```python
    'log_gdf_memory',
    # Split views
    'create_split_views',
    'register_split_views',
    'cleanup_split_view_metadata',
]
```

### Verify Task 3

```bash
conda run -n azgeo python -c "
from services.vector import create_split_views, register_split_views, cleanup_split_view_metadata
print('Imports from package: OK')
print('TASK 3 COMPLETE')
"
```

---

## TASK 4: Pass `split_column` through platform translation

**File**: `services/platform_translation.py`
**Depends on**: Task 1 (field must exist on VectorProcessingOptions)

### 4a. Add to the returned job_params dict

Find this section (around line 265-271):
```python
            # Processing options
            'converter_params': converter_params,
            'overwrite': opts.overwrite,

            # GPKG layer selection (24 FEB 2026)
            'layer_name': opts.layer_name,
        }
```

Change to:
```python
            # Processing options
            'converter_params': converter_params,
            'overwrite': opts.overwrite,

            # GPKG layer selection (24 FEB 2026)
            'layer_name': opts.layer_name,

            # Split views (24 FEB 2026)
            'split_column': opts.split_column,
        }
```

### Verify Task 4

```bash
conda run -n azgeo python -c "
from services.platform_translation import translate_to_coremachine
from core.models.platform import PlatformRequest
from config import get_config

config = get_config()

# Create a minimal vector request with split_column
req = PlatformRequest(
    dataset_id='test-dataset',
    resource_id='test-resource',
    container_name='bronze-vectors',
    file_name='test.geojson',
    processing_options={'split_column': 'admin_level'}
)
job_type, params = translate_to_coremachine(req, config)
assert params.get('split_column') == 'admin_level', f'Expected admin_level, got {params.get(\"split_column\")}'
print(f'job_type={job_type}, split_column={params[\"split_column\"]}')
print('TASK 4 COMPLETE')
"
```

---

## TASK 5: Add `split_column` to job schema + task params

**File**: `jobs/vector_docker_etl.py`
**Depends on**: Task 1 (so translation passes it through)

### 5a. Add to `parameters_schema` dict

Find the `layer_name` entry (around line 147-151):
```python
        'layer_name': {
            'type': 'str',
            'default': None,
            'description': 'GeoPackage layer name to extract (optional, uses first layer if omitted)'
        },
```

Add immediately after:
```python
        'split_column': {
            'type': 'str',
            'default': None,
            'description': 'Column name to split into per-value views (categorical only, max 20 values)'
        },
```

### 5b. Add to `create_tasks_for_stage()` task parameters

Find the Processing section in the task params dict (around line 390-394):
```python
                # Processing
                'chunk_size': job_params.get('chunk_size', 20000),
                'indexes': job_params.get('indexes', {'spatial': True, 'attributes': [], 'temporal': []}),
                'create_tile_view': job_params.get('create_tile_view', False),
                'max_tile_vertices': job_params.get('max_tile_vertices', 256),
```

Add after `'max_tile_vertices'`:
```python
                # Split views
                'split_column': job_params.get('split_column'),
```

### Verify Task 5

```bash
conda run -n azgeo python -c "
from jobs.vector_docker_etl import VectorDockerETLJob

# Check schema has split_column
schema = VectorDockerETLJob.parameters_schema
assert 'split_column' in schema, 'split_column not in parameters_schema'
assert schema['split_column']['type'] == 'str'
print('parameters_schema: OK')

# Check task params include split_column
tasks = VectorDockerETLJob.create_tasks_for_stage(
    stage=1,
    job_params={
        'blob_name': 'test.geojson',
        'file_extension': 'geojson',
        'table_name': 'test_table',
        'split_column': 'admin_level',
    },
    job_id='abcd1234-test-0000-0000-000000000000',
)
assert tasks[0]['parameters'].get('split_column') == 'admin_level'
print('create_tasks_for_stage: OK')

print('TASK 5 COMPLETE')
"
```

---

## TASK 6: Integrate Phase 3.7 into handler

**File**: `services/handler_vector_docker_complete.py`
**Depends on**: Task 2 (view_splitter.py), Task 5 (split_column in task params)

### 6a. Insert Phase 3.7 block

Find this exact code (lines 255-259):
```python
        logger.info(f"[{job_id[:8]}] Phase 3.5: Running ANALYZE for query planner")
        _post_handler.analyze_table(table_name, schema)
        checkpoint("table_analyzed", {"table": f"{schema}.{table_name}"})

        # =====================================================================
        # PHASE 4: Refresh TiPG Collection Catalog (05 FEB 2026 - F1.6)
```

Insert between the `checkpoint("table_analyzed", ...)` line and the Phase 4 comment:

```python
        # =====================================================================
        # PHASE 3.7: Create Split Views (if split_column specified)
        # =====================================================================
        # When the user specifies split_column="admin_level", we:
        # 1. Validate the column exists and is categorical (text/int/bool)
        # 2. Query DISTINCT values (max 20)
        # 3. CREATE VIEW per value (e.g., WHERE admin_level = 0)
        # 4. Register each view in geo.table_catalog
        # Views auto-register in geometry_columns -> TiPG discovers them.
        # =====================================================================
        split_column = parameters.get('split_column')
        split_views_result = None

        if split_column:
            logger.info(f"[{job_id[:8]}] Phase 3.7: Creating split views on column '{split_column}'")

            from services.vector.view_splitter import (
                validate_split_column,
                discover_split_values,
                create_split_views,
                register_split_views,
                cleanup_split_view_metadata,
            )
            from services.vector.column_sanitizer import sanitize_column_name as _sanitize_col

            # Normalize user's column name to post-sanitization form
            # e.g., "Type" -> "f_type", "Feature Name" -> "feature_name"
            split_column_sanitized = _sanitize_col(split_column)
            if split_column_sanitized != split_column:
                logger.info(
                    f"[{job_id[:8]}] Split column sanitized: "
                    f"'{split_column}' -> '{split_column_sanitized}'"
                )

            with _post_handler._pg_repo._get_connection() as conn:
                # Step 1: Validate column exists and has categorical type
                col_info = validate_split_column(
                    conn, table_name, schema, split_column_sanitized
                )
                logger.info(
                    f"[{job_id[:8]}] Split column validated: "
                    f"'{split_column_sanitized}' type={col_info['data_type']}"
                )

                # Step 2: Discover distinct values (enforces cardinality limit)
                values = discover_split_values(
                    conn, table_name, schema, split_column_sanitized
                )
                logger.info(f"[{job_id[:8]}] Found {len(values)} distinct values for split")

                # Step 3: Clean up stale catalog entries (handles overwrite scenario)
                cleanup_split_view_metadata(conn, table_name)

                # Step 4: Create views (atomic - all-or-nothing in this transaction)
                views = create_split_views(
                    conn, table_name, schema, split_column_sanitized, values
                )

                # Step 5: Register catalog entries (title, bbox, feature_count per view)
                registered = register_split_views(
                    conn=conn,
                    views=views,
                    base_table_name=table_name,
                    schema=schema,
                    split_column=split_column_sanitized,
                    base_title=parameters.get('title'),
                    geometry_type=table_result['geometry_type'],
                    srid=table_result.get('srid', 4326),
                )

                conn.commit()

            split_views_result = {
                'split_column': split_column_sanitized,
                'values': [str(v) for v in values],
                'views_created': len(views),
                'catalog_entries': registered,
                'view_names': [v['view_name'] for v in views],
            }
            checkpoint("split_views_created", split_views_result)

```

### 6b. Add `split_views` to return dict

Find the return dict (around line 416-433). Find this line:
```python
                "data_warnings": data_warnings if data_warnings else None,
```

Add after it:
```python
                "split_views": split_views_result,
```

### Error handling note

No additional error handling is needed. `ValueError` from validate/discover propagates to the handler's existing `except Exception` at line 435, which maps to an `ErrorCode` and returns a structured error response to the user.

---

## TASK 7: Fix DROP TABLE to CASCADE + cleanup metadata on overwrite

**File**: `services/vector/postgis_handler.py`
**Depends on**: Task 2 (cleanup_split_view_metadata must exist)

### 7a. Change DROP TABLE to include CASCADE and metadata cleanup

Find this exact code (lines 1488-1493):
```python
                    # Drop existing table (overwrite=true)
                    logger.info(f"ðŸ—‘ï¸ Dropping existing table {schema}.{table_name} (overwrite=true)")
                    cur.execute(sql.SQL("DROP TABLE {schema}.{table}").format(
                        schema=sql.Identifier(schema),
                        table=sql.Identifier(table_name)
                    ))
```

Replace with:
```python
                    # Drop existing table + dependent views (overwrite=true)
                    logger.info(f"ðŸ—‘ï¸ Dropping existing table {schema}.{table_name} CASCADE (overwrite=true)")
                    # Clean up split view catalog entries before DROP
                    try:
                        from services.vector.view_splitter import cleanup_split_view_metadata
                        cleanup_split_view_metadata(conn, table_name)
                    except Exception as cleanup_err:
                        logger.warning(f"Split view metadata cleanup failed (non-fatal): {cleanup_err}")
                    cur.execute(sql.SQL("DROP TABLE IF EXISTS {schema}.{table} CASCADE").format(
                        schema=sql.Identifier(schema),
                        table=sql.Identifier(table_name)
                    ))
```

**Why CASCADE**: Without it, dropping a base table with dependent views would fail with `ERROR: cannot drop table ... because other objects depend on it`.

**Why cleanup_split_view_metadata**: CASCADE drops PostgreSQL views but does NOT touch `geo.table_catalog`. Without this cleanup, stale catalog entries for old views would remain.

**Why try/except**: The cleanup is non-fatal. If it fails (e.g., first-ever upload, no catalog entries), the DROP should still proceed.

---

## FILES MODIFIED â€” SUMMARY

| Task | File | Action |
|------|------|--------|
| 1 | `core/models/processing_options.py` | Add `split_column` + `split_values` fields + validator |
| 2 | `services/vector/view_splitter.py` | **NEW FILE** â€” all split view logic |
| 3 | `services/vector/__init__.py` | Add imports + `__all__` entries |
| 4 | `services/platform_translation.py` | Pass `split_column` to job_params |
| 5 | `jobs/vector_docker_etl.py` | Add to schema + task params |
| 6 | `services/handler_vector_docker_complete.py` | Phase 3.7 block + return dict entry |
| 7 | `services/vector/postgis_handler.py` | DROP TABLE CASCADE + metadata cleanup |

## FILES NOT MODIFIED (and why)

| File | Reason |
|------|--------|
| `triggers/platform/submit.py` | No preflight needed â€” column validated post-upload in handler |
| `services/vector/converters.py` | No converter changes â€” views are post-upload |
| `services/vector/core.py` | No core.py changes |
| `core/models/geo.py` | `table_type` is already a flexible string field â€” `'split_view'` works |
| `core/schema.py` | No DDL changes â€” views created via raw SQL |
| `services/unpublish_handlers.py` | Already uses CASCADE; views auto-dropped |

---

## DATA FLOW DIAGRAM

```
POST /api/platform/submit
  { processing_options: { split_column: "admin_level" } }
    â”‚
    â”œâ”€â”€ VectorProcessingOptions.split_column = "admin_level"       (Task 1)
    â”œâ”€â”€ platform_translation.py -> job_params['split_column']      (Task 4)
    â”œâ”€â”€ vector_docker_etl.py -> task params -> handler             (Task 5)
    â”‚
    â–¼ handler_vector_docker_complete.py
    Phase 1: Load source -> gdf with columns [id, geom, admin_level, name, ...]
    Phase 2: CREATE TABLE geo.admin_boundaries_ord1
    Phase 3: Upload chunks
    Phase 3.5: Indexes + ANALYZE
    Phase 3.7: Split Views  <-- NEW (Task 6)
      â”œâ”€â”€ sanitize_column_name("admin_level") -> "admin_level"
      â”œâ”€â”€ validate: exists? YES  type=integer? YES
      â”œâ”€â”€ discover: SELECT DISTINCT admin_level -> [0, 1, 2]
      â”œâ”€â”€ CREATE VIEW geo.admin_boundaries_ord1_admin_level_0     (Task 2)
      â”‚     AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 0
      â”œâ”€â”€ CREATE VIEW geo.admin_boundaries_ord1_admin_level_1
      â”‚     AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 1
      â”œâ”€â”€ CREATE VIEW geo.admin_boundaries_ord1_admin_level_2
      â”‚     AS SELECT * FROM geo.admin_boundaries_ord1 WHERE admin_level = 2
      â””â”€â”€ INSERT INTO geo.table_catalog x 3 (table_type='split_view')
    Phase 4: TiPG refresh -> discovers base table + 3 views
    â–¼
    TiPG OGC Feature collections:
      geo.admin_boundaries_ord1                (all rows)
      geo.admin_boundaries_ord1_admin_level_0  (level 0 only)
      geo.admin_boundaries_ord1_admin_level_1  (level 1 only)
      geo.admin_boundaries_ord1_admin_level_2  (level 2 only)
```

---

## EDGE CASES

| Scenario | Expected Behavior |
|----------|-------------------|
| User says `split_column: "Type"` | Sanitized to `"f_type"`, looked up in DB â€” works |
| User says `split_column: "nonexistent"` | ValueError listing available columns |
| Column is float/geometry/json | ValueError: "type 'double precision' not categorical" |
| All values NULL | ValueError: "contains only NULL values" |
| >20 distinct values | ValueError: "{n} distinct values, exceeding max 20" |
| Values with spaces ("New York") | View name segment -> "new_york" |
| View name > 63 chars | Truncated (table prefix shortened first) |
| Overwrite resubmit with split | CASCADE drops old views, cleanup removes catalog, new views created |
| Resubmit WITHOUT split_column | CASCADE on DROP removes old views; cleanup removes catalog entries |
| Partial view creation failure | Transaction rollback â€” no views created, base table unaffected |
| `split_values` provided | Immediate ValueError: "reserved for future use" |
| `split_column` not provided | Phase 3.7 skipped entirely (no-op) |
| Reserved column (id, geom) | ValueError: "Cannot split on reserved column" |

---

## VERIFICATION (After All Tasks Complete)

### Quick smoke test (no DB required)

```bash
conda run -n azgeo python -c "
# Task 1: processing_options
from core.models.processing_options import VectorProcessingOptions
opts = VectorProcessingOptions(split_column='admin_level')
assert opts.split_column == 'admin_level'

# Task 2: view_splitter module
from services.vector.view_splitter import _sanitize_view_name_segment, _compute_view_name
assert _sanitize_view_name_segment(0) == '0'
assert _sanitize_view_name_segment('New York') == 'new_york'
assert len(_compute_view_name('a'*60, 'col', 1)) <= 63

# Task 3: package exports
from services.vector import create_split_views, register_split_views, cleanup_split_view_metadata

print('ALL IMPORT TESTS PASSED')
"
```

### Existing test suite

```bash
conda run -n azgeo python -m pytest test/ -v -x -k "platform or vector"
```

### End-to-end integration test (requires deployed service)

1. Upload a GeoJSON with a categorical column + `split_column` in processing_options
2. Check job completes with `split_views` in result (non-null, views_created > 0)
3. Query each view via OGC Features -> verify filtered data
4. Verify `geo.table_catalog` has entries with `table_type='split_view'`
5. Resubmit with `overwrite=true` -> verify old views replaced
6. Submit with `split_column` pointing at float column -> verify rejection
7. Submit with `split_column` pointing at column with >20 values -> verify rejection
