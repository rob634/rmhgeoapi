# ADO Work Items - Geospatial API for DDH

**Created**: 30 JAN 2026
**Revised**: 02 FEB 2026
**Purpose**: Work items for ADO REST API import
**Related**: V0.8_ADO_REFERENCE.md (wiki/reference content)

---

## Import Conventions

- `[CLOSED]` = Import as Closed state
- `[ACTIVE]` = Import as Active state
- `[NEW]` = Import as New state
- `[ENABLER: Type]` = Create as Enabler work item type
- `[EXPLORATION]` = Create as Spike work item type
- Tasks listed under stories = Create as Task work items linked to parent

---

# EPIC: Geospatial API for DDH

**Description**: Cloud-native geospatial data platform enabling DDH (Data Hub Dashboard) to publish and consume vector/raster datasets via modern APIs.

**Tags**: OGC, Zero-Trust, Cloud-Native, 12-Factor

---

## FEATURE 1: ETL Pipeline Orchestration `[ACTIVE]`

**Description**: Job orchestration engine powering all data processing. Enables reliable, scalable ETL without manual intervention. Includes standard pipelines and specialized large dataset processing.

**Parent**: EPIC: Geospatial API for DDH

---

### US 1.1: Serverless Job Orchestration `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**As a** platform operator (Platform Ops - Tier 1)
**I want** serverless job orchestration via Azure Functions and Service Bus
**So that** ETL pipelines execute reliably without infrastructure management

**Acceptance Criteria**:
- [x] Job→Stage→Task workflow pattern
- [x] Queue-based workload separation (jobs, container-tasks, functionapp-tasks)
- [x] Docker worker for heavy GDAL operations with checkpoint/resume
- [ ] pg_cron enabled for scheduled database maintenance (QA/UAT/PROD)

#### Tasks

| ID | Description | Status | Reference |
|----|-------------|--------|-----------|
| T1.1.1 | Enable pg_cron extension (DEV) | Done | [ESERVICE_PG_CRON_REQUEST.md](operations/ESERVICE_PG_CRON_REQUEST.md) |
| T1.1.2 | Run pg_cron SQL setup (DEV) | Ready | See reference doc - SQL section |
| T1.1.3 | Submit eService request for pg_cron (QA) | Not started | See reference doc - eService template |
| T1.1.4 | Submit eService request for pg_cron (UAT) | Not started | See reference doc - eService template |
| T1.1.5 | Submit eService request for pg_cron (PROD) | Not started | See reference doc - eService template |

---

### US 1.3: Job Lifecycle `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**As a** platform operator (Platform Ops - Tier 1)
**I want** complete job status tracking
**So that** I can monitor pipeline health

**Acceptance Criteria**:
- [x] Job/task status in database
- [x] Approval record auto-created on completion
- [x] Job resubmit capability

---

### US 1.5: VirtualiZarr NetCDF Pipeline `[NEW]`

**Type**: User Story
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**As a** climate data manager (B2B - DEC climate analysts)
**I want** NetCDF files served without conversion
**So that** I avoid duplicating TB of data and can immediately access existing NetCDF collections

**Acceptance Criteria**:
- [ ] Kerchunk JSON references generated for NetCDF files
- [ ] TiTiler-xarray can read references and serve tiles
- [ ] STAC collection created with VirtualiZarr asset links
- [ ] Performance validation vs. COG conversion approach

---

### EN 1.2: Metadata Architecture `[CLOSED]`

**Type**: Enabler
**Enabler Type**: Architecture
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**Purpose**: Foundation for data provenance tracking and audit trails

**Deliverables**:
- VectorMetadata and RasterMetadata schemas
- geo.table_catalog and app.cog_metadata tables populated
- Metadata populated on all ETL operations

**Enables**: Future provenance audit UI

---

### EN 1.6: DAG Orchestration Migration `[ACTIVE]`

**Type**: Enabler
**Enabler Type**: Architecture
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**Purpose**: Refactor orchestration engine to support complex pipeline dependencies

**Rationale**: Current linear stage progression limits pipeline expressiveness. DAG-based execution enables conditional branching and parallel paths required for future advanced workflows.

**Acceptance Criteria**:
- [ ] DAG workflow engine replacing linear stage progression
- [ ] Conditional branching based on task results
- [ ] Parallel execution paths within single job

**Enables**: Future complex pipelines (multi-source joins, conditional processing)

---

### SPIKE 1.4: FATHOM Complex Raster Processing `[CLOSED]`

**Type**: Spike
**Parent**: FEATURE 1: ETL Pipeline Orchestration

**Purpose**: Demonstrate platform capability for complex multi-band raster processing

**Context**: DEC climate analysts requested FATHOM flood data hosting. Requirements satisfied by standard raster pipeline (F2) + TiTiler endpoints (F5). This spike explores advanced processing patterns (band stacking, spatial merge) applicable to similar large datasets.

**Outcomes**:
- [x] Band stacking (8 return periods → 1 COG) - Proven on Rwanda
- [x] Spatial merge (N tiles → regional COG) - Proven on Rwanda
- [ ] STAC collection with datacube extension - If pursued
- [ ] Full FATHOM inventory processed - If client funds

---

## FEATURE 2: Raster Data Pipeline `[CLOSED]`

**Description**: Transform uploaded GeoTIFFs into cloud-optimized format for serving. Enables raster data hosting via modern tile and data APIs.

**Parent**: EPIC: Geospatial API for DDH

---

### US 2.1: Raster ETL Pipeline `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 2: Raster Data Pipeline

**As a** data publisher (B2B - DDH team, climate data managers)
**I want** to upload a GeoTIFF and have it converted to cloud-native format
**So that** it can be served as web map tiles and accessed directly

**Acceptance Criteria**:
- [x] Converts to OGC Cloud Optimized GeoTIFF (COG) standard
- [x] Handles large rasters (>1GB) via tiling pipeline
- [x] Enables HTTP range requests (no full download required)

---

### US 2.2: Raster STAC Registration `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 2: Raster Data Pipeline

**As a** data consumer (B2C - Internal staff for OUO, public for PUBLIC data)
**I want** raster datasets in the STAC catalog
**So that** I can discover and access them programmatically

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog with COG asset links
- [x] Discoverable via standard STAC API queries

---

### US 2.3: Raster Unpublish `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 2: Raster Data Pipeline

**As a** data manager (B2B - Data stewards)
**I want** to remove published raster datasets
**So that** storage is reclaimed and outdated data removed

**Acceptance Criteria**:
- [x] Deletes blob, STAC item, and metadata records

---

## FEATURE 3: Vector Data Pipeline `[CLOSED]`

**Description**: Transform uploaded vector files into PostGIS tables ready for serving. Enables data publishers to ingest vector data for API consumption. Supported formats: CSV (lat/lon or WKT), KML, KMZ, Shapefile (zipped), GeoJSON, GeoPackage (.gpkg). Future: GeoParquet.

**Parent**: EPIC: Geospatial API for DDH

---

### US 3.1: Vector ETL Pipeline `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 3: Vector Data Pipeline

**As a** data publisher (B2B - DDH team, DEC developers)
**I want** to upload vector files and have them processed automatically
**So that** they become available as queryable datasets

**Acceptance Criteria**:
- [x] Accepts CSV, KML, KMZ, Shapefile, GeoJSON, GeoPackage
- [x] Corrects geometry and topology errors for PostGIS data model
- [x] Transforms to EPSG:4326 and bulk loads to PostGIS

---

### US 3.2: Vector STAC Registration `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 3: Vector Data Pipeline

**As a** data consumer (B2C - Internal staff for OUO, public for PUBLIC data)
**I want** vector datasets registered in the STAC catalog
**So that** I can discover them using standard tools

**Acceptance Criteria**:
- [x] Registered in OGC STAC catalog (same standard used by AWS, Microsoft Planetary Computer)

---

### US 3.3: Vector Unpublish `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 3: Vector Data Pipeline

**As a** data manager (B2B - Data stewards)
**I want** to remove published vector datasets
**So that** outdated data doesn't remain accessible

**Acceptance Criteria**:
- [x] Drops PostGIS table, deletes STAC item, removes catalog entry

---

## FEATURE 4: Data Governance & Classification `[ACTIVE]`

**Description**: Enforce data classification and provide auditable external delivery. Prevents accidental exposure of OUO data; ensures compliance via audit trails. Data classification determines routing—only PUBLIC-approved data can exit to external systems.

**Parent**: EPIC: Geospatial API for DDH

---

### US 4.1: Classification Enforcement `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 4: Data Governance & Classification

**As a** data steward (B2B)
**I want** data classification enforced at submission
**So that** miscategorized data doesn't leak externally

**Acceptance Criteria**:
- [x] Platform API validates access_level parameter
- [x] Rejects invalid classification values
- [ ] Pipeline tasks fail-fast if classification missing (Phase 2 - optional)

#### Tasks

| ID | Description | Status |
|----|-------------|--------|
| T4.1.1 | Create AccessLevel enum | Done |
| T4.1.2 | Add Pydantic validator to PlatformRequest | Done |
| T4.1.3 | Add fail-fast check in pipeline tasks | Optional/Future |

---

### US 4.2: Approval Workflow `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 4: Data Governance & Classification

**As a** data steward (B2B)
**I want** to approve datasets before they're published
**So that** I can review data quality and classification

**Acceptance Criteria**:
- [x] Approval record created when job completes
- [x] Approve/reject endpoints functional
- [x] STAC item updated on approval (published=true)
- [x] Revocation supported for approved items

---

### US 4.3: Governed External Delivery `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 4: Data Governance & Classification

**As a** data steward (B2B)
**I want** PUBLIC data delivered externally via Azure Data Factory
**So that** external delivery is auditable and OUO data cannot be accidentally exposed

**Why ADF**: Azure Data Factory provides enterprise audit trails, RBAC, and activity logging required for compliance.

**Acceptance Criteria**:
- [x] ADF repository code complete with classification gate
- [ ] Environment variables configured
- [ ] ADF pipeline created in Azure with audit logging enabled
- [ ] End-to-end test verifying PUBLIC routes externally, OUO blocked

#### Tasks

| ID | Description | Status |
|----|-------------|--------|
| T4.3.1 | Create AzureDataFactoryRepository | Done |
| T4.3.2 | Wire to ApprovalService with classification check | Done |
| T4.3.3 | Submit eService for ADF instance | Not started |
| T4.3.4 | Configure environment variables | Blocked by T4.3.3 |
| T4.3.5 | Create ADF pipeline with audit logging | Blocked by T4.3.3 |

---

## FEATURE 5: Service Layer (TiTiler/TiPG) `[ACTIVE]`

**Description**: OGC-compliant APIs for consuming geospatial data. Any OGC-compatible client can access data—no proprietary tools required. Components: TiTiler (raster), TiPG (vector), pgSTAC (catalog). Deployed as separate Docker application.

**Parent**: EPIC: Geospatial API for DDH

---

### US 5.1: COG Tile Serving `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** web developer (B2C - Building applications for internal/external users)
**I want** server-side rendered map tiles from COGs
**So that** I can build interactive web maps

**Acceptance Criteria**:
- [x] XYZ tile endpoint (/cog/tiles/{z}/{x}/{y})
- [x] Supports rescaling and colormap parameters
- [x] Preview/thumbnail generation

---

### US 5.2: COG Data Access `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** data analyst (B2C - Internal researchers, external analysts)
**I want** to extract data from COGs without downloading files
**So that** I can analyze raster data efficiently via API

**Acceptance Criteria**:
- [x] Point queries: GET /cog/point/{lon}/{lat} returns pixel values for all bands
- [x] Image statistics: GET /cog/statistics returns band min/max/mean/std
- [x] Zonal statistics: POST /cog/statistics with GeoJSON polygon returns stats within area
- [x] HTTP range requests enable partial reads

---

### US 5.3: Multidimensional Data `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** climate analyst (B2C - Internal researchers)
**I want** tiles from Zarr/NetCDF data
**So that** I can visualize time-series climate data

**Acceptance Criteria**:
- [x] TiTiler-xarray integration working
- [x] Zarr tiles functional
- [x] Time dimension selectable

---

### US 5.4: pgSTAC Mosaic Searches `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** data analyst (B2C - Internal researchers, external analysts)
**I want** dynamic mosaics from STAC queries
**So that** I can view multiple datasets as one layer

**Acceptance Criteria**:
- [x] Mosaic search registration endpoint
- [x] Tiles from search results
- [x] Works with temporal queries
- [ ] Dynamic CQL queries for filtered mosaics

---

### US 5.5: OGC Features API `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** data scientist (B2C - Internal/external researchers)
**I want** to query vector data via OGC Features API
**So that** I can use any OGC-compatible tool (QGIS, ArcGIS, Python, R)

**Acceptance Criteria**:
- [x] Implements OGC API - Features standard
- [x] GeoJSON responses compatible with all major clients
- [x] Supports bbox and property filters
- [x] Server-side geometry simplification via `simplify` parameter (decimal degrees threshold)

---

### US 5.6: Vector Tiles `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** web developer (B2C - Building applications for internal/external users)
**I want** vector tiles for web maps
**So that** I can render large datasets efficiently

**Acceptance Criteria**:
- [x] Implements OGC API - Tiles standard
- [x] Serves Mapbox Vector Tiles (MVT) format

---

### US 5.7: STAC API `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** data consumer (B2C - Internal staff, external researchers, public)
**I want** to search the STAC catalog
**So that** I can discover available datasets

**Acceptance Criteria**:
- [x] Implements OGC STAC API specification

---

### US 5.8: Service Infrastructure `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 5: Service Layer (TiTiler/TiPG)

**As a** platform operator (Platform Ops - Tier 1)
**I want** the service layer deployed and monitored
**So that** consumers can reliably access data

**Acceptance Criteria**:
- [x] TiTiler, TiPG, pgSTAC deployed as Docker application
- [x] Health probes (/livez, /readyz)
- [x] Azure Managed Identity authentication
- [x] Azure Blob Storage SAS token for COG access
- [x] OpenTelemetry observability

---

## FEATURE 6: Admin & Developer Portal `[ACTIVE]`

**Description**: Web interfaces for operators and integrators. Enables self-service operations and accelerates B2B integration.

**Parent**: EPIC: Geospatial API for DDH

---

### US 6.1: Admin Portal `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 6: Admin & Developer Portal

**As a** platform administrator (Platform Ops - Tier 1 - platform operations team)
**I want** web UI access to manage the system
**So that** I can monitor health, submit jobs, and manage data without CLI

**Acceptance Criteria**:
- [x] System health dashboard (Function App, Docker Worker, queues, database)
- [x] Job submission and monitoring interface
- [x] STAC browser with collection/item management
- [x] Vector data browser with map preview
- [x] Approval workflow interface
- [ ] DAG workflow visualization

---

### US 6.2: Developer Integration Portal `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 6: Admin & Developer Portal

**As a** B2B application developer (B2B - DDH ITSDA team, other WB app integrators)
**I want** comprehensive API documentation and integration guides
**So that** I can integrate with the platform without support calls

**Acceptance Criteria**:
- [x] Swagger UI for interactive API exploration
- [x] ReDoc for readable API reference
- [x] OpenAPI 3.0 specification downloadable
- [x] CURL examples for common operations
- [ ] Integration quick-start guide

---

## FEATURE 7: DDH Platform Integration `[ACTIVE]`

**Description**: Enable DDH to submit ETL jobs (B2B orchestration) and consume resulting geospatial services (B2C consumption). DDH drives data publishing and embeds data access for end users.

**Parent**: EPIC: Geospatial API for DDH

---

### US 7.1: API Contract Documentation `[ACTIVE]`

**Type**: User Story
**Parent**: FEATURE 7: DDH Platform Integration

**As a** DDH developer (B2B - DDH ITSDA team)
**I want** formal API documentation
**So that** I can integrate with the geospatial platform

**Acceptance Criteria**:
- [x] OpenAPI 3.0 specification published
- [x] Swagger UI available at /api/interface/swagger
- [ ] Request/response examples reviewed with DDH team

#### Tasks

| ID | Description | Status |
|----|-------------|--------|
| T7.1.1 | Generate OpenAPI spec | Done |
| T7.1.2 | Deploy Swagger UI | Done |
| T7.1.3 | Review with DDH team | In Progress |

---

### US 7.2: Identity & Access Configuration `[CLOSED]`

**Type**: User Story
**Parent**: FEATURE 7: DDH Platform Integration

**As a** platform administrator (Platform Ops - Tier 1)
**I want** DDH to authenticate via Managed Identity
**So that** no secrets are shared between teams

**Acceptance Criteria**:
- [x] DDH Managed Identity granted Bronze Storage write access
- [x] DDH Managed Identity granted Platform API access
- [x] DDH Managed Identity granted Service Layer read access

---

### US 7.3: Environment Provisioning `[NEW]`

**Type**: User Story
**Parent**: FEATURE 7: DDH Platform Integration

**As a** platform administrator (Platform Ops - Tier 1)
**I want** configuration replicated to QA/UAT/Prod
**So that** DDH can test in each environment

**Acceptance Criteria**:
- [ ] QA environment configuration documented
- [ ] UAT environment provisioned
- [ ] Production environment provisioned

---

### EN 7.4: Integration Test Suite `[NEW]`

**Type**: Enabler
**Enabler Type**: Exploration
**Parent**: FEATURE 7: DDH Platform Integration
**Priority**: Optional

**Purpose**: Provide DDH team with ready-to-use integration tests

**Rationale**: Optional test suite provided as platform service. DDH can use for CI/CD validation or ignore and write their own tests.

**Acceptance Criteria**:
- [ ] Vector publish round-trip test
- [ ] Raster publish round-trip test
- [ ] OGC Features query test
- [ ] Job status polling test

**Enables**: DDH CI/CD integration (if they choose to use)

---

# IMPORT SUMMARY

| Work Item Type | Count | Closed | Active | New |
|----------------|-------|--------|--------|-----|
| Epic | 1 | 0 | 1 | 0 |
| Feature | 7 | 2 | 5 | 0 |
| User Story | 25 | 16 | 7 | 2 |
| Enabler | 3 | 1 | 1 | 1 |
| Spike | 1 | 1 | 0 | 0 |
| **Total** | **37** | **20** | **14** | **3** |

---

*End of ADO Work Items*
