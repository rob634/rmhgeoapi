# Artifact Registry Design Document

**Status**: DRAFT - Pending Review
**Created**: 20 JAN 2026
**Author**: Claude Code
**Epic**: Internal Artifact Tracking & Overwrite Workflow

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Problem Statement](#2-problem-statement)
3. [Architecture Overview](#3-architecture-overview)
4. [Schema Design](#4-schema-design)
5. [Artifact Service Design](#5-artifact-service-design)
6. [Overwrite Workflow](#6-overwrite-workflow)
7. [Query Patterns](#7-query-patterns)
8. [Migration Strategy](#8-migration-strategy)
9. [Open Questions](#9-open-questions)

---

## 1. Executive Summary

### Purpose

Create an **internal artifact registry** that tracks all data pipeline outputs with:
- **Client-agnostic identifiers** (UUIDs generated by our system)
- **Flexible client reference mapping** (supports DDH, Data360, or any future client)
- **Lineage tracking** for overwrite/supersession workflows
- **Robust to client schema changes** (our IDs survive external parameter changes)

### Key Benefits

1. **Overwrite Support**: Track when the same logical dataset is updated with new data
2. **Audit Trail**: Full history of what was published, when, and what replaced it
3. **Client Independence**: Internal IDs don't depend on external parameter schemes
4. **Future-Proof**: Add new clients without schema changes

---

## 2. Problem Statement

### Current State

Today, every identifier in our system traces back to client-provided parameters:

| Identifier | Derivation | Client-Dependent? |
|------------|------------|-------------------|
| `request_id` | `SHA256(dataset_id\|resource_id\|version_id)[:32]` | Yes (DDH) |
| `job_id` | `SHA256(job_type + all_params)` | Yes (includes DDH IDs) |
| `stac_item_id` | From blob path or custom `item_id` param | Partially |
| `collection_id` | Usually = `dataset_id` | Yes (DDH) |

### Problems

1. **No Overwrite Support**: Resubmitting same DDH IDs returns existing job (idempotency blocks updates)

2. **No Supersession Tracking**: Can't query "what replaced this item?"

3. **Client Coupling**: If DDH renames `dataset_id` to `data_source_id`, we lose linkage

4. **No Internal Identity**: We don't have our own identifier for "this COG file"

### User Story

> As a DDH user, I want to update an existing dataset (same dataset_id + resource_id + version_id)
> with new data, so that my consumers see the updated information without changing their references.

---

## 3. Architecture Overview

### Layer Separation

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         EXTERNAL CLIENTS                                 │
│                                                                          │
│    ┌─────────┐     ┌─────────┐     ┌─────────┐     ┌─────────┐         │
│    │   DDH   │     │ Data360 │     │  Manual │     │  Future │         │
│    │   API   │     │   API   │     │  Upload │     │ Clients │         │
│    └────┬────┘     └────┬────┘     └────┬────┘     └────┬────┘         │
│         │               │               │               │               │
└─────────┼───────────────┼───────────────┼───────────────┼───────────────┘
          │               │               │               │
          ▼               ▼               ▼               ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                    PLATFORM LAYER (Anti-Corruption Layer)                │
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                                                                    │  │
│  │  Responsibilities:                                                 │  │
│  │  • Accept client-specific request formats                         │  │
│  │  • Validate client parameters (DDH IDs, Data360 refs, etc.)       │  │
│  │  • Translate to INTERNAL parameters (client-agnostic)             │  │
│  │  • Track client request → internal job mapping (api_requests)     │  │
│  │  • Return results in client-expected format                       │  │
│  │                                                                    │  │
│  │  DOES NOT:                                                         │  │
│  │  • Process data                                                    │  │
│  │  • Create COGs                                                     │  │
│  │  • Manage STAC catalog                                            │  │
│  │  • Know about GDAL/rasterio                                       │  │
│  │                                                                    │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  Endpoints:                                                              │
│  • POST /api/platform/raster      (DDH format)                          │
│  • POST /api/platform/vector      (DDH format)                          │
│  • POST /api/data360/ingest       (hypothetical Data360 format)         │
│  • POST /api/upload               (generic manual upload)               │
│                                                                          │
└──────────────────────────────────┬──────────────────────────────────────┘
                                   │
                                   │  Internal parameters (client-agnostic)
                                   │  {blob_name, container, output_tier, ...}
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                  ORCHESTRATOR / ETL LAYER (CoreMachine)                  │
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                                                                    │  │
│  │  Responsibilities:                                                 │  │
│  │  • Execute data processing pipelines                              │  │
│  │  • Manage job → stage → task lifecycle                            │  │
│  │  • Route tasks to appropriate handlers                            │  │
│  │  • Track execution state (pending/processing/completed/failed)    │  │
│  │  • Persist results to app.jobs and app.tasks                      │  │
│  │                                                                    │  │
│  │  DOES NOT:                                                         │  │
│  │  • Know about DDH, Data360, or any client specifics               │  │
│  │  • Understand client parameter semantics                          │  │
│  │  • Track artifact lineage (that's the Artifact Layer's job)       │  │
│  │                                                                    │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  Jobs:     process_raster_docker, process_vector, stac_catalog          │
│  Handlers: raster_process_complete, vector_postgis, stac_extract        │
│  Tables:   app.jobs, app.tasks                                          │
│                                                                          │
└──────────────────────────────────┬──────────────────────────────────────┘
                                   │
                                   │  Creates outputs (COGs, STAC items)
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                          ARTIFACT LAYER (NEW)                            │
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                                                                    │  │
│  │  Responsibilities:                                                 │  │
│  │  • Assign internal artifact IDs (UUIDs)                           │  │
│  │  • Track what outputs exist and where they're stored              │  │
│  │  • Map client references → artifact (bidirectional)               │  │
│  │  • Track supersession/lineage (what replaced what)                │  │
│  │  • Enable audit queries ("what overwrote this?")                  │  │
│  │                                                                    │  │
│  │  DOES NOT:                                                         │  │
│  │  • Know how artifacts were created (processing details)           │  │
│  │  • Understand client parameter semantics                          │  │
│  │  • Manage STAC catalog (just references STAC IDs)                 │  │
│  │                                                                    │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  Tables:   app.artifacts                                                 │
│  Service:  ArtifactService                                              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Data Flow for Overwrite

```
1. Client Request
   DDH: POST /api/platform/raster?overwrite=true
   Body: {dataset_id: "X", resource_id: "Y", version_id: "v1.0", file_name: "new.tif"}

2. Platform Layer
   a. Check if artifact exists for client_refs = {X, Y, v1.0}
   b. If exists AND overwrite=true:
      - Mark existing artifact as "pending_supersession"
      - Generate new job with overwrite context
   c. Pass to CoreMachine

3. CoreMachine (Orchestrator)
   a. Execute job (download, validate, create COG, STAC catalog)
   b. Store results in app.jobs
   c. Signal completion

4. Artifact Layer (Post-Processing)
   a. Create new artifact record (new UUID)
   b. Link: new_artifact.supersedes = old_artifact.id
   c. Link: old_artifact.superseded_by = new_artifact.id
   d. Delete old STAC item (or mark superseded)
   e. Delete old COG blob (or archive)

5. Response to Client
   Same DDH IDs now point to new artifact
```

---

## 4. Schema Design

### 4.1 Primary Table: `app.artifacts`

```sql
-- ============================================================================
-- ARTIFACT REGISTRY TABLE
-- ============================================================================
-- Purpose: Track all data pipeline outputs with internal IDs
-- Created: 20 JAN 2026
-- ============================================================================

CREATE TABLE app.artifacts (
    -- ========================================================================
    -- INTERNAL IDENTIFIER (Our System's ID - Never Changes)
    -- ========================================================================
    artifact_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- ========================================================================
    -- CONTENT FINGERPRINT (For Deduplication & Integrity)
    -- ========================================================================
    -- SHA256 hash of the output file content
    -- Enables: "Is this upload actually different from what we have?"
    content_hash VARCHAR(64),

    -- ========================================================================
    -- STORAGE LOCATION (Where the Asset Lives)
    -- ========================================================================
    storage_account VARCHAR(64) NOT NULL,
    container VARCHAR(64) NOT NULL,
    blob_path TEXT NOT NULL,
    size_bytes BIGINT,
    content_type VARCHAR(100),  -- e.g., 'image/tiff', 'application/geo+json'

    -- ========================================================================
    -- STAC REFERENCE (Optional - Artifact May Not Have STAC Item)
    -- ========================================================================
    -- Some artifacts (e.g., intermediate files) may not be in STAC catalog
    stac_collection_id VARCHAR(255),
    stac_item_id VARCHAR(255),

    -- ========================================================================
    -- CLIENT REFERENCE MAPPING (Supports ANY Client Schema)
    -- ========================================================================
    -- client_type: Identifies the source system
    -- client_refs: Flexible JSONB for any client's parameter schema
    client_type VARCHAR(50) NOT NULL,
    client_refs JSONB NOT NULL,

    -- Examples of client_refs:
    -- DDH:     {"dataset_id": "flood-2024", "resource_id": "site-a", "version_id": "v1.0"}
    -- Data360: {"catalog_entry": "12345", "revision": "3", "workspace": "geo"}
    -- Manual:  {"uploader": "jsmith", "upload_id": "abc123", "project": "demo"}

    -- ========================================================================
    -- PROCESSING REFERENCE (Link to CoreMachine)
    -- ========================================================================
    source_job_id VARCHAR(64),
    source_task_id VARCHAR(16),

    -- ========================================================================
    -- LINEAGE / SUPERSESSION (Track Overwrites)
    -- ========================================================================
    -- supersedes: The artifact THIS ONE replaced (NULL if first version)
    -- superseded_by: The artifact that replaced THIS ONE (NULL if current)
    supersedes UUID REFERENCES app.artifacts(artifact_id),
    superseded_by UUID REFERENCES app.artifacts(artifact_id),

    -- revision: Sequential counter for human readability
    -- Increments each time same client_refs gets new artifact
    revision INTEGER NOT NULL DEFAULT 1,

    -- ========================================================================
    -- STATUS (Lifecycle State)
    -- ========================================================================
    status VARCHAR(20) NOT NULL DEFAULT 'active',
    -- Values: 'active', 'superseded', 'deleted', 'archived', 'pending'

    -- ========================================================================
    -- METADATA (Flexible Additional Info)
    -- ========================================================================
    metadata JSONB DEFAULT '{}',
    -- Can store: raster_type, band_count, bbox, crs, processing_time, etc.

    -- ========================================================================
    -- TIMESTAMPS
    -- ========================================================================
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deleted_at TIMESTAMPTZ,  -- Soft delete timestamp

    -- ========================================================================
    -- CONSTRAINTS
    -- ========================================================================
    -- Unique within client context (only one active artifact per client_refs)
    -- Note: Superseded artifacts won't conflict due to status check
    CONSTRAINT uq_active_client_refs
        UNIQUE (client_type, client_refs)
        WHERE status = 'active'
);

-- ============================================================================
-- INDEXES
-- ============================================================================

-- Primary lookup: Find artifact by client references
CREATE INDEX idx_artifacts_client_refs
    ON app.artifacts USING GIN (client_refs);

-- Client type filtering
CREATE INDEX idx_artifacts_client_type
    ON app.artifacts (client_type);

-- STAC reverse lookup: "Which artifact created this STAC item?"
CREATE INDEX idx_artifacts_stac
    ON app.artifacts (stac_collection_id, stac_item_id)
    WHERE stac_item_id IS NOT NULL;

-- Job lookup: "What artifacts did this job create?"
CREATE INDEX idx_artifacts_job
    ON app.artifacts (source_job_id)
    WHERE source_job_id IS NOT NULL;

-- Lineage queries: Find supersession chains
CREATE INDEX idx_artifacts_supersedes
    ON app.artifacts (supersedes)
    WHERE supersedes IS NOT NULL;

-- Active artifacts only
CREATE INDEX idx_artifacts_active
    ON app.artifacts (status)
    WHERE status = 'active';

-- Content deduplication
CREATE INDEX idx_artifacts_content_hash
    ON app.artifacts (content_hash)
    WHERE content_hash IS NOT NULL;

-- Time-based queries
CREATE INDEX idx_artifacts_created
    ON app.artifacts (created_at DESC);

-- ============================================================================
-- COMMENTS
-- ============================================================================

COMMENT ON TABLE app.artifacts IS
    'Internal artifact registry tracking all data pipeline outputs with client-agnostic UUIDs';

COMMENT ON COLUMN app.artifacts.artifact_id IS
    'Internal UUID - our system''s identifier, never derived from client params';

COMMENT ON COLUMN app.artifacts.client_refs IS
    'Flexible JSONB storing client-specific reference IDs (DDH IDs, Data360 refs, etc.)';

COMMENT ON COLUMN app.artifacts.supersedes IS
    'Previous artifact that THIS artifact replaced (overwrite lineage)';

COMMENT ON COLUMN app.artifacts.superseded_by IS
    'Newer artifact that replaced THIS artifact (forward reference)';
```

### 4.2 Pydantic Model

```python
# core/models/artifact.py

from pydantic import BaseModel, Field
from typing import Dict, Any, Optional
from datetime import datetime
from uuid import UUID
from enum import Enum


class ArtifactStatus(str, Enum):
    """Artifact lifecycle status."""
    PENDING = "pending"      # Being created
    ACTIVE = "active"        # Current version
    SUPERSEDED = "superseded"  # Replaced by newer version
    ARCHIVED = "archived"    # Moved to archive storage
    DELETED = "deleted"      # Soft deleted


class Artifact(BaseModel):
    """
    Artifact database model - Internal asset tracking.

    Represents a single output from the data pipeline with:
    - Internal UUID (artifact_id) independent of client params
    - Flexible client_refs JSONB for any client's parameter schema
    - Supersession tracking for overwrite lineage
    """

    # Internal identifier
    artifact_id: UUID = Field(
        description="Internal UUID - our system's identifier"
    )

    # Content fingerprint
    content_hash: Optional[str] = Field(
        None, max_length=64,
        description="SHA256 hash of output file content"
    )

    # Storage location
    storage_account: str = Field(..., max_length=64)
    container: str = Field(..., max_length=64)
    blob_path: str = Field(...)
    size_bytes: Optional[int] = None
    content_type: Optional[str] = Field(None, max_length=100)

    # STAC reference
    stac_collection_id: Optional[str] = Field(None, max_length=255)
    stac_item_id: Optional[str] = Field(None, max_length=255)

    # Client reference mapping
    client_type: str = Field(..., max_length=50)
    client_refs: Dict[str, Any] = Field(
        ...,
        description="Client-specific reference IDs as JSONB"
    )

    # Processing reference
    source_job_id: Optional[str] = Field(None, max_length=64)
    source_task_id: Optional[str] = Field(None, max_length=16)

    # Lineage
    supersedes: Optional[UUID] = Field(
        None, description="Artifact this one replaced"
    )
    superseded_by: Optional[UUID] = Field(
        None, description="Artifact that replaced this one"
    )
    revision: int = Field(default=1)

    # Status
    status: ArtifactStatus = Field(default=ArtifactStatus.ACTIVE)

    # Metadata
    metadata: Dict[str, Any] = Field(default_factory=dict)

    # Timestamps
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    deleted_at: Optional[datetime] = None

    class Config:
        use_enum_values = True


# Schema metadata for SQL generator
ARTIFACT_TABLE_NAME = 'artifacts'
ARTIFACT_PRIMARY_KEY = ['artifact_id']
ARTIFACT_INDEXES = [
    ('client_type',),
    ('source_job_id',),
    ('status',),
]
```

### 4.3 Client Reference Examples

```python
# DDH Client
client_type = "ddh"
client_refs = {
    "dataset_id": "flood-risk-assessment-2024",
    "resource_id": "site-alpha-imagery",
    "version_id": "v1.0"
}

# Data360 Client (Hypothetical)
client_type = "data360"
client_refs = {
    "catalog_entry_id": "CAT-2024-12345",
    "revision_number": "3",
    "workspace_id": "geo-analytics",
    "data_product_id": "DP-789"
}

# Manual Upload
client_type = "manual"
client_refs = {
    "uploader_email": "jsmith@example.com",
    "upload_session_id": "sess_abc123",
    "project_code": "DEMO-2024",
    "notes": "Test upload for client demo"
}

# Internal/System Generated
client_type = "system"
client_refs = {
    "pipeline": "fathom_flood_etl",
    "source_dataset": "fathom-global-v3",
    "tile_id": "N45W090",
    "processing_date": "2026-01-20"
}
```

---

## 5. Artifact Service Design

### 5.1 Service Interface

```python
# services/artifact_service.py

from typing import Dict, Any, Optional, List
from uuid import UUID
from datetime import datetime


class ArtifactService:
    """
    Service for managing the artifact registry.

    Provides:
    - Artifact creation with automatic revision tracking
    - Client reference lookups (any client schema)
    - Supersession management for overwrites
    - Lineage queries (history, chain traversal)
    """

    def __init__(self):
        """Initialize with repository dependency."""
        from infrastructure.artifact_repository import ArtifactRepository
        self._repo = ArtifactRepository()

    # =========================================================================
    # CREATION
    # =========================================================================

    def create_artifact(
        self,
        storage_account: str,
        container: str,
        blob_path: str,
        client_type: str,
        client_refs: Dict[str, Any],
        stac_collection_id: Optional[str] = None,
        stac_item_id: Optional[str] = None,
        source_job_id: Optional[str] = None,
        content_hash: Optional[str] = None,
        size_bytes: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None,
        overwrite: bool = False
    ) -> Dict[str, Any]:
        """
        Create a new artifact record.

        If overwrite=True and existing artifact found:
        - Mark old artifact as superseded
        - Create new artifact with supersedes link
        - Increment revision number

        Args:
            storage_account: Azure storage account name
            container: Blob container name
            blob_path: Path within container
            client_type: Client identifier (e.g., 'ddh', 'data360')
            client_refs: Client-specific reference IDs as dict
            stac_collection_id: STAC collection ID (if cataloged)
            stac_item_id: STAC item ID (if cataloged)
            source_job_id: CoreMachine job that created this
            content_hash: SHA256 of file content
            size_bytes: File size
            metadata: Additional metadata
            overwrite: If True, supersede existing artifact

        Returns:
            Created artifact dict with artifact_id

        Raises:
            ArtifactExistsError: If artifact exists and overwrite=False
        """
        pass

    # =========================================================================
    # LOOKUP
    # =========================================================================

    def get_by_id(self, artifact_id: UUID) -> Optional[Dict[str, Any]]:
        """Get artifact by internal UUID."""
        pass

    def get_by_client_refs(
        self,
        client_type: str,
        client_refs: Dict[str, Any],
        include_superseded: bool = False
    ) -> Optional[Dict[str, Any]]:
        """
        Get artifact by client reference.

        Args:
            client_type: Client identifier
            client_refs: Client-specific reference dict
            include_superseded: If True, return even if superseded

        Returns:
            Active artifact dict, or None if not found
        """
        pass

    def get_by_stac(
        self,
        collection_id: str,
        item_id: str
    ) -> Optional[Dict[str, Any]]:
        """Reverse lookup: Get artifact that created this STAC item."""
        pass

    def get_by_job(self, job_id: str) -> List[Dict[str, Any]]:
        """Get all artifacts created by a job."""
        pass

    # =========================================================================
    # LINEAGE / HISTORY
    # =========================================================================

    def get_history(
        self,
        client_type: str,
        client_refs: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Get full history of artifacts for client refs.

        Returns all revisions (active + superseded) ordered by revision desc.

        Example result:
        [
            {"artifact_id": "uuid-3", "revision": 3, "status": "active", ...},
            {"artifact_id": "uuid-2", "revision": 2, "status": "superseded", ...},
            {"artifact_id": "uuid-1", "revision": 1, "status": "superseded", ...}
        ]
        """
        pass

    def get_supersession_chain(
        self,
        artifact_id: UUID,
        direction: str = "both"  # "forward", "backward", "both"
    ) -> List[Dict[str, Any]]:
        """
        Traverse supersession chain from an artifact.

        Args:
            artifact_id: Starting artifact
            direction:
                - "forward": What replaced this? (follow superseded_by)
                - "backward": What did this replace? (follow supersedes)
                - "both": Full chain
        """
        pass

    # =========================================================================
    # SUPERSESSION / OVERWRITE
    # =========================================================================

    def supersede(
        self,
        old_artifact_id: UUID,
        new_artifact_id: UUID
    ) -> None:
        """
        Mark old artifact as superseded by new artifact.

        Updates:
        - old_artifact.status = 'superseded'
        - old_artifact.superseded_by = new_artifact_id
        - new_artifact.supersedes = old_artifact_id
        """
        pass

    def mark_deleted(
        self,
        artifact_id: UUID,
        hard_delete: bool = False
    ) -> None:
        """
        Delete an artifact.

        Args:
            artifact_id: Artifact to delete
            hard_delete: If True, remove from DB. If False, soft delete.
        """
        pass

    # =========================================================================
    # QUERIES
    # =========================================================================

    def list_by_client_type(
        self,
        client_type: str,
        status: Optional[str] = "active",
        limit: int = 100,
        offset: int = 0
    ) -> Dict[str, Any]:
        """List artifacts for a client type with pagination."""
        pass

    def find_duplicates(
        self,
        content_hash: str
    ) -> List[Dict[str, Any]]:
        """Find artifacts with same content hash (potential duplicates)."""
        pass

    # =========================================================================
    # STATISTICS
    # =========================================================================

    def get_stats(self, client_type: Optional[str] = None) -> Dict[str, Any]:
        """
        Get artifact statistics.

        Returns:
            {
                "total_artifacts": 1234,
                "active": 1000,
                "superseded": 200,
                "deleted": 34,
                "total_size_bytes": 12345678,
                "by_client_type": {
                    "ddh": 800,
                    "manual": 200,
                    ...
                }
            }
        """
        pass
```

### 5.2 Repository Implementation

```python
# infrastructure/artifact_repository.py

from typing import Dict, Any, Optional, List
from uuid import UUID
from datetime import datetime
from psycopg import sql
import json

from infrastructure.postgresql import PostgreSQLRepository
from core.models.artifact import Artifact, ArtifactStatus


class ArtifactRepository(PostgreSQLRepository):
    """
    Repository for artifact CRUD operations.

    Uses app.artifacts table for storage.
    """

    def create(self, artifact: Artifact) -> Artifact:
        """Insert new artifact record."""
        query = sql.SQL("""
            INSERT INTO {}.artifacts (
                artifact_id, content_hash, storage_account, container,
                blob_path, size_bytes, content_type,
                stac_collection_id, stac_item_id,
                client_type, client_refs,
                source_job_id, source_task_id,
                supersedes, revision, status, metadata,
                created_at, updated_at
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                %s, %s, %s, %s, %s, %s, %s, %s
            )
            RETURNING *
        """).format(sql.Identifier(self.schema_name))

        params = (
            str(artifact.artifact_id),
            artifact.content_hash,
            artifact.storage_account,
            artifact.container,
            artifact.blob_path,
            artifact.size_bytes,
            artifact.content_type,
            artifact.stac_collection_id,
            artifact.stac_item_id,
            artifact.client_type,
            json.dumps(artifact.client_refs),
            artifact.source_job_id,
            artifact.source_task_id,
            str(artifact.supersedes) if artifact.supersedes else None,
            artifact.revision,
            artifact.status,
            json.dumps(artifact.metadata),
            artifact.created_at,
            artifact.updated_at
        )

        row = self._execute_query(query, params, fetch='one')
        return self._row_to_model(row)

    def get_by_id(self, artifact_id: UUID) -> Optional[Artifact]:
        """Get artifact by UUID."""
        query = sql.SQL("""
            SELECT * FROM {}.artifacts WHERE artifact_id = %s
        """).format(sql.Identifier(self.schema_name))

        row = self._execute_query(query, (str(artifact_id),), fetch='one')
        return self._row_to_model(row) if row else None

    def get_active_by_client_refs(
        self,
        client_type: str,
        client_refs: Dict[str, Any]
    ) -> Optional[Artifact]:
        """Get active artifact by client references using JSONB containment."""
        query = sql.SQL("""
            SELECT * FROM {}.artifacts
            WHERE client_type = %s
              AND client_refs @> %s::jsonb
              AND status = 'active'
            LIMIT 1
        """).format(sql.Identifier(self.schema_name))

        row = self._execute_query(
            query,
            (client_type, json.dumps(client_refs)),
            fetch='one'
        )
        return self._row_to_model(row) if row else None

    def get_all_by_client_refs(
        self,
        client_type: str,
        client_refs: Dict[str, Any]
    ) -> List[Artifact]:
        """Get all artifacts (any status) by client references."""
        query = sql.SQL("""
            SELECT * FROM {}.artifacts
            WHERE client_type = %s
              AND client_refs @> %s::jsonb
            ORDER BY revision DESC
        """).format(sql.Identifier(self.schema_name))

        rows = self._execute_query(
            query,
            (client_type, json.dumps(client_refs)),
            fetch='all'
        )
        return [self._row_to_model(row) for row in rows]

    def get_max_revision(
        self,
        client_type: str,
        client_refs: Dict[str, Any]
    ) -> int:
        """Get highest revision number for client refs."""
        query = sql.SQL("""
            SELECT COALESCE(MAX(revision), 0) as max_rev
            FROM {}.artifacts
            WHERE client_type = %s
              AND client_refs @> %s::jsonb
        """).format(sql.Identifier(self.schema_name))

        row = self._execute_query(
            query,
            (client_type, json.dumps(client_refs)),
            fetch='one'
        )
        return row['max_rev'] if row else 0

    def update_status(
        self,
        artifact_id: UUID,
        status: ArtifactStatus,
        superseded_by: Optional[UUID] = None
    ) -> None:
        """Update artifact status and optional superseded_by link."""
        if superseded_by:
            query = sql.SQL("""
                UPDATE {}.artifacts
                SET status = %s, superseded_by = %s, updated_at = NOW()
                WHERE artifact_id = %s
            """).format(sql.Identifier(self.schema_name))
            params = (status, str(superseded_by), str(artifact_id))
        else:
            query = sql.SQL("""
                UPDATE {}.artifacts
                SET status = %s, updated_at = NOW()
                WHERE artifact_id = %s
            """).format(sql.Identifier(self.schema_name))
            params = (status, str(artifact_id))

        self._execute_query(query, params, fetch=None)

    def _row_to_model(self, row: Dict[str, Any]) -> Artifact:
        """Convert database row to Artifact model."""
        return Artifact(
            artifact_id=UUID(row['artifact_id']),
            content_hash=row.get('content_hash'),
            storage_account=row['storage_account'],
            container=row['container'],
            blob_path=row['blob_path'],
            size_bytes=row.get('size_bytes'),
            content_type=row.get('content_type'),
            stac_collection_id=row.get('stac_collection_id'),
            stac_item_id=row.get('stac_item_id'),
            client_type=row['client_type'],
            client_refs=row['client_refs'],  # Already parsed by psycopg
            source_job_id=row.get('source_job_id'),
            source_task_id=row.get('source_task_id'),
            supersedes=UUID(row['supersedes']) if row.get('supersedes') else None,
            superseded_by=UUID(row['superseded_by']) if row.get('superseded_by') else None,
            revision=row['revision'],
            status=row['status'],
            metadata=row.get('metadata', {}),
            created_at=row['created_at'],
            updated_at=row['updated_at'],
            deleted_at=row.get('deleted_at')
        )
```

---

## 6. Overwrite Workflow

### 6.1 Sequence Diagram

```
Client                  Platform Layer              CoreMachine              Artifact Service
  │                          │                          │                          │
  │ POST /platform/raster    │                          │                          │
  │ overwrite=true           │                          │                          │
  │─────────────────────────>│                          │                          │
  │                          │                          │                          │
  │                          │ Check existing artifact  │                          │
  │                          │─────────────────────────────────────────────────────>│
  │                          │                          │                          │
  │                          │<────────────── Found: artifact_id=uuid-1, rev=1 ────│
  │                          │                          │                          │
  │                          │ (overwrite=true)         │                          │
  │                          │ Mark pending supersession│                          │
  │                          │─────────────────────────────────────────────────────>│
  │                          │                          │                          │
  │                          │ Create job with context  │                          │
  │                          │─────────────────────────>│                          │
  │                          │                          │                          │
  │                          │                          │ Process raster           │
  │                          │                          │ Create COG               │
  │                          │                          │ STAC catalog             │
  │                          │                          │                          │
  │                          │                          │ Job complete             │
  │                          │<─────────────────────────│                          │
  │                          │                          │                          │
  │                          │ Post-processing:         │                          │
  │                          │ Create new artifact      │                          │
  │                          │─────────────────────────────────────────────────────>│
  │                          │                          │                          │
  │                          │                          │ Create artifact_id=uuid-2│
  │                          │                          │ Set supersedes=uuid-1    │
  │                          │                          │ Set revision=2           │
  │                          │                          │                          │
  │                          │                          │ Update uuid-1:           │
  │                          │                          │ status='superseded'      │
  │                          │                          │ superseded_by=uuid-2     │
  │                          │                          │                          │
  │                          │<───────────────────────── New artifact created ─────│
  │                          │                          │                          │
  │                          │ Cleanup old assets:      │                          │
  │                          │ - Delete old STAC item   │                          │
  │                          │ - Archive old COG        │                          │
  │                          │                          │                          │
  │<───────── Response: Success, artifact_id=uuid-2 ───│                          │
  │                          │                          │                          │
```

### 6.2 Implementation Flow

```python
# In platform trigger (simplified)

async def handle_platform_raster_submit(request: PlatformRequest):
    """Handle raster submission with overwrite support."""

    artifact_service = ArtifactService()

    # Build client refs from DDH parameters
    client_refs = {
        "dataset_id": request.dataset_id,
        "resource_id": request.resource_id,
        "version_id": request.version_id
    }

    # Check for existing artifact
    existing = artifact_service.get_by_client_refs(
        client_type="ddh",
        client_refs=client_refs
    )

    overwrite = request.processing_options.get("overwrite", False)

    if existing and not overwrite:
        # Return existing result (idempotency)
        return {
            "status": "exists",
            "artifact_id": str(existing.artifact_id),
            "message": "Artifact already exists. Use overwrite=true to replace."
        }

    # Create job with overwrite context
    job_params = {
        "blob_name": request.file_name,
        "container_name": request.container_name,
        "dataset_id": request.dataset_id,
        "resource_id": request.resource_id,
        "version_id": request.version_id,
        "overwrite": overwrite,
        "_supersedes_artifact_id": str(existing.artifact_id) if existing else None,
        # ... other params
    }

    # Submit to CoreMachine
    job_result = submit_job("process_raster_docker", job_params)

    return {
        "status": "submitted",
        "job_id": job_result["job_id"],
        "overwriting": existing is not None
    }
```

```python
# In post-processing (handler completion)

def register_artifact_on_job_complete(job_id: str, result_data: dict):
    """Called when a raster job completes to register the artifact."""

    artifact_service = ArtifactService()
    job_repo = JobRepository()

    job = job_repo.get_job(job_id)
    params = job.parameters

    # Build artifact
    artifact_data = artifact_service.create_artifact(
        storage_account=result_data["cog"]["storage_account"],
        container=result_data["cog"]["cog_container"],
        blob_path=result_data["cog"]["cog_blob"],
        client_type="ddh",
        client_refs={
            "dataset_id": params["dataset_id"],
            "resource_id": params["resource_id"],
            "version_id": params["version_id"]
        },
        stac_collection_id=result_data["stac"]["collection_id"],
        stac_item_id=result_data["stac"]["item_id"],
        source_job_id=job_id,
        size_bytes=int(result_data["cog"]["size_mb"] * 1024 * 1024),
        metadata={
            "raster_type": result_data["cog"].get("raster_type"),
            "compression": result_data["cog"].get("compression"),
            "bbox": result_data["stac"].get("bbox")
        },
        overwrite=params.get("overwrite", False)
    )

    # If overwriting, clean up old assets
    supersedes_id = params.get("_supersedes_artifact_id")
    if supersedes_id:
        cleanup_superseded_artifact(supersedes_id)

    return artifact_data
```

---

## 7. Query Patterns

### 7.1 DDH Lookup with History

```sql
-- "Show me all artifacts for DDH dataset X, resource Y, version v1.0"
-- Including overwrite history
SELECT
    artifact_id,
    revision,
    status,
    created_at,
    supersedes,
    superseded_by,
    stac_collection_id,
    stac_item_id,
    blob_path,
    size_bytes
FROM app.artifacts
WHERE client_type = 'ddh'
  AND client_refs @> '{"dataset_id": "X", "resource_id": "Y", "version_id": "v1.0"}'::jsonb
ORDER BY revision DESC;
```

**Example Result:**

| artifact_id | revision | status | created_at | supersedes | superseded_by |
|-------------|----------|--------|------------|------------|---------------|
| uuid-3 | 3 | active | 2026-01-20 | uuid-2 | NULL |
| uuid-2 | 2 | superseded | 2026-01-15 | uuid-1 | uuid-3 |
| uuid-1 | 1 | superseded | 2026-01-10 | NULL | uuid-2 |

### 7.2 Get Current Active Artifact

```sql
-- "What's the current artifact for DDH X/Y/v1.0?"
SELECT * FROM app.artifacts
WHERE client_type = 'ddh'
  AND client_refs @> '{"dataset_id": "X", "resource_id": "Y", "version_id": "v1.0"}'::jsonb
  AND status = 'active'
LIMIT 1;
```

### 7.3 Reverse Lookup from STAC

```sql
-- "Which artifact created this STAC item?"
SELECT * FROM app.artifacts
WHERE stac_collection_id = 'my-collection'
  AND stac_item_id = 'my-item-id';
```

### 7.4 Find All Artifacts from a Job

```sql
-- "What did job XYZ produce?"
SELECT * FROM app.artifacts
WHERE source_job_id = 'abc123...'
ORDER BY created_at;
```

### 7.5 Statistics by Client

```sql
-- "How many artifacts per client type?"
SELECT
    client_type,
    status,
    COUNT(*) as count,
    SUM(size_bytes) as total_bytes
FROM app.artifacts
GROUP BY client_type, status
ORDER BY client_type, status;
```

### 7.6 Find Duplicates by Content

```sql
-- "Are there duplicate files?"
SELECT
    content_hash,
    COUNT(*) as count,
    array_agg(artifact_id) as artifact_ids
FROM app.artifacts
WHERE content_hash IS NOT NULL
GROUP BY content_hash
HAVING COUNT(*) > 1;
```

---

## 8. Migration Strategy

### 8.1 Phase 1: Schema Deployment

```bash
# Add artifacts table via schema deployment
curl -X POST "https://rmhazuregeoapi.../api/dbadmin/maintenance?action=ensure&confirm=yes"
```

### 8.2 Phase 2: Backfill Existing Data

Create a migration job to populate artifacts from existing:
- `app.api_requests` (DDH client refs)
- `app.jobs` (source job reference)
- `pgstac.items` (STAC references)

```python
# Migration script (conceptual)
def backfill_artifacts():
    """Populate artifacts table from existing data."""

    # Get all completed raster/vector jobs
    jobs = job_repo.get_completed_jobs(job_types=['process_raster_docker', 'process_vector'])

    for job in jobs:
        result = job.result_data
        if not result:
            continue

        # Extract client refs from job params
        params = job.parameters
        client_refs = {
            "dataset_id": params.get("dataset_id"),
            "resource_id": params.get("resource_id"),
            "version_id": params.get("version_id")
        }

        if not all(client_refs.values()):
            # Non-DDH job, use different client type
            client_type = "legacy"
            client_refs = {"job_id": job.job_id}
        else:
            client_type = "ddh"

        # Create artifact record
        artifact_service.create_artifact(
            storage_account="rmhstorage123",  # or extract from result
            container=result.get("cog", {}).get("cog_container"),
            blob_path=result.get("cog", {}).get("cog_blob"),
            client_type=client_type,
            client_refs=client_refs,
            stac_collection_id=result.get("stac", {}).get("collection_id"),
            stac_item_id=result.get("stac", {}).get("item_id"),
            source_job_id=job.job_id,
            overwrite=False  # First revision
        )
```

### 8.3 Phase 3: Wire Up Creation

Modify job completion handlers to create artifact records:
- `handler_process_raster_complete.py`
- `handler_process_vector.py`

### 8.4 Phase 4: Add Overwrite Parameter

Add `overwrite` parameter to:
- `PlatformRequest` DTO
- `process_raster_docker` job
- Platform trigger logic

### 8.5 Phase 5: Cleanup Logic

Implement cleanup for superseded artifacts:
- Delete old STAC item (or mark superseded)
- Archive or delete old COG blob

---

## 9. Design Decisions (Resolved)

*All questions reviewed and resolved 20 JAN 2026*

| # | Question | Decision | Rationale |
|---|----------|----------|-----------|
| 1 | **STAC Item Handling** | Delete old STAC item, create new with same ID | Clean replacement, no orphaned items |
| 2 | **COG Blob Handling** | Overwrite blob in place | Artifacts table tracks history, no need for versioned files |
| 3 | **Revision Numbering** | Global monotonic (never resets) | Simple `MAX(revision) + 1`, clear audit trail |
| 4 | **Content Hash** | Hash output COG after creation | Detect duplicate uploads, avoid redundant processing |
| 5 | **Cleanup Timing** | Synchronous | Overwrite is atomic, DB update is fast |
| 6 | **API Response** | `artifact_id` is **INTERNAL ONLY** | Not exposed to DDH or external clients until deliberately decided |

### Content Hash Benefits

Computing SHA256 of the output COG enables:
- **Duplicate detection**: "This overwrite has identical content to revision 2"
- **Skip redundant work**: If content_hash matches existing active artifact, can short-circuit
- **Integrity verification**: Confirm COG wasn't corrupted in storage

---

## 10. Implementation Plan

### Phase 1: Schema & Core Infrastructure ✅ COMPLETE (20 JAN 2026)
**Goal**: Deploy artifacts table and basic CRUD operations

Tasks:
1. ✅ Add `app.artifacts` table definition to schema (`core/schema/sql_generator.py`)
2. ✅ Create `core/models/artifact.py` with Pydantic model
3. ✅ Create `infrastructure/artifact_repository.py` with basic CRUD
4. ✅ Create `services/artifact_service.py` with create/lookup methods
5. ⏳ Deploy schema with `action=ensure` (pending deployment)
6. ⏳ Write unit tests for repository and service (Phase 1b)

**Files Created**:
- `core/models/artifact.py` - Artifact model + ArtifactStatus enum
- `infrastructure/artifact_repository.py` - Repository with CRUD operations
- `services/artifact_service.py` - Business logic with supersession handling

**Deliverable**: Can manually create and query artifacts via service

---

### Phase 2: Wire Up Artifact Creation
**Goal**: Automatically create artifact records when jobs complete

Tasks:
1. Modify `handler_process_raster_complete.py` to call ArtifactService
2. Extract COG metadata (size, path, storage account) from job result
3. Compute content_hash of output COG
4. Create artifact record with DDH client_refs from job params
5. Add artifact_id to job result_data (internal tracking)

**Deliverable**: Every completed raster job creates an artifact record

---

### Phase 3: Overwrite Support
**Goal**: Enable overwrite workflow with supersession tracking

Tasks:
1. Add `overwrite` parameter to `PlatformRequest` DTO
2. Modify platform trigger to check for existing artifact
3. If exists + overwrite=true:
   - Pass `_supersedes_artifact_id` to job params
   - After completion, link supersession chain
   - Update old artifact status to 'superseded'
4. Delete old STAC item, create new with same ID
5. Overwrite COG blob in place

**Deliverable**: DDH can submit with `overwrite=true` to replace existing datasets

---

### Phase 4: Backfill & Queries
**Goal**: Populate history and enable audit queries

Tasks:
1. Create backfill migration script for existing completed jobs
2. Add `/api/artifacts/history` endpoint (internal/admin)
3. Add `/api/artifacts/lookup` endpoint (by client_refs)
4. Add artifact stats to `/api/dbadmin/stats`

**Deliverable**: Full history visibility for all artifacts

---

### Phase 5: Vector Pipeline Integration
**Goal**: Extend artifact tracking to vector jobs

Tasks:
1. Modify vector job completion handlers
2. Track PostGIS table as artifact (different storage pattern)
3. Handle vector-specific metadata

**Deliverable**: Both raster and vector pipelines create artifacts

---

## Appendix A: Related Files

| File | Purpose |
|------|---------|
| `core/models/platform.py` | Existing Platform models (ApiRequest) |
| `infrastructure/platform.py` | Existing Platform repository |
| `services/platform_catalog_service.py` | B2B STAC catalog service |
| `jobs/mixins.py` | Job ID generation (SHA256 hash) |
| `triggers/trigger_platform.py` | Platform API triggers |

---

## Appendix B: Glossary

| Term | Definition |
|------|------------|
| **Artifact** | A single output from the data pipeline (COG, vector table, etc.) |
| **Client Refs** | External system's identifiers (e.g., DDH's dataset_id/resource_id/version_id) |
| **Supersession** | When a new artifact replaces an old one for the same logical dataset |
| **Revision** | Sequential counter for overwrites of the same client_refs |
| **ACL** | Anti-Corruption Layer - translates external formats to internal |

---

*Document Version: 1.0*
*Last Updated: 20 JAN 2026*
