# V0.8 PostGIS Bulk Insert Optimization

**Created**: 26 JAN 2026
**Status**: Implementation Plan
**Priority**: Critical - Current vector ETL is 50-100x slower than optimal

---

## Executive Summary

Current vector ETL uses row-by-row INSERT statements, resulting in ~130 rows/sec performance. This document outlines two bulk insert strategies to achieve 5,000-50,000+ rows/sec:

1. **`execute_values()` pattern** - Immediate fix for vector ETL (10-50x speedup)
2. **COPY + temp table pattern** - Future-ready for H3 aggregation and GeoParquet pipelines (50-100x speedup)

---

## Current Problem

### Root Cause
`services/vector/postgis_handler.py` lines 1544-1552:

```python
# CURRENT: Row-by-row inserts (SLOW!)
for idx, row in chunk.iterrows():
    geom_wkt = row.geometry.wkt
    values = [geom_wkt, batch_id] + [row[col] for col in attr_cols]
    cur.execute(insert_stmt, values)  # 20,000 network round-trips per chunk!
```

### Performance Impact
- 20,000 rows × individual INSERT = 20,000 network round-trips
- Each round-trip: ~5-10ms (Azure PostgreSQL latency)
- Result: ~100-200 rows/sec instead of potential 10,000+ rows/sec

### Affected Methods
1. `insert_chunk_idempotent()` - Used by Docker vector ETL
2. `_insert_features()` - Used by Function App vector ETL

---

## Solution 1: execute_values() Pattern

**Use Case**: Vector ETL, moderate datasets (10K-500K rows)
**Expected Speedup**: 10-50x
**Complexity**: Low - minimal code change

### psycopg3 Implementation

psycopg3 doesn't have `execute_values()` from psycopg2.extras, but provides equivalent functionality via `executemany()` with automatic batching, or we can use the `execute_batch` pattern with prepared statements.

**Recommended approach for psycopg3**: Use `cur.executemany()` which automatically batches in psycopg3, OR construct a multi-row VALUES clause manually.

#### Option A: executemany with Prepared Statement (Simplest)

```python
def insert_chunk_idempotent_fast(
    self,
    chunk: gpd.GeoDataFrame,
    table_name: str,
    schema: str,
    batch_id: str
) -> Dict[str, int]:
    """
    Bulk insert using psycopg3 executemany (auto-batched).
    """
    rows_deleted = 0
    rows_inserted = 0

    with self._pg_repo._get_connection() as conn:
        with conn.cursor() as cur:
            # Step 1: DELETE existing rows for this batch
            delete_stmt = sql.SQL("""
                DELETE FROM {schema}.{table}
                WHERE etl_batch_id = %s
            """).format(
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name)
            )
            cur.execute(delete_stmt, (batch_id,))
            rows_deleted = cur.rowcount

            # Step 2: Prepare all values
            reserved_cols = {'id', 'geom', 'geometry', 'etl_batch_id'}
            attr_cols = [col for col in chunk.columns
                        if col != 'geometry' and col.lower() not in reserved_cols]

            # Build values list (much faster than iterrows in a loop with execute)
            all_values = []
            for idx, row in chunk.iterrows():
                geom_wkt = row.geometry.wkt
                if attr_cols:
                    values = (geom_wkt, batch_id, *[row[col] for col in attr_cols])
                else:
                    values = (geom_wkt, batch_id)
                all_values.append(values)

            # Step 3: Build and execute INSERT
            if attr_cols:
                cols_sql = sql.SQL(', ').join([sql.Identifier(col) for col in attr_cols])
                placeholders = sql.SQL(', ').join([sql.Placeholder()] * len(attr_cols))

                insert_stmt = sql.SQL("""
                    INSERT INTO {schema}.{table} (geom, etl_batch_id, {cols})
                    VALUES (ST_GeomFromText(%s, 4326), %s, {placeholders})
                """).format(
                    schema=sql.Identifier(schema),
                    table=sql.Identifier(table_name),
                    cols=cols_sql,
                    placeholders=placeholders
                )
            else:
                insert_stmt = sql.SQL("""
                    INSERT INTO {schema}.{table} (geom, etl_batch_id)
                    VALUES (ST_GeomFromText(%s, 4326), %s)
                """).format(
                    schema=sql.Identifier(schema),
                    table=sql.Identifier(table_name)
                )

            # Execute all inserts in batched manner
            cur.executemany(insert_stmt, all_values)
            rows_inserted = len(all_values)

            conn.commit()

    logger.info(f"✅ Chunk {batch_id}: deleted={rows_deleted}, inserted={rows_inserted}")
    return {'rows_deleted': rows_deleted, 'rows_inserted': rows_inserted}
```

#### Option B: Multi-row VALUES (Single Statement - Fastest)

```python
def insert_chunk_idempotent_multirow(
    self,
    chunk: gpd.GeoDataFrame,
    table_name: str,
    schema: str,
    batch_id: str,
    page_size: int = 5000
) -> Dict[str, int]:
    """
    Bulk insert using multi-row VALUES clause.
    Splits into pages to avoid query size limits.
    """
    rows_deleted = 0
    rows_inserted = 0

    with self._pg_repo._get_connection() as conn:
        with conn.cursor() as cur:
            # Step 1: DELETE existing rows
            delete_stmt = sql.SQL("""
                DELETE FROM {schema}.{table}
                WHERE etl_batch_id = %s
            """).format(
                schema=sql.Identifier(schema),
                table=sql.Identifier(table_name)
            )
            cur.execute(delete_stmt, (batch_id,))
            rows_deleted = cur.rowcount

            # Step 2: Prepare columns
            reserved_cols = {'id', 'geom', 'geometry', 'etl_batch_id'}
            attr_cols = [col for col in chunk.columns
                        if col != 'geometry' and col.lower() not in reserved_cols]

            # Step 3: Build values and insert in pages
            all_rows = list(chunk.itertuples(index=False))

            for page_start in range(0, len(all_rows), page_size):
                page_rows = all_rows[page_start:page_start + page_size]

                # Build VALUES clause
                values_list = []
                params = []

                for row in page_rows:
                    geom_wkt = row.geometry.wkt

                    if attr_cols:
                        # (ST_GeomFromText(%s, 4326), %s, %s, %s, ...)
                        placeholders = ', '.join(['%s'] * (2 + len(attr_cols)))
                        values_list.append(f"(ST_GeomFromText(%s, 4326), %s, {', '.join(['%s'] * len(attr_cols))})")
                        params.extend([geom_wkt, batch_id] + [getattr(row, col) for col in attr_cols])
                    else:
                        values_list.append("(ST_GeomFromText(%s, 4326), %s)")
                        params.extend([geom_wkt, batch_id])

                # Construct full INSERT
                if attr_cols:
                    cols_part = ', '.join([f'"{col}"' for col in attr_cols])
                    insert_sql = f"""
                        INSERT INTO "{schema}"."{table_name}" (geom, etl_batch_id, {cols_part})
                        VALUES {', '.join(values_list)}
                    """
                else:
                    insert_sql = f"""
                        INSERT INTO "{schema}"."{table_name}" (geom, etl_batch_id)
                        VALUES {', '.join(values_list)}
                    """

                cur.execute(insert_sql, params)
                rows_inserted += len(page_rows)

            conn.commit()

    logger.info(f"✅ Chunk {batch_id}: deleted={rows_deleted}, inserted={rows_inserted}")
    return {'rows_deleted': rows_deleted, 'rows_inserted': rows_inserted}
```

### Implementation Steps

1. **Add new method** to `VectorToPostGISHandler`:
   - `insert_chunk_bulk()` - new optimized method

2. **Update Docker handler** (`handler_vector_docker_complete.py`):
   - Change `_upload_chunks_with_checkpoints()` to use new bulk method
   - Increase default chunk_size to 100,000

3. **Update Function App handler** (`process_vector_tasks.py`):
   - Update `process_vector_upload()` to use bulk method

4. **Testing**:
   - Benchmark with roads.kml (483 rows) - baseline
   - Benchmark with acled_export.csv (~2M rows) - at scale

---

## Solution 2: COPY + Temp Table Pattern

**Use Case**: H3 aggregation, GeoParquet ETL, analytics pipelines (1M-100M+ rows)
**Expected Speedup**: 50-100x over row-by-row, 2-5x over execute_values
**Complexity**: Medium - requires temp table management

### Why COPY is Fastest

PostgreSQL's COPY command uses a binary streaming protocol that bypasses:
- SQL parsing overhead
- Per-statement transaction overhead
- Network round-trip latency (streams continuously)

**Challenge**: COPY doesn't support function calls (like `ST_GeomFromText()`) in the data stream.

**Solution**: COPY to temp table with WKT text, then INSERT...SELECT with geometry conversion.

### Implementation

```python
def bulk_insert_with_copy(
    self,
    chunk: gpd.GeoDataFrame,
    table_name: str,
    schema: str,
    batch_id: str,
    srid: int = 4326
) -> Dict[str, int]:
    """
    Maximum performance bulk insert using PostgreSQL COPY.

    Strategy:
    1. Create temp table with WKT text column
    2. COPY data to temp table (binary streaming)
    3. INSERT...SELECT with ST_GeomFromText conversion
    4. Drop temp table

    This achieves 50-100x speedup over row-by-row inserts.
    """
    import io

    rows_deleted = 0
    rows_inserted = 0

    # Prepare column metadata
    reserved_cols = {'id', 'geom', 'geometry', 'etl_batch_id'}
    attr_cols = [col for col in chunk.columns
                if col != 'geometry' and col.lower() not in reserved_cols]

    # Generate unique temp table name
    temp_table = f"_tmp_bulk_{batch_id.replace('-', '_')}"

    with self._pg_repo._get_connection() as conn:
        with conn.cursor() as cur:
            try:
                # ============================================================
                # Step 1: DELETE existing rows for idempotency
                # ============================================================
                delete_stmt = sql.SQL("""
                    DELETE FROM {schema}.{table}
                    WHERE etl_batch_id = %s
                """).format(
                    schema=sql.Identifier(schema),
                    table=sql.Identifier(table_name)
                )
                cur.execute(delete_stmt, (batch_id,))
                rows_deleted = cur.rowcount

                # ============================================================
                # Step 2: Create temp table matching target schema
                # ============================================================
                # Build column definitions for temp table
                col_defs = ["wkt TEXT NOT NULL", "batch_id TEXT NOT NULL"]
                for col in attr_cols:
                    pg_type = self._get_postgres_type(chunk[col].dtype)
                    col_defs.append(f'"{col}" {pg_type}')

                create_temp = f"""
                    CREATE TEMP TABLE {temp_table} (
                        {', '.join(col_defs)}
                    ) ON COMMIT DROP
                """
                cur.execute(create_temp)

                # ============================================================
                # Step 3: COPY data to temp table using binary stream
                # ============================================================
                # Build column list for COPY
                copy_cols = ['wkt', 'batch_id'] + [f'"{col}"' for col in attr_cols]

                copy_sql = f"COPY {temp_table} ({', '.join(copy_cols)}) FROM STDIN"

                with cur.copy(copy_sql) as copy:
                    for _, row in chunk.iterrows():
                        wkt = row.geometry.wkt
                        values = [wkt, batch_id] + [row[col] for col in attr_cols]
                        copy.write_row(values)

                # ============================================================
                # Step 4: INSERT...SELECT with geometry conversion
                # ============================================================
                if attr_cols:
                    attr_cols_sql = ', '.join([f'"{col}"' for col in attr_cols])
                    insert_select = f"""
                        INSERT INTO "{schema}"."{table_name}"
                            (geom, etl_batch_id, {attr_cols_sql})
                        SELECT
                            ST_GeomFromText(wkt, {srid}),
                            batch_id,
                            {attr_cols_sql}
                        FROM {temp_table}
                    """
                else:
                    insert_select = f"""
                        INSERT INTO "{schema}"."{table_name}" (geom, etl_batch_id)
                        SELECT ST_GeomFromText(wkt, {srid}), batch_id
                        FROM {temp_table}
                    """

                cur.execute(insert_select)
                rows_inserted = cur.rowcount

                # Temp table auto-drops on commit (ON COMMIT DROP)
                conn.commit()

            except Exception as e:
                conn.rollback()
                raise

    logger.info(f"✅ COPY bulk insert {batch_id}: deleted={rows_deleted}, inserted={rows_inserted}")
    return {'rows_deleted': rows_deleted, 'rows_inserted': rows_inserted}
```

### WKB Variant (Even Faster for Large Geometries)

For complex geometries, WKB (Well-Known Binary) is more efficient than WKT:

```python
def bulk_insert_with_copy_wkb(
    self,
    chunk: gpd.GeoDataFrame,
    table_name: str,
    schema: str,
    batch_id: str,
    srid: int = 4326
) -> Dict[str, int]:
    """
    COPY with WKB encoding - faster for complex geometries.
    """
    # ... same setup ...

    with cur.copy(copy_sql) as copy:
        for _, row in chunk.iterrows():
            # Use WKB hex encoding
            wkb_hex = row.geometry.wkb_hex
            values = [wkb_hex, batch_id] + [row[col] for col in attr_cols]
            copy.write_row(values)

    # Use ST_GeomFromWKB instead
    insert_select = f"""
        INSERT INTO "{schema}"."{table_name}" (geom, etl_batch_id, {attr_cols_sql})
        SELECT
            ST_SetSRID(ST_GeomFromWKB(decode(wkb, 'hex')), {srid}),
            batch_id,
            {attr_cols_sql}
        FROM {temp_table}
    """
```

### PostgreSQL Repository Integration

Add to `infrastructure/postgres.py`:

```python
class PostgresRepository:
    # ... existing code ...

    def bulk_copy_to_temp(
        self,
        temp_table: str,
        columns: List[str],
        rows: Iterable[tuple]
    ) -> int:
        """
        Bulk COPY rows to a temp table.

        Args:
            temp_table: Temp table name (must already exist)
            columns: Column names in order
            rows: Iterable of tuples matching columns

        Returns:
            Number of rows copied
        """
        copy_sql = f"COPY {temp_table} ({', '.join(columns)}) FROM STDIN"

        row_count = 0
        with self._get_connection() as conn:
            with conn.cursor() as cur:
                with cur.copy(copy_sql) as copy:
                    for row in rows:
                        copy.write_row(row)
                        row_count += 1
                conn.commit()

        return row_count
```

### Implementation Steps

1. **Add PostgresRepository method**:
   - `bulk_copy_to_temp()` - generic COPY helper

2. **Add VectorToPostGISHandler method**:
   - `bulk_insert_with_copy()` - COPY-based bulk insert

3. **Create abstract base class** for bulk inserters:
   - `BulkInserter` protocol with `insert_chunk()` method
   - `ExecuteValuesBulkInserter` - Solution 1
   - `CopyBulkInserter` - Solution 2

4. **Configuration flag**:
   - `BULK_INSERT_METHOD: Literal['executemany', 'copy'] = 'copy'`

5. **Future: H3 aggregation pipeline**:
   - Use COPY pattern for massive H3 cell aggregations
   - GeoParquet → COPY → temp → INSERT...SELECT with H3 functions

---

## Performance Benchmarks (Expected)

| Method | 20K rows | 100K rows | 1M rows | 10M rows |
|--------|----------|-----------|---------|----------|
| Row-by-row | 2.5 min | 12.5 min | 2+ hrs | 20+ hrs |
| executemany | 5-15 sec | 25-60 sec | 5-10 min | 50-100 min |
| COPY + temp | 2-5 sec | 10-25 sec | 2-4 min | 20-40 min |

---

## Migration Plan

### Phase 1: Immediate Fix (execute_values pattern)
1. Add `insert_chunk_bulk()` to VectorToPostGISHandler
2. Update `insert_chunk_idempotent()` to use bulk pattern
3. Increase Docker default chunk_size to 100,000
4. Deploy and verify with acled_export.csv test
5. **Target**: Complete within 1 day

### Phase 2: COPY Infrastructure (for future pipelines)
1. Add `bulk_copy_to_temp()` to PostgresRepository
2. Add `bulk_insert_with_copy()` to VectorToPostGISHandler
3. Add configuration flag to choose method
4. Document for H3 aggregation epic
5. **Target**: Complete within 1 week

### Phase 3: H3/Analytics Pipelines (future)
1. GeoParquet reader with COPY integration
2. H3 aggregation with bulk temp tables
3. Analytics pipeline optimization
4. **Target**: V0.9 or later

---

## Testing Checklist

- [ ] Benchmark baseline (current row-by-row)
- [ ] Benchmark executemany with 20K chunks
- [ ] Benchmark executemany with 100K chunks
- [ ] Benchmark COPY pattern with 100K chunks
- [ ] Verify idempotency (re-run same job)
- [ ] Verify data integrity (row counts, geometry validity)
- [ ] Test with all file formats (KML, ZIP, GPKG, GeoJSON, CSV)
- [ ] Test error handling (connection failures, invalid geometries)
- [ ] Memory profiling with large datasets

---

## Files to Modify

| File | Changes |
|------|---------|
| `services/vector/postgis_handler.py` | Add bulk insert methods |
| `services/handler_vector_docker_complete.py` | Use bulk methods, increase chunk size |
| `services/vector/process_vector_tasks.py` | Use bulk methods for Function App |
| `infrastructure/postgres.py` | Add COPY helper method |
| `config/__init__.py` | Add BULK_INSERT_METHOD config |

---

## Summary

**Immediate action**: Implement `executemany` pattern in `insert_chunk_idempotent()` and increase chunk size to 100K for Docker. This is a minimal change with 10-50x speedup.

**Future-ready**: Add COPY + temp table infrastructure for H3 aggregation and GeoParquet pipelines where 1-100M row ETL will be common.

Both patterns maintain our idempotency guarantees (DELETE + INSERT with batch_id) and follow the existing PostgresRepository patterns.
