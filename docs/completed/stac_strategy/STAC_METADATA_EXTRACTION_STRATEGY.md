# STAC Metadata Extraction Strategy (DRY Analysis)

**Date**: 5 OCT 2025
**Author**: Robert and Geospatial Claude Legion

## üéØ Core Question

**What metadata needs to be extracted for STAC Items, and how much can we delegate to existing libraries (DRY principle)?**

## üìã STAC Item Required Fields (Specification)

### Mandatory Fields
```python
{
    # GeoJSON core fields
    "id": str,                    # Unique identifier
    "type": "Feature",            # Must be "Feature"
    "geometry": {...},            # GeoJSON geometry (footprint)
    "bbox": [xmin, ymin, xmax, ymax],  # Bounding box (required if geometry != null)

    # STAC-specific fields
    "properties": {
        "datetime": str           # ISO 8601 UTC timestamp (REQUIRED)
        # ... additional metadata
    },
    "assets": {...},              # Files/resources
    "links": [...],               # Related resources
    "stac_version": "1.0.0",
    "collection": str             # Collection ID (if part of collection)
}
```

## üîß What Libraries Extract Automatically

### rio-stac: `create_stac_item()`

**Automatically Extracted from GeoTIFF:**
```python
from rio_stac import stac
import rasterio

with rasterio.open("file.tif") as dataset:
    item_dict = stac.create_stac_item(
        dataset,
        id="auto-generated-id",           # Optional - defaults to filename
        collection="collection-id",       # Optional - we provide
        input_datetime=datetime.now(),    # Optional - defaults to NOW or ACQUISITIONDATETIME metadata
        with_proj=True,                   # Add projection extension
        with_raster=True,                 # Add raster extension
        with_eo=True                      # Add EO extension
    )
```

**What rio-stac Extracts Automatically:**
‚úÖ **Geometry** - Footprint polygon from dataset bounds
‚úÖ **BBox** - Bounding box from dataset bounds
‚úÖ **Projection** - EPSG code, WKT2, or PROJJSON from dataset CRS
‚úÖ **Raster Properties** - Band count, data types, nodata values
‚úÖ **Band Statistics** - Min/max/mean/stddev (if `with_raster=True`)
‚úÖ **Media Type** - `image/tiff; application=geotiff; profile=cloud-optimized`
‚úÖ **Asset Definition** - Creates default asset with href

**What We Must Provide:**
‚ùå **Item ID** - Defaults to filename, but we should use semantic IDs
‚ùå **Collection ID** - Must specify which collection
‚ùå **Datetime** - Defaults to NOW, but we should use blob last_modified or custom
‚ùå **Additional Properties** - Custom metadata (Azure container, tier, etc.)
‚ùå **Links** - Collection links, self links, etc.

### stac-pydantic: Validation & Type Safety

**What stac-pydantic Provides:**
‚úÖ **Automatic Schema Validation** - Validates against STAC 1.0.0 spec
‚úÖ **Type Safety** - Pydantic models with full type hints
‚úÖ **Extension Validation** - Validates extension properties against JSON schemas
‚úÖ **Serialization** - Converts to/from JSON with proper formatting

```python
from stac_pydantic import Item

# Validate dict from rio-stac
item = Item(**item_dict)  # Raises ValidationError if invalid

# Access with type safety
item.id              # str
item.geometry        # dict
item.properties      # dict
item.assets          # dict[str, Asset]
```

## üìä Current vs STAC Metadata Comparison

### What We Already Extract (container_list.py)

```python
# From analyze_single_blob()
{
    "blob_name": str,              # ‚úÖ Can use for Item ID
    "blob_path": str,              # ‚úÖ Can use for Asset href
    "container_name": str,         # ‚úÖ Use for collection determination
    "size_bytes": int,             # ‚úÖ Already have
    "size_mb": float,              # ‚úÖ Already have
    "file_extension": str,         # ‚úÖ Filter .tif files
    "content_type": str,           # ‚úÖ Already have
    "last_modified": str,          # ‚úÖ Use for datetime property
    "etag": str,                   # ‚úÖ Already have
    "metadata": dict               # ‚úÖ Azure blob metadata
}
```

### What STAC Needs (Additional)

```python
# From rio-stac extraction
{
    "geometry": {...},             # ‚ùå Need rasterio
    "bbox": [xmin, ymin, xmax, ymax],  # ‚ùå Need rasterio
    "projection": {                # ‚ùå Need rasterio
        "epsg": int,
        "wkt2": str,
        "projjson": {...}
    },
    "raster": {                    # ‚ùå Need rasterio (optional but valuable)
        "bands": [...],
        "statistics": {...}
    },
    "eo": {                        # ‚ùå Need specialized extraction (optional)
        "cloud_cover": float
    }
}
```

## ‚úÖ DRY Strategy: Leverage Libraries Maximally

### Recommended Approach

**DON'T re-implement what rio-stac already does:**
- ‚ùå Don't manually calculate geometry from bounds
- ‚ùå Don't manually parse CRS to get EPSG codes
- ‚ùå Don't manually read band statistics
- ‚ùå Don't manually construct asset definitions

**DO use rio-stac for heavy lifting:**
- ‚úÖ Let rio-stac extract geometry, bbox, projection, raster metadata
- ‚úÖ Let rio-stac construct proper STAC Item structure
- ‚úÖ Let rio-stac handle extension properties

**DO supplement with our existing metadata:**
- ‚úÖ Use our `last_modified` for `datetime` property
- ‚úÖ Use our `container_name` to determine collection
- ‚úÖ Use our `blob_path` to generate Azure Storage URLs
- ‚úÖ Add custom Azure-specific properties

## üèóÔ∏è Proposed Implementation Pattern

### Service: `services/service_stac_metadata.py`

```python
from typing import Dict, Any, Optional
from datetime import datetime
from stac_pydantic import Item, Asset
from rio_stac import stac
import rasterio
from rasterio.session import AWSSession
from infrastructure.blob import BlobRepository


class StacMetadataService:
    """
    Extract STAC metadata from blobs using rio-stac + our existing metadata.

    Strategy: DRY - Leverage libraries for heavy lifting, supplement with our data.
    """

    def __init__(self):
        self.blob_repo = BlobRepository.instance()

    def extract_item_from_blob(
        self,
        container: str,
        blob_name: str,
        collection_id: str,
        existing_metadata: Optional[Dict[str, Any]] = None
    ) -> Item:
        """
        Extract STAC Item from blob using rio-stac.

        Args:
            container: Azure container name
            blob_name: Blob path within container
            collection_id: STAC collection ID
            existing_metadata: Optional metadata from analyze_single_blob()

        Returns:
            Validated stac-pydantic Item

        Strategy:
            1. Generate blob URL with SAS token
            2. Let rio-stac extract geometry, bbox, projection, raster metadata
            3. Supplement with our existing metadata (datetime, Azure properties)
            4. Validate with stac-pydantic
        """
        # Generate blob URL with SAS token for rasterio access
        blob_url = self.blob_repo.generate_sas_url(
            container=container,
            blob_path=blob_name,
            hours=1
        )

        # Determine datetime (prefer existing metadata, fallback to NOW)
        if existing_metadata and existing_metadata.get('last_modified'):
            item_datetime = datetime.fromisoformat(existing_metadata['last_modified'])
        else:
            # rio-stac will use NOW if not provided
            item_datetime = None

        # Generate semantic item ID
        item_id = self._generate_item_id(container, blob_name)

        # LET RIO-STAC DO THE HEAVY LIFTING
        with rasterio.open(blob_url) as dataset:
            item_dict = stac.create_stac_item(
                dataset,
                id=item_id,
                collection=collection_id,
                input_datetime=item_datetime,
                asset_name="data",
                asset_roles=["data"],
                asset_media_type="image/tiff; application=geotiff; profile=cloud-optimized",
                with_proj=True,     # Extract projection extension
                with_raster=True,   # Extract raster extension
                with_eo=False       # EO requires specialized extraction
            )

        # SUPPLEMENT with Azure-specific properties
        item_dict['properties']['azure:container'] = container
        item_dict['properties']['azure:blob_path'] = blob_name
        item_dict['properties']['azure:tier'] = self._determine_tier(container)

        if existing_metadata:
            item_dict['properties']['azure:size_mb'] = existing_metadata.get('size_mb')
            item_dict['properties']['azure:etag'] = existing_metadata.get('etag')
            item_dict['properties']['azure:content_type'] = existing_metadata.get('content_type')

        # VALIDATE with stac-pydantic
        item = Item(**item_dict)

        return item

    def _generate_item_id(self, container: str, blob_name: str) -> str:
        """Generate semantic STAC Item ID."""
        # Example: bronze-rmhazuregeobronze-path-to-file-tif
        tier = self._determine_tier(container)
        safe_name = blob_name.replace('/', '-').replace('.', '-')
        return f"{tier}-{safe_name}"

    def _determine_tier(self, container: str) -> str:
        """Determine tier from container name."""
        container_lower = container.lower()
        if 'bronze' in container_lower:
            return 'bronze'
        elif 'silver' in container_lower:
            return 'silver'
        elif 'gold' in container_lower:
            return 'gold'
        return 'unknown'
```

## üîÑ Integration with Existing Workflow

### Two-Stage Pattern Enhancement

**Current Pattern:**
```
Stage 1: list_container_blobs()  ‚Üí Returns blob names
Stage 2: analyze_single_blob()   ‚Üí Returns blob metadata (size, modified, etc.)
```

**Enhanced with STAC:**
```
Stage 1: list_container_blobs()       ‚Üí Returns blob names
Stage 2: extract_stac_item()          ‚Üí Uses rio-stac + our metadata ‚Üí Insert into PgSTAC
         ‚îú‚îÄ analyze_single_blob()     ‚Üí Get Azure metadata (size, modified)
         ‚îî‚îÄ rio-stac extraction       ‚Üí Get spatial metadata (geometry, bbox, projection)
```

### Code Integration

```python
def extract_and_insert_stac_item(params: dict) -> dict[str, Any]:
    """
    Stage 2: Extract STAC metadata and insert into PgSTAC.

    Replaces: analyze_single_blob()
    Enhances: Now extracts full STAC metadata + inserts into PgSTAC
    """
    try:
        container = params["container_name"]
        blob_name = params["blob_name"]
        collection_id = params.get("collection_id", f"bronze-{container}")

        # 1. Get basic Azure metadata (REUSE existing code)
        blob_metadata = analyze_single_blob(params)
        if not blob_metadata['success']:
            return blob_metadata

        # 2. Extract STAC Item using rio-stac (DRY - don't reimplement)
        stac_service = StacMetadataService()
        item = stac_service.extract_item_from_blob(
            container=container,
            blob_name=blob_name,
            collection_id=collection_id,
            existing_metadata=blob_metadata['result']
        )

        # 3. Insert into PgSTAC
        stac_infra = StacInfrastructure()
        insert_result = stac_infra.insert_item(item, collection_id)

        return {
            'success': True,
            'result': {
                'item_id': item.id,
                'collection': collection_id,
                'blob_metadata': blob_metadata['result'],
                'stac_item': item.model_dump(mode='json'),
                'pgstac_result': insert_result
            }
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__
        }
```

## üìä Metadata Sources Summary

| Field | Source | Method |
|-------|--------|--------|
| **id** | Generated | `_generate_item_id()` |
| **geometry** | rio-stac | `create_stac_item()` |
| **bbox** | rio-stac | `create_stac_item()` |
| **datetime** | Our metadata | `blob.last_modified` |
| **projection** | rio-stac | `with_proj=True` |
| **raster** | rio-stac | `with_raster=True` |
| **assets** | rio-stac | `create_stac_item()` |
| **azure:container** | Our data | Custom property |
| **azure:tier** | Our logic | `_determine_tier()` |
| **azure:size_mb** | Our metadata | `blob.size_mb` |
| **links** | Generated | Collection links |

## ‚úÖ Benefits of This Approach

### 1. DRY Principle
- ‚úÖ Don't reimplement geometry calculation (rio-stac does it)
- ‚úÖ Don't reimplement projection parsing (rio-stac does it)
- ‚úÖ Don't reimplement band statistics (rio-stac does it)
- ‚úÖ Don't reimplement STAC schema validation (stac-pydantic does it)

### 2. Leverage Existing Code
- ‚úÖ Reuse `analyze_single_blob()` for Azure metadata
- ‚úÖ Reuse `BlobRepository.generate_sas_url()` for access
- ‚úÖ Reuse existing filter logic from container operations

### 3. Library Strengths
- ‚úÖ rio-stac: Spatial metadata extraction (geometry, projection)
- ‚úÖ stac-pydantic: Validation & type safety
- ‚úÖ Our code: Azure-specific metadata & workflow orchestration

### 4. Maintainability
- ‚úÖ Libraries handle STAC spec updates
- ‚úÖ Libraries handle edge cases (malformed rasters, projections)
- ‚úÖ Our code focuses on business logic (tier determination, workflow)

## üéØ Implementation Checklist

- [ ] Add `stac-pydantic>=3.4.0` to requirements.txt (already have `rio-stac>=0.9.0`)
- [ ] Create `services/service_stac_metadata.py` with `StacMetadataService`
- [ ] Add `generate_sas_url()` to `BlobRepository` (for rasterio access)
- [ ] Add `insert_item()` to `infrastructure/stac.py`
- [ ] Create `extract_and_insert_stac_item()` function
- [ ] Create HTTP trigger for STAC extraction workflow
- [ ] Test with Bronze container
- [ ] Verify Items appear in PgSTAC

## üîë Key Takeaway

**Maximize DRY by delegating to libraries:**

- **rio-stac** ‚Üí Geometry, BBox, Projection, Raster metadata
- **stac-pydantic** ‚Üí Validation, Type safety, Serialization
- **Our code** ‚Üí Azure metadata, Workflow orchestration, Custom properties

**Result**: Minimal custom code, maximum reliability, easy maintenance.

---

## üó∫Ô∏è IMPORTANT: TiTiler-pgSTAC Visualization Requirements

**Date Added**: November 15, 2025

### Critical Metadata for Map Viewers

When creating STAC items for use with TiTiler-pgSTAC, **ensure proper geographic extent metadata** is included. This is essential for map viewers to automatically zoom to the correct location.

#### ‚ö†Ô∏è Known Limitation

TiTiler-pgSTAC's search registration endpoint **does NOT compute or cache geographic bounds** from search results. The TileJSON endpoint returns global bounds (`-180, -85, 180, 85`) instead of actual data extent, causing map viewers to default to a global view.

#### ‚úÖ Solution: Proper STAC Item Metadata

Ensure every STAC item has accurate `bbox` and `geometry`:

```python
def create_stac_item_from_cog(cog_path, item_id, collection_id):
    """Create STAC item with proper bounds for TiTiler visualization"""

    with rasterio.open(cog_path) as src:
        bounds = src.bounds  # (minx, miny, maxx, maxy)

        # Reproject to WGS84 if needed
        if src.crs != 'EPSG:4326':
            from rasterio.warp import transform_bounds
            bbox_wgs84 = transform_bounds(src.crs, 'EPSG:4326', *bounds)
        else:
            bbox_wgs84 = bounds

        # Create proper geometry
        from shapely.geometry import box, mapping
        geometry = mapping(box(*bbox_wgs84))

    item = {
        "type": "Feature",
        "stac_version": "1.0.0",
        "id": item_id,
        "collection": collection_id,
        "bbox": list(bbox_wgs84),  # ‚úÖ CRITICAL: Accurate bbox in WGS84
        "geometry": geometry,       # ‚úÖ CRITICAL: Accurate geometry
        "properties": {
            "datetime": "2019-08-14T00:00:00Z",
            # ... other properties
        },
        "assets": {
            "data": {
                "href": f"/vsiaz/container/{item_id}.tif",
                "type": "image/tiff; application=geotiff",
                "roles": ["data"]
            }
        }
    }

    return item
```

#### üìä Collection-Level Extent

Also ensure collections have proper spatial extent calculated from all items:

```python
def update_collection_extent(collection_id):
    """
    Calculate and update collection extent from all items.
    Essential for client applications to know data coverage.
    """

    query = """
    SELECT
        ST_XMin(ST_Extent(geometry)) as minx,
        ST_YMin(ST_Extent(geometry)) as miny,
        ST_XMax(ST_Extent(geometry)) as maxx,
        ST_YMax(ST_Extent(geometry)) as maxy,
        MIN(datetime) as min_datetime,
        MAX(datetime) as max_datetime
    FROM pgstac.items
    WHERE collection = %s
    """

    result = cursor.execute(query, (collection_id,))
    extent = result.fetchone()

    collection_extent = {
        "spatial": {
            "bbox": [[
                extent['minx'],
                extent['miny'],
                extent['maxx'],
                extent['maxy']
            ]]
        },
        "temporal": {
            "interval": [[
                extent['min_datetime'],
                extent['max_datetime']
            ]]
        }
    }

    # Update collection
    update_query = """
    UPDATE pgstac.collections
    SET content = jsonb_set(content, '{extent}', %s::jsonb)
    WHERE id = %s
    """

    cursor.execute(update_query, (json.dumps(collection_extent), collection_id))
```

#### üéØ Why This Matters

**Without proper bounds:**
- Map viewers start at global extent (0¬∞, 0¬∞ - off the coast of Africa)
- Users must manually search/pan to find imagery
- Poor user experience for tile visualization

**With proper bounds:**
- Clients can query collection extent
- Map applications can zoom to data automatically
- Better integration with GIS tools
- Proper spatial indexing in pgSTAC

#### üìù ETL Checklist

When implementing STAC extraction in your ETL pipeline:

- [ ] ‚úÖ Extract accurate `bbox` from each COG using `rasterio`
- [ ] ‚úÖ Reproject bounds to WGS84 (EPSG:4326) if source CRS differs
- [ ] ‚úÖ Create proper GeoJSON `geometry` from bounds
- [ ] ‚úÖ Use `rio-stac.create_stac_item()` which handles this automatically
- [ ] ‚úÖ After ingesting all items, calculate and update collection `extent`
- [ ] ‚úÖ Verify items have valid geometries: `SELECT COUNT(*) FROM pgstac.items WHERE geometry IS NULL`
- [ ] ‚úÖ Test visualization in TiTiler-pgSTAC map viewer

#### üîó Related Documentation

- TiTiler-pgSTAC Search API: Limited to search query parameters, doesn't compute bounds
- STAC Specification: `bbox` is required when `geometry` is non-null
- pgSTAC: Uses PostGIS for spatial indexing, requires valid geometries
