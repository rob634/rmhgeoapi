# Product Backlog - Geospatial ETL Platform

**Last Updated**: 18 DEC 2025
**Framework**: SAFe (Scaled Agile Framework)

---

## ğŸ“‹ BACKLOG MANAGEMENT RULES

> **All future updates to this file MUST follow SAFe structure:**
> - Organize work under **Epics** (strategic initiatives)
> - Break Epics into **Features** (deliverable capabilities)
> - Break Features into **Stories** (1-3 day atomic tasks)
> - Use **Enablers** for technical debt/infrastructure
> - Use **Spikes** for research/investigation

**Status Icons:**
- âœ… Complete
- ğŸŸ¢ In Progress
- â¬œ Ready (refined, can start)
- ğŸ“‹ Planned (needs refinement)
- ğŸ”µ Backlog (future)

---

## ğŸ¯ CURRENT PROGRAM INCREMENT (PI 2025.4)

**PI Objectives:**
1. Complete Data Access Simplification (E1) - Reader App Migration
2. Implement Data Publishing workflow (E2)
3. Begin Climate Data Virtualization OR Vector Styling (client priority)

---

# EPICS

## Epic E1: Data Access Simplification

**Business Outcome**: Reduce client complexity for raster/xarray queries
**Status**: ğŸŸ¢ Near Complete

### Feature F1.1: Service Layer API âœ… COMPLETE

**Delivered**: `/api/raster/` and `/api/xarray/` endpoints in rmhazuregeoapi

| Story | Status | Notes |
|-------|--------|-------|
| S1.1.1: TiTiler client service | âœ… | `services/titiler_client.py` |
| S1.1.2: STAC client service | âœ… | `services/stac_client.py` |
| S1.1.3: xarray reader service | âœ… | `services/xarray_reader.py` |
| S1.1.4: Raster extract endpoint | âœ… | `/api/raster/extract/{collection}/{item}` |
| S1.1.5: Raster point endpoint | âœ… | `/api/raster/point/{collection}/{item}` |
| S1.1.6: Raster clip endpoint | âœ… | `/api/raster/clip/{collection}/{item}` |
| S1.1.7: Raster preview endpoint | âœ… | `/api/raster/preview/{collection}/{item}` |
| S1.1.8: xarray point time-series | âœ… | `/api/xarray/point/{collection}/{item}` |
| S1.1.9: xarray statistics | âœ… | `/api/xarray/statistics/{collection}/{item}` |
| S1.1.10: xarray aggregate | âœ… | `/api/xarray/aggregate/{collection}/{item}` |
| S1.1.11: Error handling + validation | âœ… | bbox, date range, STAC lookup errors |
| S1.1.12: STAC caching | âœ… | TTL cache (5min items, 1hr collections) |

**API Reference**: See `/SERVICE-LAYER-API-DESIGN.md`

---

### Feature F1.2: Reader App Migration â¬œ READY

**Goal**: Migrate raster_api/xarray_api to rmhogcstac for clean separation
**Depends On**: F1.1 âœ…

| Story | Status | Acceptance Criteria |
|-------|--------|---------------------|
| S1.2.1: Copy raster_api module | â¬œ | Module exists in rmhogcstac |
| S1.2.2: Copy xarray_api module | â¬œ | Module exists in rmhogcstac |
| S1.2.3: Copy service clients | â¬œ | titiler_client, stac_client, xarray_reader |
| S1.2.4: Update requirements.txt | â¬œ | xarray, zarr, httpx added |
| S1.2.5: Register routes | â¬œ | Routes in rmhogcstac function_app.py |
| S1.2.6: Deploy and validate | â¬œ | All endpoints return correct responses |

**Deliverable**: Read-only queries in rmhogcstac, ETL in rmhazuregeoapi

---

## Epic E2: Data Governance & Publishing

**Business Outcome**: Controlled data approval workflow with access tiers
**Status**: ğŸŸ¢ Partially Complete

### Feature F2.1: Managed Datasets Infrastructure âœ… COMPLETE

**Delivered**: System-maintained datasets from external sources (WDPA, FATHOM, ACLED)

| Story | Status | Location |
|-------|--------|----------|
| S2.1.1: Data models | âœ… | `core/models/curated.py` |
| S2.1.2: Database schema | âœ… | `app.curated_datasets`, `app.curated_update_log` |
| S2.1.3: Repository layer | âœ… | `infrastructure/curated_repository.py` |
| S2.1.4: Registry service | âœ… | `services/curated/registry_service.py` |
| S2.1.5: HTTP CRUD endpoints | âœ… | `/api/curated/datasets` |
| S2.1.6: Timer scheduler | âœ… | 2 AM UTC daily check |
| S2.1.7: 4-stage update job | âœ… | `jobs/curated_update.py` |
| S2.1.8: WDPA handler | âœ… | `services/curated/wdpa_handler.py` |

---

### Feature F2.2: Data Publishing Workflow ğŸ“‹ PLANNED

**Goal**: Human-approved data promotion with access control via storage zones

**Terminology:**
| Term | Definition |
|------|------------|
| **Published** | User-submitted data that passed human review |
| **Managed** | System-maintained datasets from external sources |

**Architecture:**
```
User Submits â†’ [PENDING] â†’ Human Review â†’ APPROVE â†’ Access Zone
                                              â†“
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â–¼               â–¼               â–¼
                         Internal Use    Internal Copy    External Zone
                         (in-place)      (ADF copy)       (ADF to ext storage)
```

| Story | Status | Acceptance Criteria |
|-------|--------|---------------------|
| S2.2.1: Design publish schema | â¬œ | `app.publish_queue`, `app.published_datasets`, `app.publish_audit_log` |
| S2.2.2: Create Publishing repository | â¬œ | CRUD for publish queue |
| S2.2.3: Submit for review endpoint | â¬œ | `POST /api/publish/submit/{dataset_id}` |
| S2.2.4: List pending reviews | â¬œ | `GET /api/publish/queue` |
| S2.2.5: Approve endpoint (Action A) | â¬œ | `POST /api/publish/approve/{dataset_id}` - in-place |
| S2.2.6: Reject endpoint | â¬œ | `POST /api/publish/reject/{dataset_id}` |
| S2.2.7: Status check endpoint | â¬œ | `GET /api/publish/status/{dataset_id}` |
| S2.2.8: Action B - internal copy | ğŸ“‹ | ADF job for container/schema copy |
| S2.2.9: Action C - external zone | ğŸ“‹ | ADF job for external storage |
| S2.2.10: Unpublish placeholder | ğŸ”µ | Future enhancement |

---

### Feature F2.3: Managed Dataset Handlers â¬œ READY

| Story | Status | Description |
|-------|--------|-------------|
| S2.3.1: Manual update trigger | â¬œ | Connect `/api/curated/datasets/{id}/update` to job |
| S2.3.2: FATHOM handler | â¬œ | Flood data integration |
| S2.3.3: Admin0 handler | ğŸ“‹ | Natural Earth boundaries |
| S2.3.4: Style integration | ğŸ“‹ | Auto-create OGC styles (depends on E4) |

---

## Epic E3: Climate Data Virtualization

**Business Outcome**: Eliminate unnecessary NetCDFâ†’Zarr conversion (save weeks + 2x storage)
**Status**: ğŸ“‹ Planned

### Feature F3.1: Virtual Zarr Pipeline ğŸ“‹ PLANNED

**Problem**: Client converting 20-100GB CMIP6 NetCDF to physical Zarr unnecessarily.
**Solution**: Kerchunk reference files that make NetCDF accessible as virtual Zarr.

| Story | Status | Acceptance Criteria |
|-------|--------|---------------------|
| S3.1.1: CMIP6 filename parser | â¬œ | Parse variable, model, scenario from filename |
| S3.1.2: Chunking validator | â¬œ | Pre-flight NetCDF compatibility check |
| S3.1.3: Reference generator | â¬œ | Single file â†’ Kerchunk JSON |
| S3.1.4: Virtual combiner | â¬œ | Combine time series references |
| S3.1.5: STAC datacube registration | â¬œ | xarray-compatible STAC items |
| S3.1.6: Inventory job | â¬œ | Scan and group CMIP6 files |
| S3.1.7: Generate job | â¬œ | Full pipeline orchestration |
| S3.1.8: TiTiler-xarray config | â¬œ | Serve virtual Zarr as tiles |

**Dependencies**: `virtualizarr`, `kerchunk`, `h5netcdf`, `h5py`

---

## Epic E4: Vector Styling System

**Business Outcome**: Server-side OGC styles for map rendering
**Status**: ğŸ“‹ Planned

### Feature F4.1: OGC API Styles ğŸ“‹ PLANNED

**Solution**: CartoSym-JSON canonical storage with multi-format output

| Story | Status | Acceptance Criteria |
|-------|--------|---------------------|
| S4.1.1: Pydantic models | â¬œ | CartoSym-JSON schema models |
| S4.1.2: Style translator service | â¬œ | CartoSym â†’ Leaflet/Mapbox GL |
| S4.1.3: Repository methods | â¬œ | CRUD for `geo.feature_collection_styles` |
| S4.1.4: Service orchestration | â¬œ | Style lookup and format conversion |
| S4.1.5: List styles endpoint | â¬œ | `GET /features/collections/{id}/styles` |
| S4.1.6: Get style endpoint | â¬œ | `GET /features/collections/{id}/styles/{sid}` |
| S4.1.7: Schema migration | â¬œ | Add `geo.feature_collection_styles` table |
| S4.1.8: ETL style integration | ğŸ“‹ | Auto-create default styles on ingest |

---

## Epic E5: Platform Observability

**Business Outcome**: Remote diagnostics without direct DB access
**Status**: ğŸŸ¢ Partially Complete

### Feature F5.1: Health & Diagnostics âœ… MOSTLY COMPLETE

| Story | Status | Notes |
|-------|--------|-------|
| S5.1.1: Enhanced health endpoint | âœ… | `schema_summary`, `database_config` |
| S5.1.2: Platform status for DDH | âœ… | `/api/platform/health`, `/stats`, `/failures` |
| S5.1.3: 29 dbadmin endpoints | âœ… | Comprehensive inspection |
| S5.1.4: Verbose pre-flight validation | ğŸ”µ | Job context in error messages |
| S5.1.5: Unified DEBUG_MODE | ğŸ”µ | Consistent verbose behavior |

---

## Epic E6: Enterprise Integration

**Business Outcome**: ADF pipelines for stagingâ†’production, audit logging
**Status**: ğŸ“‹ Ready

### Feature F6.1: Azure Data Factory Integration ğŸ“‹ READY

**Code complete**, needs Azure resource creation.

| Story | Status | Acceptance Criteria |
|-------|--------|---------------------|
| S6.1.1: Create ADF instance | â¬œ | `az datafactory create` |
| S6.1.2: Create copy pipeline | â¬œ | `copy_staging_to_production` |
| S6.1.3: Add env variables | â¬œ | ADF config in Function App |
| S6.1.4: Test repository factory | â¬œ | `create_data_factory_repository()` works |
| S6.1.5: Create promote_data job | â¬œ | Job triggers ADF pipeline |

---

# ENABLERS (Technical Debt)

## Enabler: PgSTAC Repository Consolidation

**Purpose**: Fix "Collection not found after insertion" - two classes manage pgSTAC data
**Status**: ğŸ”µ Backlog

| Task | Status |
|------|--------|
| Rename PgStacInfrastructure â†’ PgStacBootstrap | â¬œ |
| Move data operations to PgStacRepository | â¬œ |
| Remove duplicate methods | â¬œ |
| Update StacMetadataService | â¬œ |

---

## Enabler: Repository Pattern Enforcement

**Purpose**: Eliminate direct database connections
**Status**: ğŸ”µ Backlog

**Problem**: 5+ files bypass `PostgreSQLRepository`

| Task | Status |
|------|--------|
| Fix `triggers/schema_pydantic_deploy.py` | â¬œ |
| Fix `triggers/db_query.py` | â¬œ |
| Fix `core/schema/deployer.py` | â¬œ |
| Create PgSTACRepository | â¬œ |
| Update vector handlers | â¬œ |

---

## Enabler: Dead Code Audit

**Purpose**: Remove orphaned code, reduce maintenance burden
**Status**: ğŸ”µ Backlog

| Task | Status |
|------|--------|
| Audit `core/` folder | â¬œ |
| Audit `infrastructure/` folder | â¬œ |
| Remove commented-out code | â¬œ |
| Update FILE_CATALOG.md | â¬œ |

---

# FUTURE BACKLOG

## Feature: Docker Worker for Long-Running GDAL

**Problem**: Azure Functions 30-minute timeout
**Solution**: Separate Docker worker on Azure Web App
**Status**: ğŸ”µ Backlog
**Reference**: `/GDAL_WORKER.md`

---

## Feature: Function App Separation

**Problem**: Single host.json can't optimize for both raster (2-8GB, low concurrency) and vector (200MB, high concurrency)
**Status**: ğŸ”µ Backlog
**Reference**: `/PRODUCTION_ARCHITECTURE.md`

---

## Feature: Service Bus Sessions

**Purpose**: Per-job FIFO ordering at high volume
**Status**: ğŸ”µ Backlog
**Trigger**: When stage_complete timeouts observed

---

## Feature: Azure API Management

**Purpose**: Route single domain to specialized Function Apps
**Status**: ğŸ”µ Backlog
**Depends On**: Function App Separation

---

## Feature: Unpublish Workflows âœ… IMPLEMENTED

**Status**: âœ… Code complete, needs deploy + test with `dry_run=true`

Files:
- `jobs/unpublish_raster.py`
- `jobs/unpublish_vector.py`
- `services/unpublish_handlers.py`
- `core/models/unpublish.py`
- `infrastructure/validators.py` (stac_item_exists, stac_collection_exists)

---

## Feature: Janitor Blob Cleanup ğŸ”µ BACKLOG

**Status**: Database cleanup done, blob cleanup deferred

| Task | Status |
|------|--------|
| Add `delete_blobs_by_prefix()` | ğŸ”µ |
| Integrate into JanitorService | ğŸ”µ |

---

## Feature: Sensor Metadata Extraction ğŸ”µ BACKLOG

**Purpose**: Extract EXIF/TIFF tags, track provenance

| Task | Status |
|------|--------|
| Create `services/sensor_metadata.py` | ğŸ”µ |
| Add `stac:processing_history` extension | ğŸ”µ |
| Update `process_raster_v2` | ğŸ”µ |

---

## Feature: Dynamic OpenAPI Documentation ğŸ”µ BACKLOG

**Purpose**: Generate interactive API docs from OpenAPI spec
**Current**: Static HTML in `web_interfaces/docs/interface.py`

---

# âœ… RECENTLY COMPLETED

See `HISTORY.md` for full details:

| Date | Item |
|------|------|
| 18 DEC 2025 | Service Layer API Phase 4 complete |
| 12 DEC 2025 | Unpublish workflows implemented |
| 11 DEC 2025 | Service Bus queue standardization |
| 07 DEC 2025 | Container inventory consolidation |
| 05 DEC 2025 | JPEG COG compression fix |

---

**Last Updated**: 18 DEC 2025 (Restructured to SAFe framework)
