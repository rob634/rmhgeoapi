# ============================================================================
# CLAUDE CONTEXT - REPOSITORY
# ============================================================================
# CATEGORY: AZURE RESOURCE REPOSITORIES
# PURPOSE: Azure SDK wrapper providing data access abstraction
# EPOCH: Shared by all epochs (infrastructure layer)# PURPOSE: Azure Service Bus repository for high-volume message operations with batch support
# EXPORTS: ServiceBusRepository - Singleton implementation for Service Bus operations
# INTERFACES: IQueueRepository - Implements queue operation interface for compatibility
# PYDANTIC_MODELS: Accepts BaseModel instances for message sending, supports batch operations
# DEPENDENCIES: azure-servicebus, azure-identity, threading, json, asyncio
# SOURCE: Azure Service Bus via DefaultAzureCredential
# SCOPE: High-volume task messaging operations (thousands to millions of messages)
# VALIDATION: Batch size limits, message size validation, retry logic
# PATTERNS: Singleton, Repository, Batch Processing, Connection pooling
# ENTRY_POINTS: ServiceBusRepository.instance(), batch_send_messages(), async operations
# INDEX: ServiceBusRepository:50, batch_send_messages:300, async_batch_send:450
# ============================================================================

"""
Service Bus Repository Implementation

High-performance message repository for Azure Service Bus with batch support.
Designed for scenarios where Queue Storage times out (>1000 messages),
particularly for H3 hexagon processing and container file listing tasks.

Key Features:
- Batch sending (up to 100 messages per batch)
- Async support for massive parallelization
- Automatic retry with exponential backoff
- Dead letter queue handling
- Session support for ordered processing
- Singleton pattern for credential reuse

Performance:
- Queue Storage: 50ms per message Ã— 1000 = 50 seconds (times out)
- Service Bus: 100 messages in ~200ms Ã— 10 = 2 seconds total

Author: Robert and Geospatial Claude Legion
Date: 25 SEP 2025
"""

from azure.servicebus import ServiceBusClient, ServiceBusMessage, ServiceBusSender, ServiceBusReceiver
from azure.servicebus.aio import ServiceBusClient as AsyncServiceBusClient
from azure.identity import DefaultAzureCredential
from azure.core.exceptions import ServiceRequestError
from typing import Optional, List, Dict, Any, Union
import json
import logging
import threading
import asyncio
import time
import os
from datetime import datetime, timezone, timedelta
from pydantic import BaseModel
from dataclasses import dataclass

from interfaces.repository import IQueueRepository
from util_logger import LoggerFactory, ComponentType

logger = LoggerFactory.create_logger(ComponentType.REPOSITORY, "ServiceBusRepository")


@dataclass
class BatchResult:
    """Result of a batch send operation."""
    success: bool
    messages_sent: int
    batch_count: int
    elapsed_ms: float
    errors: List[str] = None


class ServiceBusRepository(IQueueRepository):
    """
    High-performance Service Bus repository with batch support.

    This implementation provides both synchronous and asynchronous operations,
    with special optimizations for batch sending thousands of messages.

    Key Design Decisions:
    - Implements IQueueRepository for compatibility with existing code
    - Adds batch-specific methods for high-volume operations
    - Uses connection pooling for sender/receiver reuse
    - Thread-safe singleton pattern

    Configuration (via environment variables):
    - SERVICE_BUS_NAMESPACE: Service Bus namespace (e.g., "mynamespace")
    - SERVICE_BUS_CONNECTION_STRING: Full connection string (optional, uses DefaultAzureCredential if not provided)
    - SERVICE_BUS_MAX_BATCH_SIZE: Max messages per batch (default: 100)
    - SERVICE_BUS_RETRY_COUNT: Number of retries (default: 3)
    """

    _instance: Optional['ServiceBusRepository'] = None
    _lock = threading.Lock()

    def __new__(cls):
        """Thread-safe singleton creation."""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """Initialize Service Bus client with credential management."""
        if not hasattr(self, '_initialized'):
            logger.info("ðŸšŒ Initializing ServiceBusRepository")

            try:
                # Get configuration from centralized config
                from config import get_config
                config = get_config()

                logger.debug("ðŸ” Checking Service Bus configuration...")
                logger.debug(f"  Connection string: {'SET' if config.service_bus_connection_string else 'NOT SET'}")
                logger.debug(f"  Namespace: {config.service_bus_namespace or 'NOT SET'}")
                logger.debug(f"  Max batch size: {config.service_bus_max_batch_size}")
                logger.debug(f"  Retry count: {config.service_bus_retry_count}")

                # Check for connection string first (for local development)
                connection_string = config.service_bus_connection_string

                if connection_string:
                    logger.info("ðŸ”‘ Using connection string authentication")
                    logger.debug(f"Connection string length: {len(connection_string)} chars")

                    try:
                        self.client = ServiceBusClient.from_connection_string(connection_string)
                        logger.info("âœ… ServiceBusClient created from connection string")
                    except Exception as cs_error:
                        logger.error(f"âŒ Failed to create client from connection string: {cs_error}")
                        raise

                    self.async_client = None  # Will create on demand
                else:
                    # Use DefaultAzureCredential for production
                    logger.info("ðŸ” Using DefaultAzureCredential (no connection string found)")

                    # Azure Functions Service Bus configuration
                    fully_qualified_namespace = config.service_bus_namespace

                    if not fully_qualified_namespace:
                        logger.error("âŒ Service Bus namespace not configured")
                        logger.error("Please set SERVICE_BUS_NAMESPACE or ServiceBusConnection__fullyQualifiedNamespace")

                        raise ValueError(
                            "ServiceBusConnection__fullyQualifiedNamespace environment variable not set. "
                            "This is required for Azure Functions Service Bus connection. "
                            "Please configure this in Azure Functions Application Settings."
                        )

                    logger.info(f"ðŸšŒ Using Service Bus namespace: {fully_qualified_namespace}")

                    # Use DefaultAzureCredential as recommended by Azure docs
                    # This tries managed identity first in Azure Functions
                    logger.info("ðŸ” Creating DefaultAzureCredential for Service Bus...")
                    try:
                        self.credential = DefaultAzureCredential()
                        logger.info("âœ… DefaultAzureCredential created successfully")
                    except Exception as cred_error:
                        logger.error(f"âŒ Failed to create DefaultAzureCredential: {cred_error}")
                        raise RuntimeError(f"Credential creation failed: {cred_error}")

                    try:
                        logger.debug(f"ðŸ“¦ Creating ServiceBusClient with namespace: {fully_qualified_namespace}")
                        self.client = ServiceBusClient(
                            fully_qualified_namespace=fully_qualified_namespace,
                            credential=self.credential
                        )
                        logger.info("âœ… ServiceBusClient created successfully")
                    except Exception as client_error:
                        logger.error(f"âŒ Failed to create ServiceBusClient: {client_error}")
                        logger.error(f"Namespace used: {fully_qualified_namespace}")
                        raise RuntimeError(f"ServiceBusClient creation failed: {client_error}")

                    # Async client for batch operations
                    try:
                        self.async_client = AsyncServiceBusClient(
                            fully_qualified_namespace=fully_qualified_namespace,
                            credential=self.credential
                        )
                        logger.debug("âœ… AsyncServiceBusClient created")
                    except Exception as async_error:
                        logger.warning(f"âš ï¸ Failed to create AsyncServiceBusClient: {async_error}")
                        self.async_client = None

                # Configuration
                self.max_batch_size = config.service_bus_max_batch_size
                self.max_retries = config.service_bus_retry_count
                self.retry_delay = 1  # seconds

                # Cache for senders only (senders can be safely reused)
                # NOTE: Receivers are NOT cached - they're lightweight and context managers close them.
                # Creating fresh receivers each time avoids "handler shutdown" errors.
                self._senders: Dict[str, ServiceBusSender] = {}

                self._initialized = True
                logger.info(f"âœ… ServiceBusRepository initialized (batch_size={self.max_batch_size})")

            except Exception as e:
                logger.error(f"âŒ Failed to initialize ServiceBusRepository: {e}")
                import traceback
                logger.error(f"Full initialization error: {traceback.format_exc()}")
                raise RuntimeError(f"ServiceBusRepository initialization failed: {e}")

    @classmethod
    def instance(cls) -> 'ServiceBusRepository':
        """Get singleton instance."""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def _get_sender(self, queue_or_topic: str) -> ServiceBusSender:
        """Get or create a message sender with error handling."""
        if queue_or_topic not in self._senders:
            logger.debug(f"ðŸšŒ Creating new sender for queue: {queue_or_topic}")
            try:
                sender = self.client.get_queue_sender(queue_or_topic)
                logger.debug(f"âœ… Sender created for queue: {queue_or_topic}")
                self._senders[queue_or_topic] = sender
            except Exception as sender_error:
                logger.error(f"âŒ Failed to create sender for queue '{queue_or_topic}': {sender_error}")
                logger.error(f"Error type: {type(sender_error).__name__}")

                # Check for specific error types
                error_msg = str(sender_error).lower()
                if 'not found' in error_msg or '404' in error_msg:
                    logger.error(f"Queue '{queue_or_topic}' does not exist in Service Bus namespace")
                elif 'unauthorized' in error_msg or '401' in error_msg:
                    logger.error(f"Authentication failed - check managed identity or connection string")
                elif 'forbidden' in error_msg or '403' in error_msg:
                    logger.error(f"Access denied to queue '{queue_or_topic}' - check permissions")

                raise RuntimeError(f"Cannot create sender for queue '{queue_or_topic}': {sender_error}")
        else:
            logger.debug(f"â™»ï¸ Reusing existing sender for queue: {queue_or_topic}")

        return self._senders[queue_or_topic]

    def _get_receiver(self, queue_or_topic: str) -> ServiceBusReceiver:
        """
        Get a fresh message receiver (no caching).

        Receivers are NOT cached because:
        1. They're closed by context managers (with receiver:)
        2. Creating new receivers is lightweight (just creates a proxy object)
        3. Caching closed receivers causes "handler shutdown" errors

        This is the correct pattern for Service Bus SDK - unlike database connections,
        Service Bus receivers are designed to be created/destroyed frequently.
        """
        logger.debug(f"ðŸšŒ Creating fresh receiver for: {queue_or_topic}")
        return self.client.get_queue_receiver(queue_or_topic)

    # ========================================================================
    # IQueueRepository Implementation (for compatibility)
    # ========================================================================

    def send_message(self, queue_name: str, message: BaseModel) -> str:
        """
        Send a single message to Service Bus.

        Implements IQueueRepository interface for compatibility with existing code.
        For high-volume scenarios, use batch_send_messages() instead.

        Args:
            queue_name: Target queue/topic name
            message: Pydantic model to send

        Returns:
            Message ID (generated)
        """
        logger.debug(f"ðŸšŒ send_message called for queue: {queue_name}")
        logger.debug(f"ðŸ“¦ Message type: {type(message).__name__}")

        # Step 1: Get sender
        try:
            logger.debug(f"ðŸ”Œ Getting sender for queue: {queue_name}")
            sender = self._get_sender(queue_name)
            logger.debug(f"âœ… Sender obtained successfully")
        except Exception as sender_error:
            logger.error(f"âŒ Failed to get sender for {queue_name}: {sender_error}")
            raise RuntimeError(f"Failed to get sender: {str(sender_error)}")

        # Step 2: Serialize message
        try:
            logger.debug(f"ðŸ“ Serializing message to JSON")
            message_json = message.model_dump_json()
            logger.debug(f"âœ… Message serialized, length: {len(message_json)} chars")
        except Exception as json_error:
            logger.error(f"âŒ Failed to serialize message to JSON: {json_error}")
            raise RuntimeError(f"Message serialization failed: {str(json_error)}")

        # Step 3: Create Service Bus message
        try:
            logger.debug(f"ðŸ“¨ Creating ServiceBusMessage object")
            sb_message = ServiceBusMessage(
                body=message_json,
                content_type="application/json",
                time_to_live=timedelta(hours=24),  # Configurable TTL
                application_properties={}  # Initialize properties dict
            )
            logger.debug(f"âœ… ServiceBusMessage created successfully")
        except Exception as msg_error:
            logger.error(f"âŒ Failed to create ServiceBusMessage: {msg_error}")
            logger.error(f"Message body type: {type(message_json)}, length: {len(message_json) if message_json else 'None'}")
            raise RuntimeError(f"ServiceBusMessage creation failed: {str(msg_error)}")

        # Step 4: Add metadata properties
        try:
            logger.debug(f"ðŸ·ï¸ Adding metadata properties")
            if hasattr(message, 'task_id'):
                logger.debug(f"  Adding task_id: {message.task_id}")
                sb_message.application_properties['task_id'] = message.task_id
            if hasattr(message, 'job_id'):
                logger.debug(f"  Adding job_id: {message.job_id}")
                sb_message.application_properties['job_id'] = message.job_id
            logger.debug(f"âœ… Metadata properties added: {sb_message.application_properties}")
        except Exception as prop_error:
            logger.error(f"âŒ Failed to add metadata properties: {prop_error}")
            logger.error(f"application_properties type: {type(sb_message.application_properties)}")
            raise RuntimeError(f"Failed to add properties: {str(prop_error)}")

        logger.debug(f"ðŸ“¤ Ready to send message to Service Bus: {queue_name}")

        for attempt in range(self.max_retries):
            try:
                logger.debug(f"ðŸ“® Send attempt {attempt + 1}/{self.max_retries}")

                # Step 5: Send the message
                logger.debug(f"ðŸ“¤ Sending message without context manager (sender is cached)")
                sender.send_messages(sb_message)
                logger.debug(f"âœ… send_messages() completed successfully")

                # Step 6: Generate message ID for compatibility
                try:
                    message_id = sb_message.message_id or f"sb_{datetime.now(timezone.utc).timestamp()}"
                    logger.info(f"âœ… Message sent to Service Bus. ID: {message_id}")
                    return message_id
                except Exception as id_error:
                    logger.warning(f"âš ï¸ Could not get message_id, using timestamp: {id_error}")
                    message_id = f"sb_{datetime.now(timezone.utc).timestamp()}"
                    return message_id

            except Exception as e:
                error_msg = str(e)
                error_type = type(e).__name__
                logger.warning(f"âš ï¸ Attempt {attempt + 1}/{self.max_retries} failed")
                logger.warning(f"   Error type: {error_type}")
                logger.warning(f"   Error message: {error_msg}")

                # Check for specific error types
                if "from_env" in error_msg.lower():
                    logger.error("âŒ Authentication error: DefaultAzureCredential failed to authenticate")
                    logger.error("Ensure managed identity is configured or connection string is provided")
                    raise RuntimeError(f"Service Bus authentication failed: {error_msg}")

                if attempt == self.max_retries - 1:
                    logger.error(f"âŒ Failed to send message after {self.max_retries} attempts")
                    logger.error(f"   Final error type: {error_type}")
                    logger.error(f"   Final error: {error_msg}")
                    raise RuntimeError(f"Failed to send message to {queue_name}: {error_msg}")

                wait_time = self.retry_delay * (2 ** attempt)
                logger.debug(f"â³ Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)

    def send_message_with_delay(
        self,
        queue_name: str,
        message: BaseModel,
        delay_seconds: int
    ) -> str:
        """
        Send a message to Service Bus with scheduled delivery (exponential backoff retry).

        Uses Service Bus ScheduledEnqueueTimeUtc to delay message delivery.
        This is used for task retry with exponential backoff.

        Args:
            queue_name: Target queue name
            message: Pydantic model to send (TaskQueueMessage)
            delay_seconds: Number of seconds to delay delivery

        Returns:
            Message ID
        """
        logger.info(f"â° Scheduling message for delivery in {delay_seconds}s to queue: {queue_name}")

        # Calculate scheduled delivery time
        scheduled_time = datetime.now(timezone.utc) + timedelta(seconds=delay_seconds)
        logger.debug(f"ðŸ“… Scheduled delivery: {scheduled_time.isoformat()}")

        # Get sender
        sender = self._get_sender(queue_name)

        # Serialize message
        message_json = message.model_dump_json()

        # Create Service Bus message with scheduled time
        sb_message = ServiceBusMessage(
            body=message_json,
            content_type="application/json",
            time_to_live=timedelta(hours=24),
            scheduled_enqueue_time_utc=scheduled_time  # KEY: Delay delivery
        )

        # Add metadata properties
        if hasattr(message, 'task_id'):
            sb_message.application_properties['task_id'] = message.task_id
        if hasattr(message, 'job_id'):
            sb_message.application_properties['job_id'] = message.job_id

        # Send the message
        sender.send_messages(sb_message)

        message_id = sb_message.message_id or f"sb_{datetime.now(timezone.utc).timestamp()}"
        logger.info(f"âœ… Message scheduled for {scheduled_time.isoformat()} - ID: {message_id}")

        return message_id

    def receive_messages(
        self,
        queue_name: str,
        max_messages: int = 1,
        visibility_timeout: int = 30
    ) -> List[Dict[str, Any]]:
        """
        Receive messages from Service Bus.

        Args:
            queue_name: Queue to receive from
            max_messages: Maximum messages to receive
            visibility_timeout: Not used (Service Bus uses lock duration)

        Returns:
            List of message dictionaries
        """
        receiver = self._get_receiver(queue_name)

        try:
            with receiver:
                messages = receiver.receive_messages(
                    max_message_count=max_messages,
                    max_wait_time=5
                )

                result = []
                for msg in messages:
                    # Parse message body
                    message_data = json.loads(str(msg))

                    result.append({
                        'id': msg.message_id,
                        'content': message_data,
                        'sequence_number': msg.sequence_number,
                        'delivery_count': msg.delivery_count,
                        'enqueued_time': msg.enqueued_time_utc.isoformat() if msg.enqueued_time_utc else None,
                        'properties': dict(msg.application_properties) if msg.application_properties else {},
                        '_receiver': receiver,  # Keep for completing/abandoning
                        '_message': msg
                    })

                logger.info(f"ðŸ“¥ Received {len(result)} messages from {queue_name}")
                return result

        except Exception as e:
            logger.error(f"âŒ Failed to receive messages from {queue_name}: {e}")
            raise

    def delete_message(self, queue_name: str, message_id: str, pop_receipt: str = None) -> bool:
        """
        Complete a message (remove from queue).

        Note: pop_receipt parameter ignored (for Queue Storage compatibility).
        Message completion requires the actual message object.

        Args:
            queue_name: Queue containing the message
            message_id: Message ID (used to find message)
            pop_receipt: Ignored (Queue Storage compatibility)

        Returns:
            True if completed successfully
        """
        # Note: This is a simplified implementation
        # In production, you'd need to track message objects for completion
        logger.warning("âš ï¸ Message completion requires message object from receive_messages()")
        return True

    def peek_messages(self, queue_name: str, max_messages: int = 1) -> List[Dict[str, Any]]:
        """
        Peek at messages without removing them.

        Args:
            queue_name: Queue to peek at
            max_messages: Maximum messages to peek

        Returns:
            List of message dictionaries
        """
        receiver = self._get_receiver(queue_name)

        try:
            with receiver:
                messages = receiver.peek_messages(max_message_count=max_messages)

                result = []
                for msg in messages:
                    message_data = json.loads(str(msg))

                    result.append({
                        'id': msg.message_id,
                        'content': message_data,
                        'sequence_number': msg.sequence_number,
                        'enqueued_time': msg.enqueued_time_utc.isoformat() if msg.enqueued_time_utc else None
                    })

                logger.debug(f"ðŸ‘€ Peeked at {len(result)} messages in {queue_name}")
                return result

        except Exception as e:
            logger.error(f"âŒ Failed to peek messages in {queue_name}: {e}")
            raise

    def get_queue_length(self, queue_name: str) -> int:
        """
        Get approximate number of messages in queue.

        Note: Service Bus doesn't provide direct message count.
        This is an approximation based on peeking.

        Args:
            queue_name: Queue to check

        Returns:
            Approximate message count
        """
        # Service Bus doesn't provide direct count
        # This is a workaround by peeking
        messages = self.peek_messages(queue_name, max_messages=100)
        count = len(messages)

        if count == 100:
            logger.warning(f"Queue {queue_name} has 100+ messages (exact count unavailable)")

        return count

    def clear_queue(self, queue_name: str) -> bool:
        """
        Clear all messages from queue.

        WARNING: This receives and completes all messages. Very expensive operation.

        Args:
            queue_name: Queue to clear

        Returns:
            True if cleared successfully
        """
        logger.warning(f"ðŸ—‘ï¸ Clearing queue {queue_name} (expensive operation!)")

        receiver = self._get_receiver(queue_name)
        cleared_count = 0

        try:
            with receiver:
                while True:
                    messages = receiver.receive_messages(
                        max_message_count=100,
                        max_wait_time=1
                    )

                    if not messages:
                        break

                    for msg in messages:
                        receiver.complete_message(msg)
                        cleared_count += 1

                    logger.debug(f"Cleared {cleared_count} messages so far...")

            logger.warning(f"ðŸ—‘ï¸ Cleared {cleared_count} messages from {queue_name}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to clear queue {queue_name}: {e}")
            return False

    # ========================================================================
    # Service Bus Specific Methods (High-Volume Operations)
    # ========================================================================

    def batch_send_messages(
        self,
        queue_name: str,
        messages: List[BaseModel],
        batch_size: Optional[int] = None
    ) -> BatchResult:
        """
        Send messages in batches for high performance.

        This is the key method for high-volume scenarios like H3 hexagon processing.
        Sends messages in batches of up to 100 (Service Bus limit).

        Args:
            queue_name: Target queue/topic
            messages: List of Pydantic models to send
            batch_size: Override default batch size (max 100)

        Returns:
            BatchResult with statistics

        Example:
            # Send 10,000 task messages
            tasks = [TaskQueueMessage(...) for _ in range(10000)]
            result = repo.batch_send_messages("tasks", tasks)
            print(f"Sent {result.messages_sent} messages in {result.elapsed_ms}ms")
        """
        start_time = time.time()
        batch_size = min(batch_size or self.max_batch_size, 100)  # Service Bus limit

        logger.info(f"ðŸ“¦ Batch sending {len(messages)} messages to {queue_name}")
        logger.debug(f"Batch size: {batch_size}")

        sender = self._get_sender(queue_name)
        messages_sent = 0
        batch_count = 0
        errors = []

        try:
            # Don't use context manager with cached sender
            # Process messages in batches
            for i in range(0, len(messages), batch_size):
                batch = messages[i:i + batch_size]
                batch_count += 1

                # Convert to Service Bus messages
                sb_messages = []
                for msg in batch:
                    sb_message = ServiceBusMessage(
                        body=msg.model_dump_json(),
                        content_type="application/json"
                    )

                    # Add metadata for tracing
                    if hasattr(msg, 'task_id'):
                        sb_message.application_properties['task_id'] = msg.task_id
                    if hasattr(msg, 'job_id'):
                        sb_message.application_properties['job_id'] = msg.job_id

                    sb_messages.append(sb_message)

                # Send batch with retry
                for attempt in range(self.max_retries):
                    try:
                        sender.send_messages(sb_messages)
                        messages_sent += len(sb_messages)

                        if batch_count % 10 == 0:
                            logger.debug(f"Progress: {messages_sent}/{len(messages)} messages sent")
                        break

                    except Exception as e:
                        if attempt == self.max_retries - 1:
                            error_msg = f"Batch {batch_count} failed: {e}"
                            errors.append(error_msg)
                            logger.error(error_msg)
                        else:
                            time.sleep(self.retry_delay * (2 ** attempt))

            elapsed_ms = (time.time() - start_time) * 1000

            result = BatchResult(
                success=len(errors) == 0,
                messages_sent=messages_sent,
                batch_count=batch_count,
                elapsed_ms=elapsed_ms,
                errors=errors if errors else None
            )

            logger.info(
                f"âœ… Batch send complete: {messages_sent}/{len(messages)} messages "
                f"in {batch_count} batches, {elapsed_ms:.2f}ms"
            )

            return result

        except Exception as e:
            logger.error(f"âŒ Batch send failed: {e}")
            elapsed_ms = (time.time() - start_time) * 1000
            return BatchResult(
                success=False,
                messages_sent=messages_sent,
                batch_count=batch_count,
                elapsed_ms=elapsed_ms,
                errors=[str(e)]
            )

    async def async_batch_send_messages(
        self,
        queue_name: str,
        messages: List[BaseModel],
        batch_size: Optional[int] = None
    ) -> BatchResult:
        """
        Asynchronously send messages in batches.

        For maximum performance with very large message volumes.

        Args:
            queue_name: Target queue/topic
            messages: List of Pydantic models to send
            batch_size: Override default batch size

        Returns:
            BatchResult with statistics
        """
        if not self.async_client:
            logger.warning("Async client not available, falling back to sync")
            return self.batch_send_messages(queue_name, messages, batch_size)

        start_time = time.time()
        batch_size = min(batch_size or self.max_batch_size, 100)

        async with self.async_client:
            sender = self.async_client.get_queue_sender(queue_name)
            async with sender:
                messages_sent = 0
                batch_count = 0
                errors = []

                # Create batches
                for i in range(0, len(messages), batch_size):
                    batch = messages[i:i + batch_size]
                    batch_count += 1

                    # Convert to Service Bus messages
                    sb_messages = []
                    for msg in batch:
                        sb_message = ServiceBusMessage(
                            body=msg.model_dump_json(),
                            content_type="application/json"
                        )
                        sb_messages.append(sb_message)

                    # Send batch
                    try:
                        await sender.send_messages(sb_messages)
                        messages_sent += len(sb_messages)
                    except Exception as e:
                        errors.append(f"Batch {batch_count}: {e}")

        elapsed_ms = (time.time() - start_time) * 1000

        return BatchResult(
            success=len(errors) == 0,
            messages_sent=messages_sent,
            batch_count=batch_count,
            elapsed_ms=elapsed_ms,
            errors=errors if errors else None
        )

    def complete_message(self, receiver: ServiceBusReceiver, message: Any) -> bool:
        """
        Complete a message (mark as processed).

        Args:
            receiver: The receiver that received the message
            message: The message object from receive_messages

        Returns:
            True if completed successfully
        """
        try:
            receiver.complete_message(message)
            logger.debug(f"âœ… Message completed: {message.message_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to complete message: {e}")
            return False

    def abandon_message(self, receiver: ServiceBusReceiver, message: Any) -> bool:
        """
        Abandon a message (return to queue for retry).

        Args:
            receiver: The receiver that received the message
            message: The message object

        Returns:
            True if abandoned successfully
        """
        try:
            receiver.abandon_message(message)
            logger.debug(f"ðŸ”„ Message abandoned: {message.message_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to abandon message: {e}")
            return False

    def __del__(self):
        """Clean up resources on deletion."""
        if hasattr(self, '_initialized'):
            # Close all senders
            for sender in self._senders.values():
                try:
                    sender.close()
                except:
                    pass

            # Close all receivers
            for receiver in self._receivers.values():
                try:
                    receiver.close()
                except:
                    pass

            # Close clients
            if hasattr(self, 'client'):
                try:
                    self.client.close()
                except:
                    pass

            if hasattr(self, 'async_client') and self.async_client:
                # Note: Async cleanup requires event loop
                pass


# Factory function for dependency injection
def get_service_bus_repository() -> ServiceBusRepository:
    """
    Get ServiceBusRepository singleton instance.

    Returns:
        ServiceBusRepository singleton instance
    """
    return ServiceBusRepository.instance()