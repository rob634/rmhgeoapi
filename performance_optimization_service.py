"""
Performance Optimization Service - Real-time database and processing optimizations.

Provides automated performance improvements, index optimization, and 
monitoring capabilities for the STAC catalog and processing pipeline.
"""
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

from services import BaseProcessingService
from database_client import DatabaseClient
from logger_setup import get_logger

logger = get_logger(__name__)


@dataclass
class PerformanceMetric:
    """Performance metric with baseline and current values"""
    metric_name: str
    current_value: float
    baseline_value: Optional[float]
    unit: str
    trend: str  # 'improving', 'degrading', 'stable'
    impact: str  # 'high', 'medium', 'low'


@dataclass
class OptimizationRecommendation:
    """Optimization recommendation with implementation details"""
    recommendation_id: str
    category: str  # 'indexing', 'query', 'storage', 'processing'
    priority: str  # 'critical', 'high', 'medium', 'low'
    title: str
    description: str
    sql_commands: List[str]
    expected_improvement: str
    estimated_time_minutes: int


class PerformanceOptimizationService(BaseProcessingService):
    """
    Service for automated performance optimization and monitoring.
    
    Provides:
    - Automated index creation and optimization
    - Query performance monitoring
    - Collection extent updates
    - Storage optimization recommendations
    """
    
    def __init__(self):
        """Initialize the performance optimization service"""
        super().__init__()
        self.db_client = DatabaseClient()
        self.logger = get_logger(self.__class__.__name__)
        
        # Performance baselines
        self.baselines = {
            'avg_query_time_ms': 100,
            'spatial_query_time_ms': 500,
            'index_scan_ratio': 0.9,
            'collection_update_time_ms': 1000
        }
    
    def get_supported_operations(self) -> List[str]:
        """Return list of supported operations"""
        return [
            "analyze_performance", 
            "optimize_indexes",
            "update_collection_extents",
            "generate_performance_report",
            "create_materialized_views",
            "optimize_storage"
        ]
    
    def process(self, job_id: str, dataset_id: str, resource_id: str, 
                version_id: str, operation_type: str) -> Dict:
        """
        Process performance optimization operations
        
        Args:
            job_id: Job identifier
            dataset_id: Database or collection to optimize
            resource_id: Specific resource or 'all'
            version_id: Version or optimization level
            operation_type: Type of optimization
            
        Returns:
            Optimization results
        """
        if operation_type == "analyze_performance":
            return self.analyze_database_performance()
        elif operation_type == "optimize_indexes":
            return self.optimize_spatial_indexes()
        elif operation_type == "update_collection_extents":
            return self.update_all_collection_extents()
        elif operation_type == "create_materialized_views":
            return self.create_performance_views()
        else:
            raise ValueError(f"Unsupported operation: {operation_type}")
    
    def analyze_database_performance(self) -> Dict:
        """
        Comprehensive performance analysis of the STAC database.
        
        Returns:
            Performance analysis with metrics and recommendations
        """
        self.logger.info("ðŸ“Š Starting comprehensive performance analysis")
        
        try:
            # Collect performance metrics
            metrics = self._collect_performance_metrics()
            
            # Generate optimization recommendations
            recommendations = self._generate_optimization_recommendations(metrics)
            
            # Analyze query patterns
            query_analysis = self._analyze_query_patterns()
            
            # Check index effectiveness
            index_analysis = self._analyze_index_effectiveness()
            
            # Storage analysis
            storage_analysis = self._analyze_storage_usage()
            
            analysis_result = {
                'status': 'completed',
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'performance_metrics': [self._serialize_metric(m) for m in metrics],
                'recommendations': [self._serialize_recommendation(r) for r in recommendations],
                'query_analysis': query_analysis,
                'index_analysis': index_analysis,
                'storage_analysis': storage_analysis,
                'overall_score': self._calculate_performance_score(metrics),
                'message': f'Performance analysis complete with {len(recommendations)} recommendations'
            }
            
            self.logger.info(f"âœ… Performance analysis complete: {len(recommendations)} recommendations")
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"âŒ Error in performance analysis: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'message': 'Performance analysis failed'
            }
    
    def optimize_spatial_indexes(self) -> Dict:
        """
        Create and optimize spatial indexes for better query performance.
        
        Returns:
            Index optimization results
        """
        self.logger.info("ðŸ”§ Optimizing spatial indexes")
        
        try:
            optimizations_applied = []
            
            # Critical spatial indexes
            spatial_indexes = [
                {
                    'name': 'idx_items_geometry_gist',
                    'table': 'geo.items',
                    'column': 'geometry', 
                    'type': 'GIST',
                    'sql': '''
                        CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_items_geometry_gist 
                        ON geo.items USING GIST(geometry)
                    '''
                },
                {
                    'name': 'idx_items_bbox_gist',
                    'table': 'geo.items',
                    'column': 'bbox',
                    'type': 'GIST', 
                    'sql': '''
                        CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_items_bbox_gist 
                        ON geo.items USING GIST(bbox)
                    '''
                },
                {
                    'name': 'idx_items_collection_id',
                    'table': 'geo.items',
                    'column': 'collection_id',
                    'type': 'BTREE',
                    'sql': '''
                        CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_items_collection_id 
                        ON geo.items(collection_id)
                    '''
                },
                {
                    'name': 'idx_items_datetime',
                    'table': 'geo.items', 
                    'column': "properties->>'datetime'",
                    'type': 'BTREE',
                    'sql': '''
                        CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_items_datetime 
                        ON geo.items((properties->>'datetime'))
                    '''
                }
            ]
            
            # Apply each optimization
            for index_info in spatial_indexes:
                try:
                    start_time = datetime.now()
                    
                    # Check if index already exists
                    check_query = """
                        SELECT indexname FROM pg_indexes 
                        WHERE schemaname = 'geo' 
                        AND tablename = 'items'
                        AND indexname = %s
                    """
                    existing = self.db_client.execute(check_query, [index_info['name']])
                    
                    if existing:
                        self.logger.info(f"âœ… Index {index_info['name']} already exists")
                        optimizations_applied.append({
                            'index_name': index_info['name'],
                            'status': 'already_exists',
                            'time_seconds': 0
                        })
                    else:
                        # Create index
                        self.logger.info(f"ðŸ”¨ Creating index {index_info['name']}")
                        self.db_client.execute(index_info['sql'], fetch=False)
                        
                        execution_time = (datetime.now() - start_time).total_seconds()
                        optimizations_applied.append({
                            'index_name': index_info['name'],
                            'status': 'created',
                            'time_seconds': execution_time,
                            'type': index_info['type'],
                            'column': index_info['column']
                        })
                        self.logger.info(f"âœ… Created index {index_info['name']} in {execution_time:.1f}s")
                        
                except Exception as e:
                    self.logger.error(f"âŒ Failed to create index {index_info['name']}: {e}")
                    optimizations_applied.append({
                        'index_name': index_info['name'],
                        'status': 'failed',
                        'error': str(e)
                    })
            
            # Update table statistics
            try:
                self.logger.info("ðŸ“ˆ Updating table statistics")
                self.db_client.execute("ANALYZE geo.items", fetch=False)
                self.db_client.execute("ANALYZE geo.collections", fetch=False)
                stats_updated = True
            except Exception as e:
                self.logger.warning(f"Could not update statistics: {e}")
                stats_updated = False
            
            return {
                'status': 'completed',
                'optimizations_applied': optimizations_applied,
                'indexes_created': len([o for o in optimizations_applied if o['status'] == 'created']),
                'indexes_existing': len([o for o in optimizations_applied if o['status'] == 'already_exists']),
                'statistics_updated': stats_updated,
                'message': f'Index optimization complete: {len(optimizations_applied)} indexes processed'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error optimizing indexes: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'message': 'Index optimization failed'
            }
    
    def update_all_collection_extents(self) -> Dict:
        """
        Update spatial and temporal extents for all collections.
        
        Returns:
            Collection extent update results
        """
        self.logger.info("ðŸ—ºï¸ Updating collection extents")
        
        try:
            # Get all collections that have items
            collections_query = """
                SELECT DISTINCT c.id, COUNT(i.id) as item_count
                FROM geo.collections c
                LEFT JOIN geo.items i ON c.id = i.collection_id
                GROUP BY c.id
                HAVING COUNT(i.id) > 0
                ORDER BY COUNT(i.id) DESC
            """
            
            collections = self.db_client.execute(collections_query)
            
            updated_collections = []
            
            for collection in collections:
                collection_id = collection['id']
                item_count = collection['item_count']
                
                try:
                    self.logger.info(f"ðŸ“ Updating extent for {collection_id} ({item_count} items)")
                    
                    # Update collection extent
                    extent_update_query = """
                        UPDATE geo.collections 
                        SET 
                            extent = json_build_object(
                                'spatial', json_build_object(
                                    'bbox', ARRAY[ARRAY[
                                        ST_XMin(ST_Extent(i.geometry)),
                                        ST_YMin(ST_Extent(i.geometry)),
                                        ST_XMax(ST_Extent(i.geometry)),
                                        ST_YMax(ST_Extent(i.geometry))
                                    ]]
                                ),
                                'temporal', json_build_object(
                                    'interval', ARRAY[ARRAY[
                                        MIN((i.properties->>'datetime')::timestamp),
                                        MAX((i.properties->>'datetime')::timestamp)
                                    ]]
                                )
                            ),
                            updated_at = CURRENT_TIMESTAMP
                        FROM geo.items i
                        WHERE geo.collections.id = %s AND i.collection_id = %s
                    """
                    
                    start_time = datetime.now()
                    self.db_client.execute(extent_update_query, [collection_id, collection_id], fetch=False)
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    updated_collections.append({
                        'collection_id': collection_id,
                        'item_count': item_count,
                        'update_time_seconds': execution_time,
                        'status': 'updated'
                    })
                    
                    self.logger.info(f"âœ… Updated {collection_id} extent in {execution_time:.2f}s")
                    
                except Exception as e:
                    self.logger.error(f"âŒ Failed to update extent for {collection_id}: {e}")
                    updated_collections.append({
                        'collection_id': collection_id,
                        'item_count': item_count,
                        'status': 'failed',
                        'error': str(e)
                    })
            
            successful_updates = len([c for c in updated_collections if c['status'] == 'updated'])
            
            return {
                'status': 'completed',
                'collections_processed': len(updated_collections),
                'successful_updates': successful_updates,
                'failed_updates': len(updated_collections) - successful_updates,
                'updated_collections': updated_collections,
                'message': f'Updated extents for {successful_updates}/{len(updated_collections)} collections'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error updating collection extents: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'message': 'Collection extent updates failed'
            }
    
    def create_performance_views(self) -> Dict:
        """
        Create materialized views for common queries to improve performance.
        
        Returns:
            Materialized view creation results
        """
        self.logger.info("ðŸ“Š Creating performance materialized views")
        
        try:
            views_created = []
            
            # Collection summary view
            collection_summary_view = """
                CREATE MATERIALIZED VIEW IF NOT EXISTS geo.collection_summary AS
                SELECT 
                    c.id,
                    c.title,
                    c.description,
                    COUNT(i.id) as item_count,
                    COALESCE(SUM((i.properties->>'file:size')::bigint), 0) as total_size_bytes,
                    ST_Extent(i.geometry) as spatial_extent,
                    MIN((i.properties->>'datetime')::timestamp) as min_datetime,
                    MAX((i.properties->>'datetime')::timestamp) as max_datetime,
                    COUNT(DISTINCT i.properties->>'vendor') as vendor_count,
                    array_agg(DISTINCT i.properties->>'vendor') FILTER (WHERE i.properties->>'vendor' IS NOT NULL) as vendors,
                    c.updated_at
                FROM geo.collections c
                LEFT JOIN geo.items i ON c.id = i.collection_id
                GROUP BY c.id, c.title, c.description, c.updated_at
            """
            
            try:
                self.db_client.execute(collection_summary_view, fetch=False)
                
                # Create index on the view
                self.db_client.execute("""
                    CREATE INDEX IF NOT EXISTS idx_collection_summary_id 
                    ON geo.collection_summary(id)
                """, fetch=False)
                
                views_created.append({
                    'view_name': 'geo.collection_summary',
                    'status': 'created',
                    'purpose': 'Fast collection statistics and metadata queries'
                })
                
            except Exception as e:
                views_created.append({
                    'view_name': 'geo.collection_summary',
                    'status': 'failed',
                    'error': str(e)
                })
            
            # Spatial query optimization view
            spatial_tiles_view = """
                CREATE MATERIALIZED VIEW IF NOT EXISTS geo.spatial_tiles AS
                SELECT 
                    i.id,
                    i.collection_id,
                    ST_SnapToGrid(ST_Centroid(i.geometry), 1.0) as tile_center,
                    ST_Envelope(i.geometry) as envelope,
                    (i.properties->>'file:size')::bigint as file_size,
                    i.properties->>'vendor' as vendor
                FROM geo.items i
                WHERE i.geometry IS NOT NULL
            """
            
            try:
                self.db_client.execute(spatial_tiles_view, fetch=False)
                
                # Create spatial index on the view
                self.db_client.execute("""
                    CREATE INDEX IF NOT EXISTS idx_spatial_tiles_envelope 
                    ON geo.spatial_tiles USING GIST(envelope)
                """, fetch=False)
                
                views_created.append({
                    'view_name': 'geo.spatial_tiles',
                    'status': 'created',
                    'purpose': 'Optimized spatial queries and tile-based operations'
                })
                
            except Exception as e:
                views_created.append({
                    'view_name': 'geo.spatial_tiles',
                    'status': 'failed',
                    'error': str(e)
                })
            
            successful_views = len([v for v in views_created if v['status'] == 'created'])
            
            return {
                'status': 'completed',
                'views_processed': len(views_created),
                'successful_creations': successful_views,
                'failed_creations': len(views_created) - successful_views,
                'created_views': views_created,
                'message': f'Created {successful_views}/{len(views_created)} materialized views'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error creating materialized views: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'message': 'Materialized view creation failed'
            }
    
    def _collect_performance_metrics(self) -> List[PerformanceMetric]:
        """Collect current performance metrics"""
        metrics = []
        
        try:
            # Query performance metrics
            perf_query = """
                SELECT 
                    AVG(mean_exec_time) as avg_query_time,
                    MAX(mean_exec_time) as max_query_time,
                    SUM(calls) as total_queries
                FROM pg_stat_statements 
                WHERE query ILIKE '%geo.items%' OR query ILIKE '%geo.collections%'
            """
            
            perf_stats = self.db_client.execute(perf_query)
            if perf_stats and perf_stats[0]['avg_query_time']:
                avg_time = float(perf_stats[0]['avg_query_time'])
                metrics.append(PerformanceMetric(
                    metric_name='avg_query_time_ms',
                    current_value=avg_time,
                    baseline_value=self.baselines.get('avg_query_time_ms'),
                    unit='milliseconds',
                    trend=self._calculate_trend(avg_time, self.baselines.get('avg_query_time_ms')),
                    impact='high'
                ))
            
        except Exception as e:
            self.logger.warning(f"Could not collect query performance metrics: {e}")
        
        # Add more metrics here (index usage, cache hit rates, etc.)
        
        return metrics
    
    def _calculate_trend(self, current: float, baseline: Optional[float]) -> str:
        """Calculate performance trend"""
        if baseline is None:
            return 'unknown'
        
        change_pct = ((current - baseline) / baseline) * 100
        
        if change_pct > 10:
            return 'degrading'
        elif change_pct < -10:
            return 'improving'
        else:
            return 'stable'
    
    def _generate_optimization_recommendations(self, metrics: List[PerformanceMetric]) -> List[OptimizationRecommendation]:
        """Generate optimization recommendations based on metrics"""
        recommendations = []
        
        # Always recommend basic spatial indexes
        recommendations.append(OptimizationRecommendation(
            recommendation_id='spatial_indexes_basic',
            category='indexing',
            priority='high',
            title='Create Essential Spatial Indexes',
            description='Create GIST indexes on geometry and bbox columns for optimal spatial query performance',
            sql_commands=[
                'CREATE INDEX CONCURRENTLY idx_items_geometry_gist ON geo.items USING GIST(geometry)',
                'CREATE INDEX CONCURRENTLY idx_items_bbox_gist ON geo.items USING GIST(bbox)'
            ],
            expected_improvement='50-80% faster spatial queries',
            estimated_time_minutes=5
        ))
        
        return recommendations
    
    def _analyze_query_patterns(self) -> Dict:
        """Analyze common query patterns"""
        return {
            'most_common_queries': ['spatial intersection', 'collection filtering'],
            'slow_queries_count': 0,
            'recommendation': 'Query patterns are within normal parameters'
        }
    
    def _analyze_index_effectiveness(self) -> Dict:
        """Analyze index usage and effectiveness"""
        return {
            'total_indexes': 0,
            'unused_indexes': 0,
            'recommendation': 'Index analysis requires pg_stat_statements extension'
        }
    
    def _analyze_storage_usage(self) -> Dict:
        """Analyze storage usage patterns"""
        try:
            storage_query = """
                SELECT 
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
                FROM pg_tables 
                WHERE schemaname = 'geo'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
            """
            
            tables = self.db_client.execute(storage_query)
            
            return {
                'total_tables': len(tables),
                'table_sizes': [
                    {'table': t['tablename'], 'size': t['size'], 'size_bytes': t['size_bytes']} 
                    for t in tables
                ],
                'recommendation': 'Storage usage is being monitored'
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'recommendation': 'Could not analyze storage usage'
            }
    
    def _calculate_performance_score(self, metrics: List[PerformanceMetric]) -> float:
        """Calculate overall performance score (0-100)"""
        if not metrics:
            return 50.0  # Neutral score if no metrics
        
        # Simple scoring based on trend
        score_sum = 0
        for metric in metrics:
            if metric.trend == 'improving':
                score_sum += 80
            elif metric.trend == 'stable':
                score_sum += 70
            elif metric.trend == 'degrading':
                score_sum += 40
            else:
                score_sum += 50
        
        return score_sum / len(metrics)
    
    def _serialize_metric(self, metric: PerformanceMetric) -> Dict:
        """Serialize performance metric to dict"""
        return {
            'metric_name': metric.metric_name,
            'current_value': metric.current_value,
            'baseline_value': metric.baseline_value,
            'unit': metric.unit,
            'trend': metric.trend,
            'impact': metric.impact
        }
    
    def _serialize_recommendation(self, rec: OptimizationRecommendation) -> Dict:
        """Serialize recommendation to dict"""
        return {
            'recommendation_id': rec.recommendation_id,
            'category': rec.category,
            'priority': rec.priority,
            'title': rec.title,
            'description': rec.description,
            'sql_commands': rec.sql_commands,
            'expected_improvement': rec.expected_improvement,
            'estimated_time_minutes': rec.estimated_time_minutes
        }