# ============================================================================
# CLAUDE CONTEXT - ENHANCED VECTOR ETL TASK HANDLERS
# ============================================================================
# PURPOSE: Task handlers for vector ETL workflow with comprehensive error handling
# EXPORTS: load_vector_file, validate_vector, upload_vector_chunk
# INTERFACES: TaskRegistry (decorator registration pattern)
# PYDANTIC_MODELS: Uses Dict[str, Any] for parameters and returns
# DEPENDENCIES: geopandas, infrastructure.blob.BlobRepository, services.vector.converters
# SOURCE: Called by CoreMachine during task execution
# SCOPE: Service layer - task execution logic with detailed error tracking
# VALIDATION: Format validation, GeoDataFrame validation, parameter validation
# PATTERNS: TaskRegistry decorator pattern with granular error handling
# ENTRY_POINTS: Registered with @TaskRegistry.register() decorator
# INDEX:
#   - load_vector_file: Load file with format-specific error handling
#   - validate_vector: Validate with geometry-specific error tracking
#   - upload_vector_chunk: Upload with connection and data error handling
# ============================================================================

"""
Enhanced Vector ETL Task Handlers with Comprehensive Error Handling

These task handlers provide robust error handling for the vector ETL pipeline.
Each operation has granular try-except blocks with detailed logging and context.

Key improvements:
- Format-specific error handling for each file type
- Detailed validation with specific error messages
- Blob storage error handling with retry logic
- Geometry validation with fallback strategies
- Transaction management for PostGIS operations

Author: Robert and Geospatial Claude Legion
Date: 26 OCT 2025
"""

from typing import Dict, Any, Optional, List
import logging
import traceback
import json
import io
import tempfile
import os

import geopandas as gpd
from shapely.geometry import shape, mapping
from shapely.errors import ShapelyError

from infrastructure.blob import BlobRepository
from .converters import (
    _convert_csv, _convert_geojson, _convert_geopackage,
    _convert_kml, _convert_kmz, _convert_shapefile
)

# Configure detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
)
logger = logging.getLogger(__name__)


class VectorTaskError(Exception):
    """Base exception for vector task errors."""
    pass


class BlobAccessError(VectorTaskError):
    """Blob storage access errors."""
    pass


class FormatConversionError(VectorTaskError):
    """File format conversion errors."""
    pass


class ValidationError(VectorTaskError):
    """Data validation errors."""
    pass


def load_vector_file(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Load vector file from blob storage with comprehensive error handling.

    Enhanced features:
    - Blob access retry logic
    - Format-specific error messages
    - Partial data recovery
    - Detailed error context
    - Memory-efficient streaming

    Parameters:
        blob_name: str - Blob path in storage
        container_name: str - Container name (default: 'bronze')
        file_extension: str - File extension (determines converter)
        **converter_params: Format-specific parameters

    Returns:
        Dictionary with loaded GeoDataFrame and metadata

    Raises:
        BlobAccessError: If blob cannot be accessed
        FormatConversionError: If file cannot be converted
        ValidationError: If data validation fails
    """
    logger.info(f"üì• Starting vector file load task")

    # Extract parameters with validation
    try:
        blob_name = parameters.get("blob_name")
        if not blob_name:
            raise ValueError("blob_name is required")

        container_name = parameters.get("container_name", "bronze")
        file_extension = parameters.get("file_extension", "").lower().lstrip('.')

        if not file_extension:
            raise ValueError("file_extension is required")

        logger.info(f"Loading {file_extension} file: {container_name}/{blob_name}")

    except Exception as e:
        logger.error(f"‚ùå Parameter validation failed: {e}")
        raise ValidationError(f"Invalid parameters: {e}")

    # Step 1: Get file from blob storage with retry logic
    file_data = None
    max_retries = 3

    for attempt in range(max_retries):
        try:
            logger.debug(f"Blob access attempt {attempt + 1}/{max_retries}")

            blob_repo = BlobRepository.instance()

            # Check if blob exists
            if not blob_repo.blob_exists(container_name, blob_name):
                raise BlobAccessError(f"Blob not found: {container_name}/{blob_name}")

            # Get blob size for logging
            blob_size = blob_repo.get_blob_size(container_name, blob_name)
            logger.info(f"üìä Blob size: {blob_size / 1024 / 1024:.2f} MB")

            # Read blob with streaming
            file_data = blob_repo.read_blob_to_stream(container_name, blob_name)
            logger.info(f"‚úÖ Successfully read blob from storage")
            break

        except BlobAccessError:
            raise  # Don't retry if blob doesn't exist

        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"‚ö†Ô∏è Blob access attempt {attempt + 1} failed: {e}, retrying...")
                continue
            else:
                logger.error(f"‚ùå All blob access attempts failed: {e}")
                raise BlobAccessError(f"Cannot access blob after {max_retries} attempts: {e}")

    # Step 2: Extract converter-specific parameters
    converter_params = {}
    try:
        # Filter out standard parameters
        standard_params = ['blob_name', 'container_name', 'file_extension', 'task_id', 'job_id']
        converter_params = {
            k: v for k, v in parameters.items()
            if k not in standard_params
        }
        logger.debug(f"Converter parameters: {converter_params}")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error extracting converter params: {e}, using defaults")
        converter_params = {}

    # Step 3: Dispatch to appropriate converter with format-specific error handling
    converters = {
        'csv': (_convert_csv, "CSV with coordinates or WKT"),
        'geojson': (_convert_geojson, "GeoJSON"),
        'json': (_convert_geojson, "JSON (assuming GeoJSON)"),
        'gpkg': (_convert_geopackage, "GeoPackage"),
        'kml': (_convert_kml, "KML"),
        'kmz': (_convert_kmz, "KMZ (compressed KML)"),
        'shp': (_convert_shapefile, "Shapefile (zipped)"),
        'zip': (_convert_shapefile, "ZIP (assuming Shapefile)")
    }

    if file_extension not in converters:
        supported = ", ".join(converters.keys())
        logger.error(f"‚ùå Unsupported format: {file_extension}")
        raise FormatConversionError(
            f"Unsupported file extension: '{file_extension}'. Supported: {supported}"
        )

    converter_func, format_name = converters[file_extension]
    logger.info(f"üîÑ Converting {format_name} to GeoDataFrame")

    # Step 4: Convert to GeoDataFrame with format-specific error handling
    gdf = None
    try:
        gdf = converter_func(file_data, **converter_params)
        logger.info(f"‚úÖ Successfully converted to GeoDataFrame with {len(gdf)} features")

    except FileNotFoundError as e:
        # Specific error for missing files in archives
        logger.error(f"‚ùå File not found in archive: {e}")
        raise FormatConversionError(f"Required file missing in {format_name}: {e}")

    except ValueError as e:
        # Specific error for data format issues
        logger.error(f"‚ùå Data format error: {e}")
        raise FormatConversionError(f"Invalid {format_name} format: {e}")

    except MemoryError as e:
        # File too large
        logger.error(f"‚ùå File too large for memory: {e}")
        raise FormatConversionError(f"File too large to process in memory: {e}")

    except Exception as e:
        # Generic conversion error
        logger.error(f"‚ùå Conversion failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise FormatConversionError(f"Failed to convert {format_name}: {e}")

    # Step 5: Validate GeoDataFrame
    try:
        if gdf is None or len(gdf) == 0:
            raise ValidationError("No data found in file")

        # Check for geometry column
        if 'geometry' not in gdf.columns:
            raise ValidationError("No geometry column found")

        # Count valid geometries
        valid_geoms = gdf.geometry.notna().sum()
        if valid_geoms == 0:
            raise ValidationError("No valid geometries found")

        logger.info(f"‚úÖ Validation passed: {valid_geoms}/{len(gdf)} valid geometries")

    except ValidationError:
        raise
    except Exception as e:
        logger.error(f"‚ùå Validation error: {e}")
        raise ValidationError(f"GeoDataFrame validation failed: {e}")

    # Step 6: Extract metadata with error handling
    metadata = {}
    try:
        # Get geometry types
        try:
            geometry_types = gdf.geometry.type.unique().tolist()
        except:
            geometry_types = ["Unknown"]
            logger.warning("‚ö†Ô∏è Could not determine geometry types")

        # Get bounds
        try:
            bounds = gdf.total_bounds.tolist()
        except:
            bounds = None
            logger.warning("‚ö†Ô∏è Could not calculate bounds")

        # Get CRS
        try:
            crs = str(gdf.crs) if gdf.crs else None
        except:
            crs = None
            logger.warning("‚ö†Ô∏è Could not determine CRS")

        metadata = {
            "row_count": len(gdf),
            "geometry_types": geometry_types,
            "bounds": bounds,
            "crs": crs,
            "blob_name": blob_name,
            "container_name": container_name,
            "file_extension": file_extension
        }

        logger.info(f"üìä Metadata: {metadata['row_count']} features, CRS: {metadata['crs']}")

    except Exception as e:
        logger.error(f"‚ùå Metadata extraction error: {e}")
        # Continue with partial metadata
        metadata = {
            "row_count": len(gdf) if gdf is not None else 0,
            "error": f"Partial metadata due to: {e}"
        }

    # Step 7: Serialize GeoDataFrame with error handling
    try:
        gdf_json = gdf.to_json()
        logger.info(f"‚úÖ Successfully serialized GeoDataFrame")
    except Exception as e:
        logger.error(f"‚ùå Serialization error: {e}")
        # Try alternative serialization
        try:
            # Save to temporary file
            with tempfile.NamedTemporaryFile(suffix='.geojson', delete=False) as tmp:
                gdf.to_file(tmp.name, driver='GeoJSON')
                tmp_path = tmp.name

            with open(tmp_path, 'r') as f:
                gdf_json = f.read()

            os.unlink(tmp_path)
            logger.info("‚úÖ Used alternative serialization via temp file")

        except Exception as alt_error:
            logger.error(f"‚ùå Alternative serialization also failed: {alt_error}")
            raise ValidationError(f"Cannot serialize GeoDataFrame: {e}")

    # Return result
    result = {
        "gdf": gdf_json,
        **metadata
    }

    logger.info(f"‚úÖ Load vector file task completed successfully")
    return result


def validate_vector(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Validate and prepare GeoDataFrame with comprehensive error handling.

    Enhanced features:
    - Geometry repair strategies
    - CRS validation and transformation
    - Column cleaning with conflict resolution
    - Detailed validation report

    Parameters:
        gdf_serialized: str - JSON serialized GeoDataFrame

    Returns:
        Dictionary with validated GeoDataFrame and validation report

    Raises:
        ValidationError: If critical validation issues found
    """
    logger.info("üîç Starting vector validation task")

    # Step 1: Deserialize GeoDataFrame with error handling
    try:
        gdf_serialized = parameters.get("gdf_serialized")
        if not gdf_serialized:
            raise ValueError("gdf_serialized parameter is required")

        # Try direct deserialization
        try:
            gdf = gpd.read_file(io.StringIO(gdf_serialized))
            logger.info(f"‚úÖ Deserialized GeoDataFrame with {len(gdf)} features")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Direct deserialization failed: {e}, trying JSON parse")
            # Try parsing as JSON first
            data = json.loads(gdf_serialized)
            gdf = gpd.GeoDataFrame.from_features(data)
            logger.info(f"‚úÖ Deserialized via JSON with {len(gdf)} features")

    except Exception as e:
        logger.error(f"‚ùå Deserialization failed: {e}")
        raise ValidationError(f"Cannot deserialize GeoDataFrame: {e}")

    # Step 2: Import handler and validate
    validation_report = {
        "input_features": len(gdf),
        "issues_fixed": [],
        "warnings": [],
        "errors": []
    }

    try:
        # Import here to avoid circular dependency
        from .postgis_handler_enhanced import VectorToPostGISHandler

        handler = VectorToPostGISHandler()

        # Run validation with enhanced handler
        validated_gdf = handler.prepare_gdf(gdf)

        # Get error summary from handler
        error_summary = handler.get_error_summary()
        validation_report["warnings"].extend(error_summary["warnings"])
        validation_report["errors"].extend(error_summary["errors"])

        logger.info(f"‚úÖ Validation complete: {len(validated_gdf)} features validated")

    except ImportError:
        # Fallback to basic handler if enhanced not available
        logger.warning("‚ö†Ô∏è Enhanced handler not available, using basic validation")
        from .postgis_handler import VectorToPostGISHandler
        handler = VectorToPostGISHandler()
        validated_gdf = handler.prepare_gdf(gdf)

    except Exception as e:
        logger.error(f"‚ùå Validation failed: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise ValidationError(f"GeoDataFrame validation failed: {e}")

    # Step 3: Post-validation checks
    try:
        validation_report["output_features"] = len(validated_gdf)
        validation_report["features_removed"] = len(gdf) - len(validated_gdf)

        # Check geometry types
        try:
            geometry_types = validated_gdf.geometry.type.unique().tolist()
            validation_report["geometry_types"] = geometry_types
        except:
            validation_report["warnings"].append("Could not determine geometry types")

        # Check for remaining issues
        null_geoms = validated_gdf.geometry.isna().sum()
        if null_geoms > 0:
            validation_report["warnings"].append(f"Still has {null_geoms} null geometries")

        invalid_geoms = sum(1 for g in validated_gdf.geometry if g and not g.is_valid)
        if invalid_geoms > 0:
            validation_report["warnings"].append(f"Still has {invalid_geoms} invalid geometries")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Post-validation checks failed: {e}")
        validation_report["warnings"].append(f"Incomplete validation: {e}")

    # Step 4: Serialize validated GeoDataFrame
    try:
        validated_json = validated_gdf.to_json()
        logger.info("‚úÖ Serialized validated GeoDataFrame")
    except Exception as e:
        logger.error(f"‚ùå Serialization error: {e}")
        raise ValidationError(f"Cannot serialize validated GeoDataFrame: {e}")

    # Return result
    result = {
        "validated_gdf": validated_json,
        "geometry_types": validation_report.get("geometry_types", ["Unknown"]),
        "row_count": len(validated_gdf),
        "validation_report": validation_report
    }

    logger.info(f"‚úÖ Validation task completed successfully")
    return result


def upload_vector_chunk(parameters: Dict[str, Any]) -> Dict[str, Any]:
    """
    Upload GeoDataFrame chunk to PostGIS with comprehensive error handling.

    Enhanced features:
    - Connection pooling and retry logic
    - Transaction management
    - Progress tracking
    - Detailed error reporting
    - Automatic rollback on failure

    Parameters:
        chunk_data: str - JSON serialized GeoDataFrame chunk
        table_name: str - Target table name
        schema: str - Target schema (default: 'geo')
        chunk_index: int - Chunk number for tracking

    Returns:
        Dictionary with upload status and statistics

    Raises:
        VectorTaskError: If upload fails
    """
    logger.info("üì§ Starting vector chunk upload task")

    # Extract parameters
    try:
        chunk_data = parameters.get("chunk_data")
        if not chunk_data:
            raise ValueError("chunk_data is required")

        table_name = parameters.get("table_name")
        if not table_name:
            raise ValueError("table_name is required")

        schema = parameters.get("schema", "geo")
        chunk_index = parameters.get("chunk_index", 0)

        logger.info(f"Uploading chunk {chunk_index} to {schema}.{table_name}")

    except Exception as e:
        logger.error(f"‚ùå Parameter validation failed: {e}")
        raise ValidationError(f"Invalid upload parameters: {e}")

    # Step 1: Deserialize chunk
    try:
        chunk = gpd.read_file(io.StringIO(chunk_data))
        logger.info(f"‚úÖ Deserialized chunk with {len(chunk)} features")
    except Exception as e:
        logger.error(f"‚ùå Chunk deserialization failed: {e}")
        raise ValidationError(f"Cannot deserialize chunk data: {e}")

    # Step 2: Upload to PostGIS with enhanced handler
    upload_stats = {
        "chunk_index": chunk_index,
        "features_attempted": len(chunk),
        "features_uploaded": 0,
        "errors": [],
        "warnings": []
    }

    try:
        # Use enhanced handler with error tracking
        from .postgis_handler_enhanced import VectorToPostGISHandler

        handler = VectorToPostGISHandler()

        # Clear previous errors
        handler.clear_error_tracking()

        # Upload chunk
        handler.upload_chunk(chunk, table_name, schema)

        # Get error summary
        error_summary = handler.get_error_summary()
        upload_stats["errors"] = error_summary["errors"]
        upload_stats["warnings"] = error_summary["warnings"]
        upload_stats["features_uploaded"] = len(chunk) - error_summary["error_count"]

        logger.info(f"‚úÖ Upload completed for chunk {chunk_index}")

    except ImportError:
        # Fallback to basic handler
        logger.warning("‚ö†Ô∏è Enhanced handler not available, using basic upload")
        from .postgis_handler import VectorToPostGISHandler
        handler = VectorToPostGISHandler()
        handler.upload_chunk(chunk, table_name, schema)
        upload_stats["features_uploaded"] = len(chunk)

    except Exception as e:
        logger.error(f"‚ùå Upload failed for chunk {chunk_index}: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        upload_stats["errors"].append(str(e))
        raise VectorTaskError(f"Failed to upload chunk {chunk_index}: {e}")

    # Step 3: Verify upload (optional)
    try:
        # Could add verification query here
        logger.debug(f"Upload statistics: {upload_stats}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not verify upload: {e}")
        upload_stats["warnings"].append(f"Upload verification skipped: {e}")

    # Return result
    result = {
        "success": upload_stats["features_uploaded"] > 0,
        "table_name": table_name,
        "schema": schema,
        **upload_stats
    }

    logger.info(f"‚úÖ Upload task completed: {upload_stats['features_uploaded']}/{upload_stats['features_attempted']} features")
    return result