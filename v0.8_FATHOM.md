# V0.8 FATHOM Pipeline Redesign

**Version**: 0.8.0
**Date**: 24 JAN 2026
**Status**: PLANNING
**Epic**: E7 - Platform Maturity

---

## Executive Summary

Redesign the FATHOM ETL pipeline to leverage the new Docker worker architecture with Azure Files mount. This consolidates the current multi-stage job orchestration into a single-stage Docker job with internal checkpointing, eliminating OOM issues and enabling larger grid sizes.

### Key Changes

| Aspect | Current (V0.7) | New (V0.8) |
|--------|---------------|------------|
| Job orchestration | 3-stage jobs (inventory → process → STAC) | 1-stage Docker job with internal phases |
| Memory management | Limited by container RAM (~8GB) | Streaming via GDAL VRT + Azure Files mount |
| Max grid size | 4×4 (OOM at 5×5) | 10×10+ (disk-based merging) |
| Phase 1 + 2 | Separate jobs | Unified `process_fathom_docker` |
| Checkpointing | Stage-level only | Per-tile/per-phase granular |
| Resume capability | Restart from stage | Resume from any checkpoint |

---

## 1. ARCHITECTURE OVERVIEW

### Current Architecture (V0.7)

```
┌─────────────────────────────────────────────────────────────────────┐
│ process_fathom_stack (Phase 1)                                       │
├─────────────────────────────────────────────────────────────────────┤
│ Stage 1: fathom_tile_inventory     → Groups 8M files by tile        │
│ Stage 2: fathom_band_stack         → Fan-out: stack 8 RPs → 1 COG   │
│ Stage 3: fathom_stac_register      → Fan-in: create STAC items      │
└─────────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────────┐
│ process_fathom_merge (Phase 2)                                       │
├─────────────────────────────────────────────────────────────────────┤
│ Stage 1: fathom_grid_inventory     → Groups tiles into grid cells   │
│ Stage 2: fathom_spatial_merge      → Fan-out: merge NxN → 1 COG     │
│ Stage 3: fathom_stac_register      → Fan-in: create STAC items      │
└─────────────────────────────────────────────────────────────────────┘
```

**Problems**:
- OOM during `fathom_spatial_merge` at grid_size ≥ 5 (loads all tiles into memory)
- Two separate jobs to run sequentially
- Stage progression overhead
- No granular checkpointing within stages

### New Architecture (V0.8)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    JOB ORCHESTRATOR VIEW                             │
│                                                                      │
│   process_fathom_docker                                              │
│   └── Stage 1: fathom_process_complete (single task)                 │
│                                                                      │
│   From orchestrator: ONE stage, ONE task, SIMPLE.                    │
└─────────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────────┐
│                    DOCKER HANDLER INTERNAL VIEW                      │
│                                                                      │
│   fathom_process_complete handler                                    │
│   ├── Phase 1: Inventory (scan etl_source_files)                     │
│   ├── Phase 2: Band Stack (loop with per-tile checkpoint)            │
│   ├── Phase 3: Grid Inventory (group by grid cell)                   │
│   ├── Phase 4: Spatial Merge (VRT streaming, per-grid checkpoint)    │
│   └── Phase 5: STAC Registration                                     │
│                                                                      │
│   All complexity hidden from orchestrator.                           │
│   Uses Azure Files mount for GDAL temp files (CPL_TMPDIR).           │
│   VRT-based merge = streaming, never loads full raster into RAM.     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 2. STAC METADATA ARCHITECTURE

### FATHOM-Specific Metadata Requirements

FATHOM flood data has specific metadata that must be preserved in STAC items:

| Field | Source | STAC Location |
|-------|--------|---------------|
| Provider | "FATHOM" | `providers[]` |
| License | "proprietary" | `license` |
| Flood type | fluvial/pluvial/coastal | `properties.fathom:flood_type` |
| Defense | defended/undefended | `properties.fathom:defense` |
| Year | 2020, 2030, etc. | `properties.fathom:year` |
| SSP scenario | SSP2_4.5, SSP5_8.5 | `properties.fathom:ssp` |
| Return periods | RP5-RP500 (8 bands) | `eo:bands[]` |
| Region | ISO country code | `properties.fathom:region` |

### Integration with Unified Metadata

The new implementation uses the existing `RasterMetadata` model and `STACMetadataHelper`:

```python
from core.models.unified_metadata import (
    RasterMetadata,
    Provider,
    ProviderRole,
    SpatialExtent,
    TemporalExtent,
    Extent
)
from services.stac_metadata_helper import (
    STACMetadataHelper,
    AppMetadata,
    RasterVisualizationMetadata
)

# FATHOM provider (reusable constant)
FATHOM_PROVIDER = Provider(
    name="FATHOM",
    description="Global flood hazard mapping",
    roles=[ProviderRole.PRODUCER, ProviderRole.LICENSOR],
    url="https://www.fathom.global/"
)

# Create RasterMetadata for each output COG
def create_fathom_raster_metadata(
    cog_id: str,
    cog_url: str,
    container: str,
    blob_path: str,
    bounds: dict,
    flood_type: str,
    defense: str,
    year: int,
    ssp: Optional[str],
    region_code: str,
    job_id: str
) -> RasterMetadata:
    """Create RasterMetadata for FATHOM output COG."""
    return RasterMetadata(
        id=cog_id,
        title=f"FATHOM {flood_type.title()} {'Defended' if defense == 'defended' else 'Undefended'} {year}",
        description=f"FATHOM flood depth data for {region_code.upper()}, {flood_type} scenario",
        keywords=["flood", "hazard", "fathom", flood_type, region_code],
        license="proprietary",
        providers=[FATHOM_PROVIDER],
        stac_extensions=[
            "https://stac-extensions.github.io/eo/v1.0.0/schema.json",
            "https://stac-extensions.github.io/raster/v1.1.0/schema.json",
            "https://stac-extensions.github.io/projection/v1.0.0/schema.json"
        ],
        extent=Extent(
            spatial=SpatialExtent.from_flat_bbox(
                bounds["west"], bounds["south"],
                bounds["east"], bounds["north"]
            ),
            temporal=TemporalExtent.from_datetimes(
                start=datetime(year, 1, 1, tzinfo=timezone.utc)
            )
        ),
        etl_job_id=job_id,
        source_format="geotiff",
        cog_url=cog_url,
        container=container,
        blob_path=blob_path,
        width=0,  # Populated from COG header
        height=0,
        band_count=8,  # 8 return periods
        dtype="float32",
        nodata=-32768,
        crs="EPSG:4326",
        band_names=RETURN_PERIODS,  # ["RP5", "RP10", ..., "RP500"]
        colormap="blues",  # Flood depth visualization
        # FATHOM-specific in custom_properties
        custom_properties={
            "fathom:flood_type": flood_type,
            "fathom:defense": defense,
            "fathom:year": year,
            "fathom:ssp": ssp,
            "fathom:region": region_code
        }
    )
```

### STAC Item Generation

Use `STACMetadataHelper` for consistent TiTiler links and metadata enrichment:

```python
def create_stac_item_from_metadata(
    metadata: RasterMetadata,
    base_url: str,
    titiler_base_url: str
) -> dict:
    """Generate STAC item from RasterMetadata with TiTiler links."""

    # Use RasterMetadata.to_stac_item() for base structure
    item_dict = metadata.to_stac_item(
        base_url=base_url,
        titiler_base_url=titiler_base_url,
        renders={
            "flood_depth": {
                "colormap_name": "blues",
                "rescale": [[0, 5]],  # 0-5m flood depth
                "bidx": [1]  # Show first return period by default
            }
        }
    )

    # Add FATHOM-specific properties from custom_properties
    for key, value in metadata.custom_properties.items():
        item_dict["properties"][key] = value

    # Add EO bands for return periods
    item_dict["assets"]["data"]["eo:bands"] = [
        {
            "name": rp,
            "description": f"Flood depth for {rp} return period",
            "common_name": f"flood_depth_{rp.lower()}"
        }
        for rp in RETURN_PERIODS
    ]

    # Augment with TiTiler links using helper
    helper = STACMetadataHelper()
    raster_meta = RasterVisualizationMetadata(
        raster_type="flood_depth",
        band_count=8,
        dtype="float32",
        colormap="blues",
        rescale={"min": 0, "max": 5}
    )

    item_dict = helper.augment_item(
        item_dict=item_dict,
        container=metadata.container,
        blob_name=metadata.blob_path,
        raster=raster_meta,
        app=AppMetadata(job_id=metadata.etl_job_id, job_type="process_fathom_docker"),
        include_iso3=True,
        include_titiler=True
    )

    return item_dict
```

---

## 3. IMPLEMENTATION PLAN

### Phase 1: Job Definition

Create new single-stage Docker job:

**File**: `jobs/process_fathom_docker.py`

```python
class ProcessFathomDockerJob(JobBaseMixin, JobBase):
    """
    FATHOM flood data processing - single stage Docker job.

    Combines Phase 1 (band stacking) and Phase 2 (spatial merge) into
    a unified workflow with internal checkpointing.
    """

    job_type = "process_fathom_docker"
    description = "Process FATHOM flood data (Docker - unified pipeline)"

    # Single stage - handler does everything
    stages = [{
        "number": 1,
        "name": "process_complete",
        "task_type": "fathom_process_complete",
        "parallelism": "single"
    }]

    parameters_schema = {
        # Region selection
        'region_code': {
            'type': 'str',
            'required': True,
            'description': 'ISO 3166-1 alpha-2 country code (e.g., "RWA")'
        },
        'bbox': {
            'type': 'list',
            'required': False,
            'description': 'Optional spatial filter [west, south, east, north]'
        },

        # Processing options
        'grid_size': {
            'type': 'int',
            'default': 5,
            'min': 1,
            'max': 20,
            'description': 'Grid cell size in degrees for Phase 2 merge'
        },
        'skip_phase1': {
            'type': 'bool',
            'default': False,
            'description': 'Skip Phase 1 if already completed (resume Phase 2 only)'
        },
        'skip_phase2': {
            'type': 'bool',
            'default': False,
            'description': 'Skip Phase 2 (Phase 1 only mode)'
        },

        # Filters
        'flood_types': {
            'type': 'list',
            'required': False,
            'description': 'Filter: flood types to process'
        },
        'years': {
            'type': 'list',
            'required': False,
            'description': 'Filter: years to process'
        },
        'ssp_scenarios': {
            'type': 'list',
            'required': False,
            'description': 'Filter: SSP scenarios to process'
        },

        # STAC
        'collection_id': {
            'type': 'str',
            'default': 'fathom-flood',
            'description': 'Base STAC collection ID'
        },

        # Behavior
        'force_reprocess': {
            'type': 'bool',
            'default': False,
            'description': 'Reprocess even if outputs exist'
        },
        'dry_run': {
            'type': 'bool',
            'default': False,
            'description': 'Inventory only, no processing'
        }
    }
```

### Phase 2: Handler Implementation

Create unified handler with internal phases:

**File**: `services/handler_fathom_complete.py`

```python
def fathom_process_complete(params: dict, context: dict = None) -> dict:
    """
    Complete FATHOM processing in single execution.

    Internal Phases:
        1. Inventory: Scan etl_source_files for pending work
        2. Band Stack: Stack 8 RPs per tile (with per-tile checkpoint)
        3. Grid Inventory: Group stacked tiles into grid cells
        4. Spatial Merge: Merge tiles per grid cell (VRT streaming)
        5. STAC Registration: Create collection and items

    Checkpoint Strategy:
        - Phase 1 & 3: Save inventory result
        - Phase 2: Save after each tile (granular resume)
        - Phase 4: Save after each grid cell (granular resume)
        - Phase 5: Save after STAC creation

    VRT Streaming (Phase 4):
        - Uses GDAL VRT to reference tiles without loading
        - gdal.BuildVRT creates virtual raster from tile paths
        - gdal.Translate with COG driver streams through disk
        - CPL_TMPDIR = Azure Files mount (unlimited temp space)
        - Peak memory: ~500MB regardless of grid size
    """
    # Implementation follows process_raster_complete pattern
    ...
```

### Phase 3: VRT-Based Spatial Merge

Replace memory-intensive `rasterio.merge` with GDAL VRT streaming:

```python
def _merge_tiles_vrt(
    tile_paths: List[str],
    output_path: str,
    band_names: List[str],
    mount_path: str
) -> dict:
    """
    Merge tiles using GDAL VRT - streams data through disk.

    Memory Usage:
        - Current (rasterio.merge): ~2-5GB peak for 16 tiles
        - VRT approach: ~500MB constant (streaming)

    How it works:
        1. Create VRT for each band (tiny XML, no data loaded)
        2. Stack VRTs into multi-band VRT
        3. gdal.Translate to COG (streams through CPL_TMPDIR)
    """
    from osgeo import gdal

    # Use Azure Files mount for temp files
    temp_dir = Path(mount_path) / "fathom_merge"
    temp_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Phase 4a: Create per-band VRTs
        band_vrts = []
        for band_idx, rp in enumerate(band_names):
            vrt_path = str(temp_dir / f"band_{band_idx}.vrt")

            # BuildVRT reads headers only - no data loaded
            vrt_options = gdal.BuildVRTOptions(
                bandList=[band_idx + 1],
                resolution="highest",
                resampleAlg="nearest"
            )
            vrt_ds = gdal.BuildVRT(vrt_path, tile_paths, options=vrt_options)
            vrt_ds = None  # Close to flush
            band_vrts.append(vrt_path)

        # Phase 4b: Stack into multi-band VRT
        stacked_vrt = str(temp_dir / "stacked.vrt")
        stack_options = gdal.BuildVRTOptions(separate=True)
        stacked_ds = gdal.BuildVRT(stacked_vrt, band_vrts, options=stack_options)
        stacked_ds = None

        # Phase 4c: Convert to COG (streaming through disk)
        translate_options = gdal.TranslateOptions(
            format="COG",
            creationOptions=[
                "COMPRESS=DEFLATE",
                "PREDICTOR=2",
                "BIGTIFF=IF_SAFER",
                "BLOCKSIZE=512"
            ],
            callback=gdal.TermProgress_nocb
        )

        result_ds = gdal.Translate(output_path, stacked_vrt, options=translate_options)

        if result_ds is None:
            raise RuntimeError("GDAL Translate failed")

        # Get bounds for STAC
        gt = result_ds.GetGeoTransform()
        width = result_ds.RasterXSize
        height = result_ds.RasterYSize
        bounds = {
            "west": gt[0],
            "east": gt[0] + width * gt[1],
            "north": gt[3],
            "south": gt[3] + height * gt[5]
        }

        result_ds = None

        return {
            "success": True,
            "output_path": output_path,
            "bounds": bounds,
            "width": width,
            "height": height
        }

    finally:
        # Cleanup temp VRTs
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
```

### Phase 4: Checkpoint Strategy

```python
# Checkpoint data structure for FATHOM
CHECKPOINT_SCHEMA = {
    "phase": int,              # Current phase (1-5)
    "phase1_inventory": dict,  # Tile groups from inventory
    "phase2_progress": {
        "total": int,          # Total tiles to stack
        "completed": int,      # Tiles stacked so far
        "completed_tiles": List[str],  # Tile IDs completed
    },
    "phase3_inventory": dict,  # Grid groups from inventory
    "phase4_progress": {
        "total": int,          # Total grid cells to merge
        "completed": int,      # Grid cells merged so far
        "completed_grids": List[str],  # Grid cell IDs completed
    },
    "phase5_stac": {
        "collection_created": bool,
        "items_created": int,
    }
}
```

### Phase 5: STAC Registration with Unified Metadata

```python
def _register_stac(
    cog_results: List[dict],
    region_code: str,
    collection_id: str,
    job_id: str,
    base_url: str,
    titiler_base_url: str
) -> dict:
    """
    Register STAC collection and items using unified metadata architecture.

    Uses:
        - RasterMetadata for consistent structure
        - STACMetadataHelper for TiTiler links
        - PgStacBootstrap for database operations
    """
    from infrastructure.pgstac_bootstrap import PgStacBootstrap
    from core.models.unified_metadata import RasterMetadata, Provider, ProviderRole
    from services.stac_metadata_helper import STACMetadataHelper, AppMetadata

    stac_repo = PgStacBootstrap()
    helper = STACMetadataHelper()

    full_collection_id = f"{collection_id}-{region_code}"

    # Create collection if not exists
    if not stac_repo.collection_exists(full_collection_id):
        # Calculate collection bounds from all COGs
        all_bounds = [r["bounds"] for r in cog_results if r.get("bounds")]
        collection_bounds = [
            min(b["west"] for b in all_bounds),
            min(b["south"] for b in all_bounds),
            max(b["east"] for b in all_bounds),
            max(b["north"] for b in all_bounds)
        ]

        stac_repo.create_collection(
            container=FathomDefaults.PHASE2_OUTPUT_CONTAINER,
            tier="silver",
            collection_id=full_collection_id,
            title=f"FATHOM Flood Hazard - {region_code.upper()}",
            description=f"FATHOM global flood model data for {region_code.upper()}",
            providers=[FATHOM_PROVIDER.to_stac_dict()],
            keywords=["flood", "hazard", "fathom", region_code],
            extent={
                "spatial": {"bbox": [collection_bounds]},
                "temporal": {"interval": [[None, None]]}
            }
        )

    # Create items
    items_created = 0
    for cog_result in cog_results:
        if not cog_result.get("bounds"):
            continue

        # Create RasterMetadata
        metadata = create_fathom_raster_metadata(
            cog_id=cog_result["output_name"],
            cog_url=f"/vsiaz/{cog_result['output_container']}/{cog_result['output_blob']}",
            container=cog_result["output_container"],
            blob_path=cog_result["output_blob"],
            bounds=cog_result["bounds"],
            flood_type=cog_result["flood_type"],
            defense=cog_result["defense"],
            year=cog_result["year"],
            ssp=cog_result.get("ssp"),
            region_code=region_code,
            job_id=job_id
        )

        # Generate STAC item
        item_dict = create_stac_item_from_metadata(
            metadata=metadata,
            base_url=base_url,
            titiler_base_url=titiler_base_url
        )

        # Insert into pgSTAC
        stac_repo.insert_item(item_dict, full_collection_id)
        items_created += 1

    return {
        "collection_id": full_collection_id,
        "items_created": items_created
    }
```

---

## 4. MIGRATION STRATEGY

### Backward Compatibility

Keep existing jobs and handlers working during transition:

| Job | Status | Notes |
|-----|--------|-------|
| `process_fathom_stack` | DEPRECATED | Use `process_fathom_docker` |
| `process_fathom_merge` | DEPRECATED | Use `process_fathom_docker` |
| `fathom_stac_rebuild` | KEEP | Useful for STAC-only rebuilds |

### Migration Steps

1. **Deploy new handler** (`handler_fathom_complete.py`)
2. **Deploy new job** (`process_fathom_docker.py`)
3. **Test on small region** (e.g., RWA with grid_size=5)
4. **Validate STAC output** matches existing pattern
5. **Mark old jobs as deprecated** in docs
6. **Remove old jobs** in V0.9

---

## 5. TESTING PLAN

### Unit Tests

```python
def test_vrt_merge_memory():
    """Verify VRT merge stays under 1GB peak memory."""
    ...

def test_checkpoint_resume():
    """Verify job resumes from checkpoint correctly."""
    ...

def test_stac_metadata_consistency():
    """Verify STAC items match unified metadata schema."""
    ...
```

### Integration Tests

```bash
# Test small region (Rwanda)
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -d '{"region_code": "rwa", "grid_size": 5, "dry_run": true}'

# Test with actual processing (Phase 1 only)
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -d '{"region_code": "rwa", "skip_phase2": true}'

# Test full pipeline
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -d '{"region_code": "rwa", "grid_size": 10}'
```

### Performance Benchmarks

| Metric | Current (V0.7) | Target (V0.8) |
|--------|---------------|---------------|
| Rwanda Phase 1 | 7 min | ~7 min (same) |
| Rwanda Phase 2 (grid=4) | 25 min | ~15 min |
| Rwanda Phase 2 (grid=10) | OOM | ~20 min |
| Peak memory (Phase 2) | 4-5 GB | <1 GB |
| Resume granularity | Stage | Per-tile/grid |

---

## 6. FILES TO CREATE/MODIFY

### New Files

| File | Purpose |
|------|---------|
| `jobs/process_fathom_docker.py` | Single-stage Docker job definition |
| `services/handler_fathom_complete.py` | Unified handler with internal phases |
| `services/fathom_vrt_merge.py` | VRT-based spatial merge (extracted) |
| `core/models/fathom_metadata.py` | FATHOM-specific metadata helpers |

### Modified Files

| File | Changes |
|------|---------|
| `jobs/__init__.py` | Register new job |
| `services/__init__.py` | Register new handler |
| `config/defaults.py` | Add FATHOM Docker defaults |
| `docs_claude/FATHOM_ETL.md` | Update architecture documentation |

### Deprecated (Remove in V0.9)

| File | Reason |
|------|--------|
| `jobs/process_fathom_stack.py` | Replaced by unified job |
| `jobs/process_fathom_merge.py` | Replaced by unified job |

---

## 7. TIMELINE

| Phase | Deliverable | Status |
|-------|-------------|--------|
| 1 | Job definition + routing | TODO |
| 2 | Handler skeleton + checkpointing | TODO |
| 3 | VRT merge implementation | TODO |
| 4 | STAC integration with unified metadata | TODO |
| 5 | Testing (unit + integration) | TODO |
| 6 | Documentation update | TODO |
| 7 | Deprecation of old jobs | TODO |

---

## 8. RISKS AND MITIGATIONS

| Risk | Impact | Mitigation |
|------|--------|------------|
| VRT merge produces different output | STAC items differ from existing | Validate output checksums match |
| Azure Files mount latency | Slower than local disk | Benchmark and optimize block size |
| Checkpoint corruption | Lost progress on resume | Validate checkpoint before resume |
| STAC schema changes | Breaking API changes | Use unified metadata for consistency |

---

## 9. SUCCESS CRITERIA

1. **Memory**: Peak memory during Phase 2 merge stays under 1GB for grid_size=10
2. **Resume**: Job can resume from any checkpoint within 30 seconds
3. **STAC**: Items match existing schema (backward compatible)
4. **Performance**: Rwanda full pipeline completes in <30 minutes
5. **Reliability**: No OOM errors for any supported grid size

---

## 10. APPENDIX: RETURN PERIOD BAND MAPPING

FATHOM uses 8 return periods per scenario:

| Band | Return Period | Description |
|------|---------------|-------------|
| 1 | RP5 | 5-year flood (20% annual probability) |
| 2 | RP10 | 10-year flood (10% annual probability) |
| 3 | RP20 | 20-year flood (5% annual probability) |
| 4 | RP50 | 50-year flood (2% annual probability) |
| 5 | RP75 | 75-year flood (1.3% annual probability) |
| 6 | RP100 | 100-year flood (1% annual probability) |
| 7 | RP250 | 250-year flood (0.4% annual probability) |
| 8 | RP500 | 500-year flood (0.2% annual probability) |

These are stored as `eo:bands` in STAC items for queryability.
