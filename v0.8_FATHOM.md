# V0.8 FATHOM Pipeline Redesign

**Version**: 0.8.0
**Date**: 24 JAN 2026
**Status**: PLANNING
**Epic**: E7 - Platform Maturity

---

## Executive Summary

Redesign the FATHOM ETL pipeline to leverage the new Docker worker architecture with Azure Files mount. This introduces a **3-stage hybrid architecture** that combines Azure Functions (lightweight orchestration) with Docker workers (heavy processing), enabling parallel processing across regions for global-scale runs.

### Key Changes

| Aspect | Current (V0.7) | New (V0.8) |
|--------|---------------|------------|
| Job orchestration | 3-stage jobs per region (sequential) | 3-stage hybrid with fan-out parallelism |
| Execution model | Single region per job | Multi-region/continent per job |
| Memory management | Limited by container RAM (~8GB) | Streaming via GDAL VRT + Azure Files mount |
| Max grid size | 4×4 (OOM at 5×5) | 10×10+ (disk-based merging) |
| Parallelism | Within-stage only | **Cross-region via fan-out to Docker pool** |
| Global run time | Weeks (sequential) | **Days (parallel Docker workers)** |

---

## 1. ARCHITECTURE OVERVIEW

### Current Architecture (V0.7)

```
┌─────────────────────────────────────────────────────────────────────┐
│ process_fathom_stack (Phase 1) - ONE REGION                          │
├─────────────────────────────────────────────────────────────────────┤
│ Stage 1: fathom_tile_inventory     → Groups 8M files by tile        │
│ Stage 2: fathom_band_stack         → Fan-out: stack 8 RPs → 1 COG   │
│ Stage 3: fathom_stac_register      → Fan-in: create STAC items      │
└─────────────────────────────────────────────────────────────────────┘
                              ↓ (must complete before Phase 2)
┌─────────────────────────────────────────────────────────────────────┐
│ process_fathom_merge (Phase 2) - ONE REGION                          │
├─────────────────────────────────────────────────────────────────────┤
│ Stage 1: fathom_grid_inventory     → Groups tiles into grid cells   │
│ Stage 2: fathom_spatial_merge      → Fan-out: merge NxN → 1 COG     │
│ Stage 3: fathom_stac_register      → Fan-in: create STAC items      │
└─────────────────────────────────────────────────────────────────────┘
```

**Problems**:
- OOM during `fathom_spatial_merge` at grid_size ≥ 5
- Two separate jobs per region, run sequentially
- No cross-region parallelism - global run requires ~200 sequential jobs
- Stage progression overhead

### New Architecture (V0.8) - Hybrid Fan-Out

```
┌─────────────────────────────────────────────────────────────────────┐
│ process_fathom_docker - MULTI-REGION JOB                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Stage 1: fathom_chunk_inventory (Azure Functions - single task)     │
│  ┌─────────────────────────────────────────────────────────────────┐│
│  │ • Scan database for pending work                                 ││
│  │ • Create work chunks (by region, or split large regions)        ││
│  │ • PRE-CREATE STAC collections for each region                   ││
│  │ • Output: List of chunks for fan-out                            ││
│  └─────────────────────────────────────────────────────────────────┘│
│                              ↓                                       │
│  Stage 2: fathom_process_chunk (Docker workers - FAN-OUT)            │
│  ┌─────────────────────────────────────────────────────────────────┐│
│  │         Service Bus: long-running-tasks queue                    ││
│  │                     ↓           ↓           ↓                    ││
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐         ││
│  │  │ Docker 1 │  │ Docker 2 │  │ Docker 3 │  │ Docker N │         ││
│  │  │  RWA     │  │  KEN     │  │  UGA     │  │  ...     │         ││
│  │  └──────────┘  └──────────┘  └──────────┘  └──────────┘         ││
│  │                                                                  ││
│  │  Each chunk task:                                                ││
│  │  • Inventory tiles for chunk                                    ││
│  │  • Band stack (Phase 1) with per-tile checkpoint                ││
│  │  • Grid inventory for chunk                                     ││
│  │  • Spatial merge (Phase 2) via VRT streaming                    ││
│  │  • Upsert STAC items (collection exists from Stage 1)           ││
│  │  • Re-register mosaic search                                    ││
│  └─────────────────────────────────────────────────────────────────┘│
│                              ↓                                       │
│  Stage 3: fathom_finalize (Azure Functions - single task)            │
│  ┌─────────────────────────────────────────────────────────────────┐│
│  │ • Validate expected vs actual item counts                       ││
│  │ • Update collection extents from items                          ││
│  │ • Aggregate job metrics                                         ││
│  │ • Return job summary                                            ││
│  └─────────────────────────────────────────────────────────────────┘│
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 2. JOB INPUTS AND CHUNKING

### Input Parameters

```python
parameters_schema = {
    # ═══════════════════════════════════════════════════════════════
    # SCOPE SELECTION (mutually exclusive)
    # ═══════════════════════════════════════════════════════════════
    'continent': {
        'type': 'str',
        'required': False,
        'enum': ['africa', 'asia', 'europe', 'north_america',
                 'south_america', 'oceania', 'global'],
        'description': 'Process entire continent'
    },
    'regions': {
        'type': 'list',
        'required': False,
        'description': 'List of region codes: ["rwa", "ken", "uga"]'
    },
    'region_code': {
        'type': 'str',
        'required': False,
        'description': 'Single region code (legacy/simple mode)'
    },
    'bbox': {
        'type': 'list',
        'required': False,
        'description': 'Spatial filter [west, south, east, north]'
    },

    # ═══════════════════════════════════════════════════════════════
    # CHUNKING STRATEGY
    # ═══════════════════════════════════════════════════════════════
    'chunk_strategy': {
        'type': 'str',
        'default': 'region',
        'enum': ['region', 'grid_cell', 'adaptive'],
        'description': 'How to divide work for parallelism'
    },
    'max_tiles_per_chunk': {
        'type': 'int',
        'default': 500,
        'description': 'Split large regions if tile count exceeds this'
    },

    # ═══════════════════════════════════════════════════════════════
    # PROCESSING OPTIONS
    # ═══════════════════════════════════════════════════════════════
    'grid_size': {
        'type': 'int',
        'default': 5,
        'min': 1,
        'max': 20,
        'description': 'Grid cell size in degrees for spatial merge'
    },
    'skip_phase1': {
        'type': 'bool',
        'default': False,
        'description': 'Skip band stacking (resume Phase 2 only)'
    },
    'skip_phase2': {
        'type': 'bool',
        'default': False,
        'description': 'Skip spatial merge (Phase 1 only mode)'
    },

    # ═══════════════════════════════════════════════════════════════
    # DATA FILTERS
    # ═══════════════════════════════════════════════════════════════
    'flood_types': {
        'type': 'list',
        'required': False,
        'description': 'Filter: ["fluvial", "pluvial", "coastal"]'
    },
    'years': {
        'type': 'list',
        'required': False,
        'description': 'Filter: [2020, 2030, 2050]'
    },
    'ssp_scenarios': {
        'type': 'list',
        'required': False,
        'description': 'Filter: ["SSP2_4.5", "SSP5_8.5"]'
    },

    # ═══════════════════════════════════════════════════════════════
    # STAC / OUTPUT
    # ═══════════════════════════════════════════════════════════════
    'collection_id': {
        'type': 'str',
        'default': 'fathom-flood',
        'description': 'Base STAC collection ID (region suffix added)'
    },

    # ═══════════════════════════════════════════════════════════════
    # BEHAVIOR
    # ═══════════════════════════════════════════════════════════════
    'force_reprocess': {
        'type': 'bool',
        'default': False,
        'description': 'Reprocess even if outputs exist'
    },
    'dry_run': {
        'type': 'bool',
        'default': False,
        'description': 'Inventory only, no processing'
    }
}
```

### Chunk Granularity

| Chunk By | Chunks (Global) | Avg Duration | Docker Workers | Total Time |
|----------|-----------------|--------------|----------------|------------|
| Continent | 7 | ~1 week each | 7 | ~1 week |
| **Region (country)** | ~200 | 30min - 4hrs | 10-20 | **1-2 days** |
| Grid Cell | ~40,000 | 5-15 min | 50+ | ~12 hours |

**Recommended**: Chunk by region, with large countries split by grid cell.

### Adaptive Chunking Logic

```python
def create_chunks(regions: List[str], max_tiles_per_chunk: int) -> List[dict]:
    """Create work chunks, splitting large regions."""
    chunks = []

    for region in regions:
        tile_count = get_tile_count(region)

        if tile_count > max_tiles_per_chunk:
            # Split large region by grid cell
            grid_chunks = split_by_grid_cell(region)
            for gc in grid_chunks:
                chunks.append({
                    'chunk_id': f"{region}-{gc['grid_cell']}",
                    'region_code': region,
                    'bbox': gc['bbox'],
                    'estimated_tiles': gc['tile_count']
                })
        else:
            # Region is one chunk
            chunks.append({
                'chunk_id': region,
                'region_code': region,
                'bbox': None,  # Full region
                'estimated_tiles': tile_count
            })

    return chunks
```

---

## 3. STAGE IMPLEMENTATIONS

### Stage 1: `fathom_chunk_inventory` (Azure Functions)

```python
def fathom_chunk_inventory(params: dict, context: dict) -> dict:
    """
    Create work chunks and pre-create STAC collections.

    Runs on: Azure Functions (fast, low memory)
    Duration: ~10-30 seconds

    Responsibilities:
        1. Determine scope (continent → regions)
        2. Query database for tile counts per region
        3. Create chunks based on strategy
        4. PRE-CREATE STAC collections (eliminates race conditions)
        5. Return chunks for Stage 2 fan-out
    """
    from infrastructure.pgstac_bootstrap import PgStacBootstrap

    # Determine regions in scope
    if params.get('continent'):
        regions = CONTINENT_REGIONS[params['continent']]
    elif params.get('regions'):
        regions = params['regions']
    elif params.get('region_code'):
        regions = [params['region_code']]
    else:
        raise ValueError("Must specify continent, regions, or region_code")

    # Get tile counts from database
    tile_counts = get_tile_counts_by_region(regions)

    # Create chunks based on strategy
    chunks = create_chunks(
        regions=regions,
        max_tiles_per_chunk=params.get('max_tiles_per_chunk', 500),
        strategy=params.get('chunk_strategy', 'region')
    )

    # PRE-CREATE STAC collections for all regions
    # This eliminates race conditions when multiple chunks process same region
    stac_repo = PgStacBootstrap()
    collections_created = []

    regions_in_scope = set(chunk['region_code'] for chunk in chunks)
    collection_base = params.get('collection_id', 'fathom-flood')

    for region in regions_in_scope:
        collection_id = f"{collection_base}-{region}"

        if not stac_repo.collection_exists(collection_id):
            stac_repo.create_collection(
                collection_id=collection_id,
                title=f"FATHOM Flood Hazard - {region.upper()}",
                description=f"FATHOM global flood model data for {region.upper()}",
                providers=[FATHOM_PROVIDER.to_stac_dict()],
                keywords=["flood", "hazard", "fathom", region],
                # Placeholder extent - updated by finalization
                extent={
                    "spatial": {"bbox": [[-180, -90, 180, 90]]},
                    "temporal": {"interval": [[None, None]]}
                }
            )
            collections_created.append(collection_id)

    return {
        'success': True,
        'result': {
            'chunks': chunks,
            'total_chunks': len(chunks),
            'total_estimated_tiles': sum(c['estimated_tiles'] for c in chunks),
            'collections_created': collections_created,
            'regions_in_scope': list(regions_in_scope)
        }
    }
```

### Stage 2: `fathom_process_chunk` (Docker Workers)

```python
def fathom_process_chunk(params: dict, context: dict) -> dict:
    """
    Process one chunk: band stack + spatial merge + STAC items.

    Runs on: Docker worker (heavy processing, Azure Files mount)
    Duration: 30 min - 4 hours depending on chunk size

    Internal phases (with checkpointing):
        Phase 1: Inventory tiles for this chunk
        Phase 2: Band stack all tiles (per-tile checkpoint)
        Phase 3: Grid inventory for this chunk
        Phase 4: Spatial merge per grid cell (VRT streaming, per-grid checkpoint)
        Phase 5: Upsert STAC items + register mosaic search

    STAC Handling:
        - Collection already exists (created in Stage 1)
        - Upsert items (idempotent)
        - Re-register mosaic search (trivial overhead with connection pooling)
    """
    chunk = params['chunk']
    job_params = params.get('job_parameters', {})
    region_code = chunk['region_code']
    collection_id = f"{job_params.get('collection_id', 'fathom-flood')}-{region_code}"

    # Get checkpoint manager from Docker context
    docker_context = context.get('docker_context')
    checkpoint = docker_context.checkpoint if docker_context else None

    # ═══════════════════════════════════════════════════════════════
    # PHASE 1: Inventory tiles for this chunk
    # ═══════════════════════════════════════════════════════════════
    if checkpoint and checkpoint.should_skip(1):
        inventory = checkpoint.get_data('phase1_inventory')
    else:
        inventory = _inventory_tiles_for_chunk(chunk, job_params)
        if checkpoint:
            checkpoint.save(phase=1, data={'phase1_inventory': inventory})

    # Check for graceful shutdown
    if docker_context and docker_context.should_stop():
        return {'success': True, 'interrupted': True, 'resumable': True}

    # ═══════════════════════════════════════════════════════════════
    # PHASE 2: Band stack (with per-tile checkpoint)
    # ═══════════════════════════════════════════════════════════════
    if not job_params.get('skip_phase1'):
        stacked_tiles = _band_stack_tiles(
            tile_groups=inventory['tile_groups'],
            checkpoint=checkpoint,
            docker_context=docker_context
        )
    else:
        stacked_tiles = inventory['existing_stacked']

    if docker_context and docker_context.should_stop():
        return {'success': True, 'interrupted': True, 'resumable': True}

    # ═══════════════════════════════════════════════════════════════
    # PHASE 3: Grid inventory
    # ═══════════════════════════════════════════════════════════════
    if checkpoint and checkpoint.should_skip(3):
        grid_groups = checkpoint.get_data('phase3_grid_groups')
    else:
        grid_groups = _create_grid_groups(
            stacked_tiles=stacked_tiles,
            grid_size=job_params.get('grid_size', 5),
            bbox=chunk.get('bbox')
        )
        if checkpoint:
            checkpoint.save(phase=3, data={'phase3_grid_groups': grid_groups})

    # ═══════════════════════════════════════════════════════════════
    # PHASE 4: Spatial merge (VRT streaming, per-grid checkpoint)
    # ═══════════════════════════════════════════════════════════════
    if not job_params.get('skip_phase2'):
        cog_results = _spatial_merge_grids(
            grid_groups=grid_groups,
            checkpoint=checkpoint,
            docker_context=docker_context,
            mount_path=config.raster.etl_mount_path
        )
    else:
        cog_results = stacked_tiles  # Phase 1 outputs are the final COGs

    if docker_context and docker_context.should_stop():
        return {'success': True, 'interrupted': True, 'resumable': True}

    # ═══════════════════════════════════════════════════════════════
    # PHASE 5: STAC items + mosaic search
    # ═══════════════════════════════════════════════════════════════
    items_created = _upsert_stac_items(
        cog_results=cog_results,
        collection_id=collection_id,
        job_id=params.get('job_id')
    )

    # Re-register mosaic search (idempotent, trivial overhead)
    _register_mosaic_search(collection_id)

    return {
        'success': True,
        'result': {
            'chunk_id': chunk['chunk_id'],
            'region_code': region_code,
            'tiles_stacked': len(stacked_tiles),
            'grids_merged': len(grid_groups),
            'items_created': items_created,
            'collection_id': collection_id
        }
    }
```

### Stage 3: `fathom_finalize` (Azure Functions)

```python
def fathom_finalize(params: dict, context: dict) -> dict:
    """
    Finalize job after all chunks complete.

    Runs on: Azure Functions (fast, database operations)
    Duration: ~30 seconds regardless of job size

    Responsibilities:
        1. Validate expected vs actual item counts
        2. Update collection extents from items
        3. Aggregate job metrics
        4. Return comprehensive job summary

    Future enhancements (not in V0.8):
        - Dynamic pgSTAC extent computation
        - Cross-collection mosaic registration
        - Notification webhooks
    """
    from infrastructure.pgstac_bootstrap import PgStacBootstrap

    job_params = params.get('job_parameters', {})
    chunk_results = params.get('previous_results', [])

    # Aggregate results from all chunks
    total_items = 0
    total_tiles = 0
    total_grids = 0
    collections_updated = set()

    for result in chunk_results:
        if result.get('success'):
            total_items += result.get('items_created', 0)
            total_tiles += result.get('tiles_stacked', 0)
            total_grids += result.get('grids_merged', 0)
            if result.get('collection_id'):
                collections_updated.add(result['collection_id'])

    # Update collection extents
    stac_repo = PgStacBootstrap()
    for collection_id in collections_updated:
        try:
            # Compute bounds from items
            bounds = stac_repo.compute_collection_bounds(collection_id)
            if bounds:
                stac_repo.update_collection_extent(collection_id, bounds)
        except Exception as e:
            logger.warning(f"Could not update extent for {collection_id}: {e}")

    # Validation
    expected_chunks = len(params.get('stage1_result', {}).get('chunks', []))
    actual_chunks = len([r for r in chunk_results if r.get('success')])

    return {
        'success': True,
        'result': {
            'job_summary': {
                'chunks_expected': expected_chunks,
                'chunks_completed': actual_chunks,
                'chunks_failed': expected_chunks - actual_chunks,
                'total_tiles_stacked': total_tiles,
                'total_grids_merged': total_grids,
                'total_stac_items': total_items,
                'collections_updated': list(collections_updated)
            },
            'validation': {
                'all_chunks_completed': actual_chunks == expected_chunks,
                'status': 'success' if actual_chunks == expected_chunks else 'partial'
            }
        }
    }
```

---

## 4. VRT-BASED SPATIAL MERGE

Replace memory-intensive `rasterio.merge` with GDAL VRT streaming:

```python
def _merge_tiles_vrt(
    tile_paths: List[str],
    output_path: str,
    band_names: List[str],
    mount_path: str
) -> dict:
    """
    Merge tiles using GDAL VRT - streams data through disk.

    Memory Usage:
        - Current (rasterio.merge): ~2-5GB peak for 16 tiles
        - VRT approach: ~500MB constant (streaming)

    How it works:
        1. Create VRT for each band (tiny XML, no data loaded)
        2. Stack VRTs into multi-band VRT
        3. gdal.Translate to COG (streams through CPL_TMPDIR on Azure Files)
    """
    from osgeo import gdal

    # Use Azure Files mount for temp files
    temp_dir = Path(mount_path) / "fathom_merge" / str(uuid.uuid4())[:8]
    temp_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Step 1: Create per-band VRTs (headers only - no data loaded)
        band_vrts = []
        for band_idx, rp in enumerate(band_names):
            vrt_path = str(temp_dir / f"band_{band_idx}.vrt")

            vrt_options = gdal.BuildVRTOptions(
                bandList=[band_idx + 1],
                resolution="highest",
                resampleAlg="nearest"
            )
            vrt_ds = gdal.BuildVRT(vrt_path, tile_paths, options=vrt_options)
            vrt_ds = None  # Close to flush
            band_vrts.append(vrt_path)

        # Step 2: Stack into multi-band VRT
        stacked_vrt = str(temp_dir / "stacked.vrt")
        stack_options = gdal.BuildVRTOptions(separate=True)
        stacked_ds = gdal.BuildVRT(stacked_vrt, band_vrts, options=stack_options)
        stacked_ds = None

        # Step 3: Convert to COG (streaming through disk)
        translate_options = gdal.TranslateOptions(
            format="COG",
            creationOptions=[
                "COMPRESS=DEFLATE",
                "PREDICTOR=2",
                "BIGTIFF=IF_SAFER",
                "BLOCKSIZE=512"
            ]
        )

        result_ds = gdal.Translate(output_path, stacked_vrt, options=translate_options)

        if result_ds is None:
            raise RuntimeError("GDAL Translate failed")

        # Get bounds for STAC
        gt = result_ds.GetGeoTransform()
        width = result_ds.RasterXSize
        height = result_ds.RasterYSize
        bounds = {
            "west": gt[0],
            "east": gt[0] + width * gt[1],
            "north": gt[3],
            "south": gt[3] + height * gt[5]
        }

        result_ds = None

        return {
            "success": True,
            "output_path": output_path,
            "bounds": bounds,
            "width": width,
            "height": height
        }

    finally:
        # Cleanup temp VRTs
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
```

---

## 5. STAC METADATA ARCHITECTURE

### FATHOM-Specific Metadata

| Field | Source | STAC Location |
|-------|--------|---------------|
| Provider | "FATHOM" | `providers[]` |
| License | "proprietary" | `license` |
| Flood type | fluvial/pluvial/coastal | `properties.fathom:flood_type` |
| Defense | defended/undefended | `properties.fathom:defense` |
| Year | 2020, 2030, etc. | `properties.fathom:year` |
| SSP scenario | SSP2_4.5, SSP5_8.5 | `properties.fathom:ssp` |
| Return periods | RP5-RP500 (8 bands) | `eo:bands[]` |
| Region | ISO country code | `properties.fathom:region` |

### STAC Workflow

```
Stage 1 (Functions)              Stage 2 (Docker)              Stage 3 (Functions)
─────────────────────────────────────────────────────────────────────────────────

CREATE collections ──────────────> UPSERT items ─────────────> UPDATE extents
(one per region)                  (per chunk)                   (aggregate bounds)
                                  + register mosaic

Collection exists before          Items added in parallel       Final cleanup
any chunk starts                  No race conditions            Validation
```

### Integration with Unified Metadata

```python
from core.models.unified_metadata import RasterMetadata, Provider, ProviderRole
from services.stac_metadata_helper import STACMetadataHelper, AppMetadata

# FATHOM provider constant
FATHOM_PROVIDER = Provider(
    name="FATHOM",
    description="Global flood hazard mapping",
    roles=[ProviderRole.PRODUCER, ProviderRole.LICENSOR],
    url="https://www.fathom.global/"
)

def _upsert_stac_items(cog_results: List[dict], collection_id: str, job_id: str) -> int:
    """Upsert STAC items for chunk results."""
    from infrastructure.pgstac_bootstrap import PgStacBootstrap

    stac_repo = PgStacBootstrap()
    helper = STACMetadataHelper()
    items_created = 0

    for cog in cog_results:
        if not cog.get('bounds'):
            continue

        # Build STAC item using unified metadata patterns
        item_dict = {
            "type": "Feature",
            "stac_version": "1.0.0",
            "stac_extensions": [
                "https://stac-extensions.github.io/eo/v1.0.0/schema.json",
                "https://stac-extensions.github.io/raster/v1.1.0/schema.json"
            ],
            "id": cog['output_name'],
            "collection": collection_id,
            "geometry": _bounds_to_geometry(cog['bounds']),
            "bbox": [
                cog['bounds']['west'], cog['bounds']['south'],
                cog['bounds']['east'], cog['bounds']['north']
            ],
            "properties": {
                "datetime": None,
                "fathom:flood_type": cog['flood_type'],
                "fathom:defense": cog['defense'],
                "fathom:year": cog['year'],
                "fathom:ssp": cog.get('ssp'),
                "fathom:region": cog['region_code']
            },
            "assets": {
                "data": {
                    "href": f"/vsiaz/{cog['output_container']}/{cog['output_blob']}",
                    "type": "image/tiff; application=geotiff; profile=cloud-optimized",
                    "roles": ["data"],
                    "eo:bands": [
                        {"name": rp, "description": f"Flood depth for {rp}"}
                        for rp in RETURN_PERIODS
                    ]
                }
            }
        }

        # Augment with TiTiler links
        item_dict = helper.augment_item(
            item_dict=item_dict,
            container=cog['output_container'],
            blob_name=cog['output_blob'],
            app=AppMetadata(job_id=job_id, job_type="process_fathom_docker"),
            include_iso3=True,
            include_titiler=True
        )

        # Upsert (idempotent)
        stac_repo.upsert_item(item_dict, collection_id)
        items_created += 1

    return items_created
```

---

## 6. EXAMPLE USAGE

### Single Region (Development/Testing)
```bash
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -H "Content-Type: application/json" \
  -d '{
    "region_code": "rwa",
    "grid_size": 5
  }'

# Creates 1 chunk → 1 Docker task → ~30 min
```

### Multi-Region (Production)
```bash
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -H "Content-Type: application/json" \
  -d '{
    "regions": ["rwa", "ken", "uga", "tza", "bdi"],
    "grid_size": 5
  }'

# Creates 5 chunks → 5 parallel Docker tasks → ~1 hour (with 5 workers)
```

### Continent (Africa)
```bash
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -H "Content-Type: application/json" \
  -d '{
    "continent": "africa",
    "grid_size": 10,
    "chunk_strategy": "adaptive",
    "max_tiles_per_chunk": 500
  }'

# Creates ~54 chunks (countries, large ones split)
# With 10 Docker workers → ~6 hours
```

### Global Run
```bash
curl -X POST .../api/jobs/submit/process_fathom_docker \
  -H "Content-Type: application/json" \
  -d '{
    "continent": "global",
    "grid_size": 10,
    "chunk_strategy": "adaptive"
  }'

# Creates ~200-300 chunks
# With 20 Docker workers → ~1-2 days
```

---

## 7. SCALING DOCKER WORKERS

| Workers | Africa (~54 chunks) | Global (~300 chunks) |
|---------|---------------------|----------------------|
| 1 | ~3 days | ~2 weeks |
| 4 | ~18 hours | ~4 days |
| 10 | ~6 hours | ~2 days |
| 20 | ~3 hours | ~1 day |

```bash
# Scale Docker workers
az webapp update --name rmhheavyapi --resource-group rmhazure_rg \
  --set siteConfig.numberOfWorkers=10
```

Service Bus competing consumers pattern automatically distributes tasks.

---

## 8. FILES TO CREATE/MODIFY

### New Files

| File | Purpose |
|------|---------|
| `jobs/process_fathom_docker.py` | 3-stage hybrid job definition |
| `services/handler_fathom_chunk_inventory.py` | Stage 1: chunking + collection creation |
| `services/handler_fathom_process_chunk.py` | Stage 2: Docker processing |
| `services/handler_fathom_finalize.py` | Stage 3: finalization |
| `services/fathom_vrt_merge.py` | VRT-based spatial merge |

### Modified Files

| File | Changes |
|------|---------|
| `jobs/__init__.py` | Register new job |
| `services/__init__.py` | Register new handlers |
| `config/defaults.py` | Add continent→region mappings |

### Deprecated (V0.9)

| File | Reason |
|------|--------|
| `jobs/process_fathom_stack.py` | Replaced by unified job |
| `jobs/process_fathom_merge.py` | Replaced by unified job |

---

## 9. SUCCESS CRITERIA

1. **Memory**: Peak memory during spatial merge stays under 1GB for grid_size=10
2. **Parallelism**: N Docker workers process N chunks concurrently
3. **STAC**: Items match existing schema (backward compatible)
4. **Global**: Full global run completes in <2 days with 20 workers
5. **Resume**: Job resumes from any checkpoint within 30 seconds
6. **No OOM**: Any grid_size up to 20 works without memory errors

---

## 10. APPENDIX: CONTINENT → REGION MAPPINGS

```python
CONTINENT_REGIONS = {
    'africa': ['dza', 'ago', 'ben', 'bwa', 'bfa', 'bdi', 'cmr', 'cpv', 'caf',
               'tcd', 'com', 'cog', 'cod', 'civ', 'dji', 'egy', 'gnq', 'eri',
               'eth', 'gab', 'gmb', 'gha', 'gin', 'gnb', 'ken', 'lso', 'lbr',
               'lby', 'mdg', 'mwi', 'mli', 'mrt', 'mus', 'mar', 'moz', 'nam',
               'ner', 'nga', 'rwa', 'stp', 'sen', 'syc', 'sle', 'som', 'zaf',
               'ssd', 'sdn', 'swz', 'tza', 'tgo', 'tun', 'uga', 'zmb', 'zwe'],
    'asia': [...],
    'europe': [...],
    'north_america': [...],
    'south_america': [...],
    'oceania': [...],
    'global': [...]  # All regions
}
```

---

## 11. APPENDIX: RETURN PERIOD BANDS

| Band | Return Period | Annual Probability |
|------|---------------|--------------------|
| 1 | RP5 | 20% |
| 2 | RP10 | 10% |
| 3 | RP20 | 5% |
| 4 | RP50 | 2% |
| 5 | RP75 | 1.3% |
| 6 | RP100 | 1% |
| 7 | RP250 | 0.4% |
| 8 | RP500 | 0.2% |
